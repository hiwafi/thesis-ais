{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing :\n",
    "<br>\n",
    "Two data sources will be imported <br>\n",
    "-\"AIS_weather_h_rename_copy.csv\" will be used to replace the information for true current direction and true wind direction <br>\n",
    "-The resulting dataframe will be merged with the dataframe from the \"AIS_weather_H_ok2_copy.csv\" <br>\n",
    "-The data of the journey between Ronne and Sassnitz will be omitted <br>\n",
    "-The threshold for the ships manouvering speed will be 5kt (Abebe) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Change font to latex\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "})\n",
    "\n",
    "dfmain = pd.read_csv(\"AIS_weather_H_ok2_copy.csv\",parse_dates=[\"Time\"])\n",
    "dfmain = dfmain[dfmain['LAT'] > 55.04 ]\n",
    "\n",
    "dfpre = pd.read_csv(\"AIS_weather_h_rename_copy.csv\",parse_dates=[\"Time\"])\n",
    "dfpre = dfpre[dfpre['LAT'] > 55.04 ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpre.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpre = dfpre.drop(dfpre.columns[[0,1,2,5,6,7,8,9,10,11,12,13,14,15,16,17,18,21,22,23,26]],axis=1)\n",
    "dfpre = dfpre.rename({'Eastward wind': 'eastwind', 'Northward wind': 'northwind',\n",
    "                           'Eastward current': 'eastcurrent', 'Northward current': 'northcurrent',\n",
    "                           'SOG':'sog','COG':'cog','Heading':'heading'},axis=1) \n",
    "dfpre.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing the information for the missing east and north current data\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=50)\n",
    "imputer.fit(dfpre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pre = imputer.transform(dfpre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpre = pd.DataFrame(x_pre, columns=dfpre.columns, index=dfpre.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpre[\"Current Speed\"] = np.sqrt(dfpre[\"eastcurrent\"]**2 + dfpre[\"northcurrent\"]**2)\n",
    "dfpre[\"Wind Speed\"] = np.sqrt(dfpre[\"eastwind\"]**2 + dfpre[\"northwind\"]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the True North wind direction\n",
    "\n",
    "condwind = [(dfpre['eastwind']>0) & (dfpre['northwind']<0),\n",
    "            (dfpre['eastwind']<0) & (dfpre['northwind']>0),\n",
    "            (dfpre['eastwind']>0) & (dfpre['northwind']>0)]\n",
    "\n",
    "choicewind = [360 - np.abs(np.rad2deg(np.arctan(dfpre[\"eastwind\"]/dfpre[\"northwind\"]))),\n",
    "              180 - np.abs(np.rad2deg(np.arctan(dfpre[\"eastwind\"]/dfpre[\"northwind\"]))),\n",
    "              270 - np.abs(np.rad2deg(np.arctan(dfpre[\"eastwind\"]/dfpre[\"northwind\"])))]\n",
    "\n",
    "dfpre[\"True North Wind Direction\"] = np.select(condwind, choicewind, \n",
    "                                                np.abs(np.rad2deg(np.arctan(dfpre[\"eastwind\"]/dfpre[\"northwind\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the True North current direction\n",
    "\n",
    "condcurr = [(dfpre['eastcurrent']<0) & (dfpre['northcurrent']>0),\n",
    "            (dfpre['eastcurrent']>0) & (dfpre['northcurrent']<0),\n",
    "            (dfpre['eastcurrent']<0) & (dfpre['northcurrent']<0)]\n",
    "\n",
    "choicecurr = [360 - np.abs(np.rad2deg(np.arctan(dfpre[\"eastcurrent\"]/dfpre[\"northcurrent\"]))),\n",
    "              180 - np.abs(np.rad2deg(np.arctan(dfpre[\"eastcurrent\"]/dfpre[\"northcurrent\"]))),\n",
    "              270 - np.abs(np.rad2deg(np.arctan(dfpre[\"eastcurrent\"]/dfpre[\"northcurrent\"])))]\n",
    "\n",
    "dfpre[\"True North Current Direction\"] = np.select(condcurr, choicecurr, \n",
    "                                                np.abs(np.rad2deg(np.arctan(dfpre[\"eastcurrent\"]/dfpre[\"northcurrent\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpre.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpre = dfpre.drop(['eastwind','northwind','eastcurrent','northcurrent','sog','cog'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the wind speed and current speed from the original dataset as it contains outlier \n",
    "\n",
    "dfmain = dfmain.drop(['Wind Speed','Current Speed'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dfmain,dfpre],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOG threshold according to Abebe\n",
    "df = df[df['SOG'] > 5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude data for the month of June to check the model's forecasting performance \n",
    "dfdate6 = df[df['Time'].dt.strftime('%Y-%m') == '2021-06']\n",
    "df = df[df['Time'].dt.strftime('%Y-%m') != '2021-06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary export\n",
    "# df.to_csv(\"AIS_sog_threshold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary export\n",
    "# df.to_csv(\"AIS_impute_check.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis before modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ship = df.drop(['Unnamed: 0','Time','LON','LAT','Air density above oceans',\n",
    "                    'Surface pressure','Width','Length'],axis=1)\n",
    "df_ship.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ship2 = df_ship.rename({'Max wave height': 'waveheight', 'Draught': 'draught',\n",
    "                           'SOG': 'sog', 'Wind Speed': 'windspeed', \n",
    "                           'True Wind Direction': 'truewinddir','Temperature above oceans' : 'oceantemperature',\n",
    "                           'COG': 'cog', 'Current Speed' : 'curspeed','True Wave Direction' : 'truewavedir',\n",
    "                            'Swell period': 'swellperiod','Wind wave period': 'windwaveperiod','Sea surface temperature': 'surftemp',\n",
    "                            'Combined wind waves and swell height': 'windwaveswellheight','Swell height': 'swellheight','Wind wave height': 'windwaveheight',\n",
    "                            'Heading': 'heading','True Current Direction': 'truecurrentdir','True Swell Direction': 'trueswelldir',\n",
    "                            'True Wind Wave Direction': 'truewindwavedir','Wave period': 'waveperiod',\n",
    "                            'True North Wind Direction' : 'truenorthwinddir' , 'True North Current Direction' : 'truenorthcurrentdir'\n",
    "                           }, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ship2 = df_ship2[['oceantemperature','waveheight','swellperiod','windwaveperiod','waveperiod','surftemp','windwaveswellheight','swellheight','windwaveheight','draught','sog','cog','heading','windspeed','curspeed','truewinddir','truecurrentdir','trueswelldir','truewindwavedir','truewavedir', 'truenorthwinddir' , 'truenorthcurrentdir']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scaling script if necessary\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# df_ship2 = pd.DataFrame(scaler.fit_transform(df_ship2),columns=df_ship2.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update(plt.rcParamsDefault) # Alter default value \n",
    "df_ship2.hist(bins=50,figsize=(20,25))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_ship2.corr()\n",
    "print(corr_matrix[\"sog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfship_corr = df_ship2.drop(df_ship2.columns[[0,5,15,16,17,18,19,20,21]],axis=1)\n",
    "dfship_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(df,fontsize):\n",
    "    correlations = df.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.set(font_scale = fontsize)\n",
    "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
    "    plt.show();\n",
    "    \n",
    "correlation_heatmap(df_ship2,0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientific Justification of HCF:\n",
    "\n",
    "-According to Bitner Gregersen, the wind wave swell height is equal to the square root of the square of swell wave and wind wave. Therefore SWH and WWH is to be dropped.<br>\n",
    "-According to Mori, through wave spectrum, the maximum wave height can be approximated, therefore max wave height can be dropped as well <br>\n",
    "-According to Torsetshaugen, The type of dominating wave (Wind wave/Swell) can be predicted from the significant through an equation which include the significant wave height and a certain threshold, therefore, it is decided that the wind wave period, swell period and consequently the true wind wave direction and true swell direction is to be dropped.<br>\n",
    "-Drop The True Wind and True North direction and replace with the True North direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ship2 = df_ship2.drop(['waveheight','swellheight','windwaveheight',\n",
    "                        'windwaveperiod','swellperiod',\n",
    "                        'truewindwavedir','trueswelldir',\n",
    "                        'truecurrentdir','truewinddir'],axis=1)\n",
    "correlation_heatmap(df_ship2,.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "df_ship2.hist(bins=50,figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = df_ship2.describe()\n",
    "print(stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing using KNN so that the Random Forest Regressor may function <br>\n",
    "RFR cannot accept nan values <br>\n",
    "Imputed data is then transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=50)\n",
    "imputer.fit(df_ship2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(df_ship2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ship2tr = pd.DataFrame(X, columns=df_ship2.columns, index=df_ship2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ship2tr.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Split for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Random forest, but first, data is to be split into training and validation data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time \n",
    "\n",
    "# Dropping some feature can be done here to (possibly) increase the model's performance \n",
    "x_train,x_test,y_train,y_test = train_test_split(df_ship2tr.drop(['sog']\n",
    "                                                                 ,axis=1)\n",
    "                                                                ,df_ship2tr.sog,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of the x training dataset is:\",len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amount of the test dataset is\",len(x_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling using Random Forest Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_rfr_ftr = RandomForestRegressor(random_state=42)\n",
    "start_rf = time.time()\n",
    "model_rfr_ftr.fit(x_train,y_train)\n",
    "end_rf = time.time()\n",
    "print(f\"Training time: {end_rf-start_rf:0.4}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "start_rf = time.time()\n",
    "model_rf = rf.fit(x_train,y_train)\n",
    "end_rf = time.time()\n",
    "print(f\"Training time: {end_rf-start_rf:0.4}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# model_rfr_ftr_hpo = RandomForestRegressor(n_estimators=300,\n",
    "#                                 #   max_features=9,\n",
    "#                                   random_state=42, \n",
    "#                                   )\n",
    "# model_rfr_ftr_hpo.fit(x_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO RFR for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# model_rfr_ftr_hpov = RandomForestRegressor(n_estimators = 400,\n",
    "#                                             min_samples_split = 3,\n",
    "#                                             min_samples_leaf = 1,\n",
    "#                                             # max_features = 9,\n",
    "#                                             max_depth=170,\n",
    "#                                             # bootstrap=False,\n",
    "#                                             random_state=42)\n",
    "\n",
    "# start_rfo = time.time()\n",
    "# model_rfr_ftr_hpov.fit(x_train,y_train)\n",
    "# end_rfo = time.time()\n",
    "# print(f\"Training time: {end_rfo-start_rfo:0.4}s \")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_rfr_ftr_hpov = RandomForestRegressor(n_estimators = 900,\n",
    "                                            min_samples_split = 2,\n",
    "                                            min_samples_leaf = 2,\n",
    "                                            # max_features = 9,\n",
    "                                            max_depth=200,\n",
    "                                            # bootstrap=False,\n",
    "                                            random_state=42)\n",
    "\n",
    "start_rfo = time.time()\n",
    "model_rfr_ftr_hpov.fit(x_train,y_train)\n",
    "end_rfo = time.time()\n",
    "print(f\"Training time: {end_rfo-start_rfo:0.4}s \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling using ETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_etr = ExtraTreesRegressor(random_state=42)\n",
    "start_et = time.time()\n",
    "model_etr.fit(x_train,y_train)\n",
    "end_et = time.time()\n",
    "print(f\"Training time: {end_et-start_et:0.4}s \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO ETR for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# model_etr_hpov = ExtraTreesRegressor(random_state=42,\n",
    "#                                 n_estimators=300,\n",
    "#                                 max_depth=120,\n",
    "#                                 min_samples_split=4,\n",
    "#                                 )\n",
    "# start_eto = time.time()\n",
    "# model_etr_hpov.fit(x_train,y_train)\n",
    "# end_eto = time.time()\n",
    "# print(f\"Training time: {end_eto-start_eto:0.4}s\")\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_etr_hpov = ExtraTreesRegressor(random_state=42,\n",
    "                                n_estimators=600,\n",
    "                                max_depth=10,\n",
    "                                )\n",
    "start_eto = time.time()\n",
    "model_etr_hpov.fit(x_train,y_train)\n",
    "end_eto = time.time()\n",
    "print(f\"Training time: {end_eto-start_eto:0.4}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling using Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model_dtr = DecisionTreeRegressor()\n",
    "start_dtr = time.time()\n",
    "model_dtr.fit(x_train,y_train)\n",
    "end_dtr = time.time()\n",
    "print(f\"Training time: {end_dtr-start_dtr:0.4}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO DTR for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# model_dtr_hpov = DecisionTreeRegressor(min_samples_split=3,\n",
    "#                                   min_samples_leaf=10,\n",
    "#                                   max_features=12,\n",
    "#                                   max_depth=80)\n",
    "# start_dtro = time.time()\n",
    "# model_dtr_hpov.fit(x_train,y_train)\n",
    "# end_dtro = time.time()\n",
    "# print(f\"Training time: {end_dtro-start_dtro:0.4}s\")\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model_dtr_hpov = DecisionTreeRegressor(min_samples_split=2,\n",
    "                                  min_samples_leaf=3,\n",
    "                                  max_features=12,\n",
    "                                  max_depth=10)\n",
    "start_dtro = time.time()\n",
    "model_dtr_hpov.fit(x_train,y_train)\n",
    "end_dtro = time.time()\n",
    "print(f\"Training time: {end_dtro-start_dtro:0.4}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling using Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model_mlr = linear_model.LinearRegression()\n",
    "start_mlr = time.time()\n",
    "model_mlr.fit(x_train,y_train)\n",
    "end_mlr = time.time()\n",
    "print(f\"Training time: {end_mlr-start_mlr:0.4}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree using graphviz\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(model_rfr_ftr.estimators_[5], \n",
    "                  feature_names=x_train.columns.values.tolist(),  \n",
    "                #   class_names=class_names,  \n",
    "                  filled=True, rounded=True,  \n",
    "                  special_characters=True,\n",
    "                   out_file=None,\n",
    "                   max_depth=3,\n",
    "                           )\n",
    "\n",
    "# os.system('dot -Tpng tree.dot -o tree.png')\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph\n",
    "graph.format = \"png\"\n",
    "graph.render(\"rf_tree_it5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_importance(model_type):\n",
    "    mod_imp = model_type.feature_importances_\n",
    "    df_mod_imp = pd.DataFrame(mod_imp,index= x_train.columns,columns=[\"Importance\"])\n",
    "    print(df_mod_imp)\n",
    "\n",
    "model_importance(model_rfr_ftr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, features_x, labels_y):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    score_r2 = cross_val_score(model,features_x,labels_y,\n",
    "                           scoring='r2',cv=10)\n",
    "    rsquared = score_r2.mean()\n",
    "\n",
    "    score_expVar = cross_val_score(model,features_x,labels_y,\n",
    "                           scoring='explained_variance',cv=10)\n",
    "    expVar = score_expVar.mean()\n",
    "\n",
    "    score_MAE = cross_val_score(model,features_x,labels_y,\n",
    "                           scoring='neg_mean_absolute_error',cv=10)\n",
    "    MAE = -score_MAE.mean()\n",
    "\n",
    "    score_MAD = cross_val_score(model,features_x,labels_y,\n",
    "                           scoring='neg_median_absolute_error',cv=10)\n",
    "    MAD = -score_MAD.mean()\n",
    "\n",
    "    score_RMSE = cross_val_score(model,features_x,labels_y,\n",
    "                           scoring='neg_root_mean_squared_error',cv=10)\n",
    "    RMSE = -score_RMSE.mean()\n",
    "\n",
    "    print(f\"Model Performance of {model}\")\n",
    "    print(f\"R^2: {rsquared:0.4f}\")\n",
    "    print(f\"explained Variance = {expVar:0.4f}\")\n",
    "    print(f\"MAE = {MAE:0.4f}\")\n",
    "    print(f\"RMSE = {RMSE:0.4f}\")\n",
    "    print(f\"MAD = {MAD:0.4f}\\n\")\n",
    "\n",
    "    return score_r2,score_expVar,score_MAE,score_MAD,score_RMSE    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_rf,expVar_rf,MAE_rf,RMSE_rf,MAD_rf = evaluate(model_rfr_ftr,x_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of ETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_et,expVar_et,MAE_et,RMSE_et,MAD_et = evaluate(model_etr,x_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_dt,expVar_dt,MAE_dt,RMSE_dt,MAD_dt = evaluate(model_dtr,x_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of MLR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_mlr,expVar_mlr,MAE_mlr,RMSE_mlr,MAD_mlr = evaluate(model_mlr,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(model_mlr.coef_, x_test.columns, columns=['Coefficients'])\n",
    "print(cdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a y = mx + c for the predicted vs actual  \n",
    "\n",
    "from numpy import arange\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# plt.figure(figsize=(5,5))\n",
    "\n",
    "# define the true objective function\n",
    "def objective(x, a, b ):\n",
    "\treturn a * x + b\n",
    "\n",
    "def label_predict(model,test_features):\n",
    "    predictions = model.predict(test_features)\n",
    "    return predictions\n",
    "\n",
    "def pred_plot(model,test_feature,test_label):\n",
    "    \n",
    "    plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    })\n",
    "    plt.figure(figsize=(5,5))\n",
    "    predict = label_predict(model,test_feature)\n",
    "\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    print(r2_score(test_label, predict))\n",
    "\n",
    "    # choose the input and output variables\n",
    "    x, y = test_label, predict\n",
    "\n",
    "    # curve fit\n",
    "    popt, _ = curve_fit(objective, x, y)\n",
    "\n",
    "\n",
    "    # summarize the parameter values\n",
    "    a, b = popt\n",
    "    print('y = %.5f * x + %.5f' % (a, b ))\n",
    "\n",
    "    # plot input vs output\n",
    "    \n",
    "    plt.scatter(x, y,edgecolors='black')\n",
    "    \n",
    "    # define a sequence of inputs between the smallest and largest known inputs\n",
    "    \n",
    "    x_line = arange(min(x), max(x), 1)\n",
    "        \n",
    "    # calculate the output for the range\n",
    "    \n",
    "    y_line = objective(x_line, a, b)\n",
    "    \n",
    "    # create a line plot for the mapping function\n",
    "    \n",
    "    plt.plot(x_line, y_line, color='red')\n",
    "    plt.xlabel(r'Predicted SOG [$m/s$]', fontsize=10)\n",
    "    plt.ylabel(r'Actual SOG [$m/s$]', fontsize=10)\n",
    "    plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "    plt.xlim(0,25)\n",
    "    plt.ylim(0,25)\n",
    "    plt.show()\n",
    "\n",
    "pred_plot(model_dtr,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_plot(model_rfr_ftr,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_plot(model_etr,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_plot(model_mlr,x_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the effects of different hyperparameter optimisation on model's performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot effect of number of features on RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore random forest number of features effect on performance\n",
    "def feature_curve(x,y,regressor):\n",
    "\tfrom numpy import mean\n",
    "\tfrom numpy import std\n",
    "\tfrom sklearn.model_selection import cross_val_score\n",
    "\tfrom sklearn.model_selection import KFold\n",
    "\n",
    "\tplt.rcParams.update({\n",
    "\t\"text.usetex\": True,\n",
    "\t\"font.family\": \"serif\",\n",
    "\t})\n",
    "\n",
    "\t# get a list of models to evaluate\n",
    "\tdef get_modelsftr():\n",
    "\t\tmodels_ftr = dict()\n",
    "\t\t# explore number of features from 1 to 13\n",
    "\t\tfor n in range(1,13):\n",
    "\t\t\tif regressor == 'dt':\n",
    "\t\t\t\tmodels_ftr[str(n)] = DecisionTreeRegressor(max_features=n)\n",
    "\t\t\telif regressor == 'rf':\n",
    "\t\t\t\tmodels_ftr[str(n)] = RandomForestRegressor(max_features=n)\n",
    "\t\t\telif regressor == 'et':\n",
    "\t\t\t\tmodels_ftr[str(n)] = ExtraTreesRegressor(max_features=n)\t\n",
    "\t\treturn models_ftr\n",
    "\n",
    "\t# evaluate a given model using cross-validation\n",
    "\tdef evaluate_model(model, x, y):\n",
    "\t\t# define the evaluation procedure\n",
    "\t\tcv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\t\t# evaluate the model and collect the results\n",
    "\t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\t\t# negative scores due to scoring mechanism of sklearn\n",
    "\t\treturn -scores\n",
    "\n",
    "\t# get the models to evaluate\n",
    "\tmodels_ftr = get_modelsftr()\n",
    "\t# evaluate the models and store results\n",
    "\tresults_ftr, names_ftr = list(), list()\n",
    "\tfor name, model in models_ftr.items():\n",
    "\t\t# evaluate the model\n",
    "\t\tscores_ftr = evaluate_model(model, x, y)\n",
    "\t\t# store the results\n",
    "\t\tresults_ftr.append(scores_ftr)\n",
    "\t\tnames_ftr.append(name)\n",
    "\t\t# summarize the performance along the way\n",
    "\t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_ftr), std(scores_ftr)))\n",
    "\t\n",
    "\t# Calculate mean for the x value of the plot\n",
    "\n",
    "\tmean_ftr = np.mean(results_ftr,axis=1)\n",
    "\tmin_error_ftr = np.min(mean_ftr)\n",
    "\tbst_n_estimators = np.argmin(mean_ftr) \n",
    "\n",
    "\tprint(f\"The minimum RMSE obtained is {min_error_ftr:.3f}\")\n",
    "\n",
    "\tplt.plot(names_ftr,mean_ftr,\"b.-\")\n",
    "\tplt.plot([bst_n_estimators, bst_n_estimators], [0, min_error_ftr], \"k--\",linewidth=1)\n",
    "\tplt.plot([-1, 12], [min_error_ftr, min_error_ftr], \"k--\",linewidth=1)\n",
    "\tplt.plot(bst_n_estimators, min_error_ftr, \"ko\",linewidth = 1)\n",
    "\tplt.text(bst_n_estimators, min_error_ftr*1.1, \"Minimum\", ha=\"center\", fontsize=12)\n",
    "\tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "\tplt.xlim(0,12)\n",
    "\tplt.ylim(0,3)\n",
    "\tplt.xlabel(\"Number of features\")\n",
    "\tplt.ylabel(\"RMSE\")\n",
    "\tplt.title(\"Validation error\", fontsize=13)\n",
    "\tplt.text(4,2.5, r'def_param : \\tt{max_features = n_features}', bbox={'facecolor' : 'white','alpha':0.5})\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_curve(x_test,y_test,regressor='dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_curve(x_test,y_test,regressor='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_curve(x_test,y_test,regressor='et')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore effect of number of trees on RMSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not relevant for decision tree, as we are looking into amount of trees in a forest. Decision Tree handles only with single tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore random forest and extra tree number of trees effect on performance\n",
    "def trees_curve(x,y,regressor):\n",
    "\tfrom numpy import mean\n",
    "\tfrom numpy import std\n",
    "\tfrom sklearn.model_selection import cross_val_score\n",
    "\n",
    "\t# get a list of models to evaluate\n",
    "\tdef get_models_tree():\n",
    "\t\tmodels_tree = dict()\n",
    "\t\t# define number of trees to consider\n",
    "\t\tn_trees = [1,10,100,200,300,400,500,600,700,800,900,1000]\n",
    "\t\tfor n in n_trees:\n",
    "\t\t\tif regressor == 'rf':\n",
    "\t\t\t\tmodels_tree[str(n)] = RandomForestRegressor(n_estimators = n)\n",
    "\t\t\telif regressor == 'et':\n",
    "\t\t\t\tmodels_tree[str(n)] = ExtraTreesRegressor(n_estimators = n)\t\n",
    "\t\treturn models_tree\n",
    "\n",
    "\t# evaluate a given model using cross-validation\n",
    "\tdef evaluate_model(model_tree, x, y):\n",
    "\t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n",
    "\t\treturn -scores\n",
    "\n",
    "\t# # define dataset\n",
    "\t# get the models to evaluate\n",
    "\tmodels_tree = get_models_tree()\n",
    "\n",
    "\t# evaluate the models and store results\n",
    "\tresults_tree, names_tree = list(), list()\n",
    "\tfor name, model in models_tree.items():\n",
    "\t\t# evaluate the model\n",
    "\t\tscores_tree = evaluate_model(model, x, y)\n",
    "\t\t# store the results\n",
    "\t\tresults_tree.append(scores_tree)\n",
    "\t\tnames_tree.append(name)\n",
    "\t\t# summarize the performance along the way\n",
    "\t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_tree), std(scores_tree)))\n",
    "\n",
    "\n",
    "\tmean_tree = np.mean(results_tree,axis=1)\n",
    "\tmin_error_tree = np.min(mean_tree)\n",
    "\tprint(min_error_tree)\n",
    "\tbst_n_estimators_tree = np.argmin(mean_tree)\n",
    "\tprint(f\"The minimum RMSE obtained is {min_error_tree:.3f}\")\n",
    "\tplt.plot(names_tree,mean_tree,\"b.-\")\n",
    "\tplt.plot([bst_n_estimators_tree, bst_n_estimators_tree], [0, min_error_tree], \"k--\",linewidth=1)\n",
    "\tplt.plot([-1, 12-1], [min_error_tree, min_error_tree], \"k--\",linewidth=1)\n",
    "\tplt.plot(bst_n_estimators_tree, min_error_tree, \"ko\",linewidth = 1)\n",
    "\tplt.text(bst_n_estimators_tree, min_error_tree*1.2, \"Minimum\", ha=\"center\", fontsize=12)\n",
    "\tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "\tplt.xlim(0,12-1)\n",
    "\tplt.ylim(0,3)\n",
    "\tplt.xlabel(\"Number of Trees\")\n",
    "\tplt.ylabel(\"RMSE\")\n",
    "\tplt.title(\"Validation error\", fontsize=13)\n",
    "\tplt.text(4,2.5, r'def_param : \\tt{n_estimators = 100}', bbox={'facecolor' : 'white','alpha':0.5})\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_curve(x_test,y_test,regressor='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_curve(x_test,y_test,regressor='et')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore effect of tree depth on RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore tree based , tree depth. effect on performance\n",
    "def depth_curve(x,y,regressor):\n",
    "\tfrom numpy import mean\n",
    "\tfrom numpy import std\n",
    "\tfrom sklearn.model_selection import cross_val_score\n",
    "\tfrom sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\t# get a list of models to evaluate\n",
    "\tdef get_models_dp():\n",
    "\t\tmodels_dp = dict()\n",
    "\t\t# consider tree depths from 1 to 7 and None=full\n",
    "\t\tdepths = [1,2,3,4,5,6,7,8,9,10,100] + [None]\n",
    "\t\tfor n in depths:\n",
    "\t\t\tif regressor == 'dt':\n",
    "\t\t\t\tmodels_dp[str(n)] = DecisionTreeRegressor(max_depth=n)\n",
    "\t\t\telif regressor == 'rf':\n",
    "\t\t\t\tmodels_dp[str(n)] = RandomForestRegressor(max_depth=n)\n",
    "\t\t\telif regressor == 'et':\n",
    "\t\t\t\tmodels_dp[str(n)] = ExtraTreesRegressor(max_depth=n)\t\n",
    "\t\treturn models_dp\n",
    "\n",
    "\t# evaluate a given model using cross-validation\n",
    "\tdef evaluate_model(model, x, y):\n",
    "\t\t# define the evaluation procedure\n",
    "\t\tcv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\t\t# evaluate the model and collect the results\n",
    "\t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\t\t# negative scores due to scoring mechanism of sklearn\n",
    "\t\treturn -scores\n",
    "\n",
    "\t# get the models to evaluate\n",
    "\tmodels_dp = get_models_dp()\n",
    "\t# evaluate the models and store results\n",
    "\tresults_dp, names_dp = list(), list()\n",
    "\tfor name, model in models_dp.items():\n",
    "\t\t# evaluate the model\n",
    "\t\tscores_dp = evaluate_model(model, x, y)\n",
    "\t\t# store the results\n",
    "\t\tresults_dp.append(scores_dp)\n",
    "\t\tnames_dp.append(name)\n",
    "\t\t# summarize the performance along the way\n",
    "\t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_dp), std(scores_dp)))\n",
    "\n",
    "\n",
    "\tmean_dp = np.mean(results_dp,axis=1)\n",
    "\tmin_error_dp = np.min(mean_dp)\n",
    "\tprint(min_error_dp)\n",
    "\tbst_n_estimators_dp= np.argmin(mean_dp)\n",
    "\tprint(f\"The minimum RMSE obtained is {min_error_dp:.3f}\")\n",
    "\tplt.plot(names_dp,mean_dp,\"b.-\")\n",
    "\tplt.plot([bst_n_estimators_dp, bst_n_estimators_dp], [0, min_error_dp], \"k--\",linewidth=1)\n",
    "\tplt.plot([-1, 12-1], [min_error_dp, min_error_dp], \"k--\",linewidth=1)\n",
    "\tplt.plot(bst_n_estimators_dp, min_error_dp, \"ko\",linewidth = 1)\n",
    "\tplt.text(bst_n_estimators_dp, min_error_dp*1.2, \"Minimum\", ha=\"center\", fontsize=12)\n",
    "\tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "\tplt.xlim(0,12-1)\n",
    "\tplt.ylim(0,3)\n",
    "\tplt.xlabel(\"Tree Depth\")\n",
    "\tplt.ylabel(\"RMSE\")\n",
    "\tplt.title(\"Validation error\", fontsize=13)\n",
    "\tplt.text(4,2.5, r'def_param : \\tt{max_depth = None}', bbox={'facecolor' : 'white','alpha':0.5})\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_curve(x_test,y_test,regressor='dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_curve(x_test,y_test,regressor='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_curve(x_test,y_test,regressor='et')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for RMSE and Rsquared plots\n",
    "from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit ,learning_curve\n",
    "\n",
    "def learn_plotrmse(model,x_data,y_data,model_name):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator = model, X=x_data, y=y_data,\n",
    "                                                        cv=5, train_sizes=np.linspace(0.1, 1.0, 20),\n",
    "                                                        n_jobs=-1,scoring=\"neg_root_mean_squared_error\")\n",
    "    plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    })\n",
    "    #\n",
    "    # Calculate training and test mean and std\n",
    "    #\n",
    "    train_mean = -np.mean(train_scores, axis=1)\n",
    "    train_std = -np.std(train_scores, axis=1)\n",
    "    test_mean = -np.mean(test_scores, axis=1)\n",
    "    test_std = -np.std(test_scores, axis=1)\n",
    "    #\n",
    "    # Plot the learning curve\n",
    "    #\n",
    "    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label=r'Training Error')\n",
    "    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "    plt.plot(train_sizes, test_mean, color='green', marker='d', markersize=5, linestyle='--', label=r'Validation Error')\n",
    "    plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "    plt.title(fr'Learning Curve of {model_name}')\n",
    "    plt.xlabel(r'Training Data Size')\n",
    "    plt.ylabel(r'RMSE')\n",
    "    plt.ylim(0,2)\n",
    "    plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "def learn_plotr2(model,x_data,y_data,model_name):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator = model, X=x_data, y=y_data,\n",
    "                                                        cv=5, train_sizes=np.linspace(0.1, 1.0, 20),\n",
    "                                                        n_jobs=-1,scoring=\"r2\")\n",
    "    plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    })\n",
    "    #\n",
    "    # Calculate training and test mean and std\n",
    "    #\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    #\n",
    "    # Plot the learning curve\n",
    "    #\n",
    "    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label=r'Training Score')\n",
    "    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "    plt.plot(train_sizes, test_mean, color='green', marker='d', markersize=5, linestyle='--', label=r'Validation Score')\n",
    "    plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "    plt.title(fr'Learning Curve of {model_name}')\n",
    "    plt.xlabel(r'Training Data Size')\n",
    "    plt.ylabel(r'Rsquared')\n",
    "    plt.ylim(0.5,1)\n",
    "    plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_plotrmse(model_dtr,x_train,y_train,'DTR')\n",
    "learn_plotr2(model_dtr,x_train,y_train,'DTR')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve HPO DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_plotrmse(model_dtr_hpov,x_train,y_train,'DTR + HPO')\n",
    "learn_plotr2(model_dtr_hpov,x_train,y_train,'DTR + HPO')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_plotrmse(model_rfr_ftr,x_train,y_train,'RFR')\n",
    "learn_plotr2(model_rfr_ftr,x_train,y_train,'RFR')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve RFR + HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_plotrmse(model_rfr_ftr_hpov,x_train,y_train,'RFR + HPO')\n",
    "learn_plotr2(model_rfr_ftr_hpov,x_train,y_train,'RFR + HPO')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve ETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_plotrmse(model_etr,x_train,y_train,'ETR')\n",
    "learn_plotr2(model_etr,x_train,y_train,'ETR')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve ETR + HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_plotrmse(model_etr_hpov,x_train,y_train,'ETR + HPO')\n",
    "learn_plotr2(model_etr_hpov,x_train,y_train,'ETR + HPO')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plot for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "})\n",
    "scores = [r2_rf,r2_et,r2_dt,r2_mlr]\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "# Creating axes instance\n",
    "# ax = fig.add_axes([0, 0, 1, 1])\n",
    "fig,ax = plt.subplots()\n",
    "plt.title(\"Test Data Model Performance\")\n",
    "plt.ylabel(\"R-squared\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.yscale('linear')\n",
    "plt.ylim(0.2,1.0)\n",
    "plt.boxplot(scores,showmeans=True)\n",
    "plt.grid(axis='y',linestyle = '--', linewidth = 0.5)\n",
    "ax.set_xticklabels(['RFR', 'ETR',\n",
    "                    'DTR', 'MLR'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "})\n",
    "scores_rmse = [np.abs(RMSE_rf),np.abs(RMSE_et),np.abs(RMSE_dt),np.abs(RMSE_mlr)]\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "# Creating axes instance\n",
    "fig,ax = plt.subplots() \n",
    "plt.title(\"Test Data Model Performance\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylim(0,1.2)\n",
    "plt.yscale('linear')\n",
    "ax.boxplot(scores_rmse,showmeans=True)\n",
    "plt.grid(axis='y',linestyle = '--', linewidth = 0.5)\n",
    "ax.set_xticklabels(['RFR', 'ETR',\n",
    "                    'DTR', 'MLR'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the model to local directory\n",
    "\n",
    "# filename = 'savemodel_rfr_ftr.sav'\n",
    "# joblib.dump(model_rfr_ftr,filename)\n",
    "\n",
    "# filename = 'savemodel_rf.sav'\n",
    "# joblib.dump(model_rf,filename)\n",
    "\n",
    "# # filename = 'savemodel_rfr_ftr_hpo.sav'\n",
    "# # joblib.dump(model_rfr_ftr_hpo,filename)\n",
    "\n",
    "# filename = 'savemodel_rfr_ftr_hpov.sav'\n",
    "# joblib.dump(model_rfr_ftr_hpov,filename)\n",
    "\n",
    "# filename = 'savemodel_etr.sav'\n",
    "# joblib.dump(model_etr,filename)\n",
    "\n",
    "# filename = 'savemodel_etr_hpov.sav'\n",
    "# joblib.dump(model_etr_hpov,filename)\n",
    "\n",
    "# filename = 'savemodel_dtr_ftr.sav'\n",
    "# joblib.dump(model_dtr,filename)\n",
    "\n",
    "# filename = 'savemodel_dtr_hpov.sav'\n",
    "# joblib.dump(model_dtr_hpov,filename)\n",
    "\n",
    "# filename = 'savemodel_mlr_ftr.sav'\n",
    "# joblib.dump(model_mlr,filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geron1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
