{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=3, figsize=(18, 7.2), sharey=True)\n",
    "# plt.sca(axes[0])\n",
    "# poly_act_yr = poly_reg_best_fit(stw_act_yr,FOC_act_yr)\n",
    "# plt.sca(axes[1])\n",
    "# poly_act_w = poly_reg_best_fit(stw_act_w,FOC_act_w)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.sca(axes[2])\n",
    "# poly_act_s = poly_reg_best_fit(stw_act_s,FOC_act_s)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=3, figsize=(18, 7.2), sharey=True)\n",
    "# plt.sca(axes[0])\n",
    "# poly_etr_yr = poly_reg_best_fit(stw_pred_et_yr,FOC_pred_yr)\n",
    "# plt.sca(axes[1])\n",
    "# poly_etr_w = poly_reg_best_fit(stw_pred_et_w,FOC_pred_w)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.sca(axes[2])\n",
    "# poly_etr_s = poly_reg_best_fit(stw_pred_et_s,FOC_pred_s)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=3, figsize=(15, 6), sharey=True)\n",
    "# plt.sca(axes[0])\n",
    "# poly_dtr_yr = poly_reg_best_fit(stw_pred_dt_yr,FOC_pred_dt_yr)\n",
    "# plt.sca(axes[1])\n",
    "# poly_dtr_w = poly_reg_best_fit(stw_pred_dt_w,FOC_pred_dt_w)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.sca(axes[2])\n",
    "# poly_dtr_s = poly_reg_best_fit(stw_pred_dt_s,FOC_pred_dt_s)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=3, figsize=(20, 8), sharey=True)\n",
    "# plt.sca(axes[0])\n",
    "# poly_rfr_yr = poly_reg_best_fit(stw_pred_rf_yr,FOC_pred_rf_yr)\n",
    "# plt.sca(axes[1])\n",
    "# poly_rfr_w = poly_reg_best_fit(stw_pred_rf_w,FOC_pred_rf_w)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.sca(axes[2])\n",
    "# poly_rfr_s = poly_reg_best_fit(stw_pred_rf_s,FOC_pred_rf_s)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Resistance_plot(RFric,RApp,RWave,RBulb,RTrans,RCorr,RWind,RStaWave,RTot):\n",
    "#     rmse_opt = [RFric/1e3,RApp/1e3,RWave/1e3,RBulb/1e3,RTrans/1e3,RCorr/1e3,RWind/1e3,RStaWave/1e3,RTot/1e3]\n",
    "#     # fig = plt.figure(figsize=(5,3))\n",
    "#     # Creating axes instance\n",
    "#     fig,ax = plt.subplots() \n",
    "#     plt.title(\"Resistance Encountered by ship\")\n",
    "#     plt.ylabel(\"Force [kN]\")\n",
    "#     plt.xlabel(\"Resistance Type\")\n",
    "#     plt.yscale('linear')\n",
    "#     ax.boxplot(rmse_opt,showmeans=True)\n",
    "#     plt.grid(axis='y',linestyle = '--', linewidth = 0.5)\n",
    "#     ax.set_xticklabels([r'$R_F$', r'$R_{APP}$',\n",
    "#                         r'$R_W$', r'$R_{B}$',\n",
    "#                         r'$R_{TR}$',r'$R_{A}$',\n",
    "#                         r'$R_{AA}$',r'$R_{AWL}$',\n",
    "#                         r'$R_{TOT}$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function defines the model performance for the given stw range\n",
    "\n",
    "# def evaluate_FOC(model,FOC_act,FOC_pred):\n",
    "#     from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,r2_score,explained_variance_score,median_absolute_error,mean_absolute_error\n",
    "    \n",
    "#     Rsquared_FOC = r2_score(FOC_act,FOC_pred)\n",
    "#     expVar_FOC = explained_variance_score(FOC_act,FOC_pred)\n",
    "#     MAE_FOC = mean_absolute_error(FOC_act,FOC_pred)\n",
    "#     RMSE_FOC = np.sqrt(mean_squared_error(FOC_act, FOC_pred))\n",
    "#     MAD_FOC = median_absolute_error(FOC_act,FOC_pred)\n",
    "#     MAPE_FOC = mean_absolute_percentage_error(FOC_act, FOC_pred)\n",
    "    \n",
    "#     n = len(FOC_act)\n",
    "#     num_params = model.n_features_in_\n",
    "#     aic = n * math.log(mean_squared_error(FOC_act, FOC_pred)) + 2 * num_params\n",
    "\n",
    "#     print(f\"Model Performance of {model}\")\n",
    "#     print(f\"AIC {aic:0.4f}\")\n",
    "#     print(f\"R^2 {Rsquared_FOC:0.4f}\")\n",
    "#     print(f\"Explained Variance {expVar_FOC:0.4f}\")\n",
    "#     print(f\"MAE {MAE_FOC:0.4f} T/h\")    \n",
    "#     print(f\"RMSE FOC {RMSE_FOC:0.4f} T/h\")\n",
    "#     print(f\"MAD {MAD_FOC:0.4f} T/h\")    \n",
    "#     print(f\"MAPE FOC {MAPE_FOC*100:0.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phased Out polyfit evaluation<br>\n",
    "\n",
    "-Good model expected, not represntative of the actual model performance\n",
    "-Good estimates for fast estimation, but must be tested with real data to see its effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def FOC_coeff_plot(stw_predicted,stw_actual,FOC_predicted,FOC_actual):\n",
    "\n",
    "#     slow_steam = 0.2*9760*(169.4/1e6)\n",
    "#     normal = 0.65*9760*(169.4/1e6)\n",
    "#     max_Pb = 9760*(169.4/1e6)\n",
    "\n",
    "#     # To predict FOC\n",
    "\n",
    "#     Xp = stw_predicted\n",
    "#     Yp = FOC_predicted\n",
    "\n",
    "#     Xa = stw_actual\n",
    "#     Ya = FOC_actual\n",
    "\n",
    "#     coefs_pred = np.polyfit(Xp, Yp, 4)\n",
    "#     coefs_act = np.polyfit(Xa, Ya, 4)\n",
    "\n",
    "\n",
    "#     print(\"Coefficients for FOC curve\")\n",
    "#     print(coefs_pred)\n",
    "#     print(coefs_act)\n",
    "\n",
    "#     p_pred = np.poly1d(coefs_pred)\n",
    "#     p_act = np.poly1d(coefs_act)\n",
    "\n",
    "#     plt.scatter(Xa, Ya,marker='o',linewidths=.8,edgecolors='orange',facecolor='none', label = 'Actual STW',s=12 )\n",
    "#     plt.scatter(Xp, Yp,marker='x',linewidths=.5,c='black',label = 'Predicted STW',s=12 )\n",
    "\n",
    "#     sorted_pred= np.sort(Xp)\n",
    "#     sorted_act= np.sort(Xa)\n",
    "\n",
    "#     plt.plot(sorted_pred, p_pred(sorted_pred), linestyle = '-',color = 'b',\n",
    "#             label=rf'$y = ({coefs_pred[0]:.3G})x^4 {coefs_pred[1]:.3G}x^3 + {coefs_pred[2]:.3G}x^2 {coefs_pred[3]:.3G}x + {coefs_pred[4]:.3G}$') #p(X) evaluates the polynomial at X\n",
    "# #     plt.plot(sorted_act, p_act(sorted_act), linestyle = \"-.\" , color = 'red',\n",
    "# #             label=rf'$y = ({coefs_act[0]:.3G})x^4 {coefs_act[1]:.3G}x^3 + {coefs_act[2]:.3G}x^2 {coefs_act[3]:.3G}x + {coefs_act[4]:.3G}$') #p(X) evaluates the polynomial at X\n",
    "#     plt.title(\"FOC Plot\")\n",
    "#     plt.xlabel(r'STW [$knots$]', fontsize=13)\n",
    "#     plt.ylabel(r'FOC [$T/h$]', fontsize=13)\n",
    "\n",
    "#     plt.axhline(y=slow_steam,linestyle = 'dotted',c='k')\n",
    "#     plt.axhline(y=normal,linestyle = 'dotted',c='k')\n",
    "#     plt.axhline(y=max_Pb,linestyle = 'dotted',c='k')\n",
    "\n",
    "#     plt.text(6.1,1.1*slow_steam,'Slow Steaming',rotation=360)\n",
    "#     plt.text(6.1,1.03*normal,'Normal Crusing',rotation=360)\n",
    "#     plt.text(6.1,1.01*max_Pb,'Max Power',rotation=360)\n",
    "\n",
    "\n",
    "#     plt.xlim(6,21)\n",
    "#     plt.ylim(0,2)\n",
    "#     plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "#     # plt.legend(bbox_to_anchor=(0.5,-.4),loc=\"lower left\")\n",
    "#     plt.legend(loc=\"lower left\",bbox_to_anchor=(-0.025, -0.34))\n",
    "#     # plt.show()\n",
    "\n",
    "#     return coefs_act,coefs_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def Pb_plot(stw_predicted,stw_actual,Pb_predicted,Pb_actual):\n",
    "\n",
    "#         # To predict Power\n",
    "\n",
    "#         # According to 3rd IMO GHG study\n",
    "\n",
    "#         slow_steam = 0.2*9760\n",
    "#         normal = 0.65*9760\n",
    "#         max_Pb = 9760\n",
    "\n",
    "#         Xp = stw_predicted\n",
    "#         Yp = Pb_predicted\n",
    "\n",
    "#         Xa = stw_actual\n",
    "#         Ya = Pb_actual\n",
    "\n",
    "#         coefs_pred = np.polyfit(Xp, Yp, 4)\n",
    "#         coefs_act = np.polyfit(Xa, Ya, 4)\n",
    "\n",
    "#         print(\"Coefficients for power curve\")\n",
    "#         print(coefs_pred)\n",
    "#         print(coefs_act)  \n",
    "        \n",
    "#         p_pred = np.poly1d(coefs_pred)\n",
    "#         p_act = np.poly1d(coefs_act)   \n",
    "\n",
    "#         sorted_pred= np.sort(Xp)\n",
    "#         sorted_act= np.sort(Xa)\n",
    " \n",
    "#         plt.scatter(Xa, Ya,marker='x',linewidths=.5,edgecolors='black', label = 'Actual STW',s=12 )\n",
    "#         plt.scatter(Xp, Yp,marker='D',linewidths=.5,facecolors='none',edgecolors='blue',label = 'Predicted STW',s=12 )\n",
    "\n",
    "#         plt.plot(sorted_pred, p_pred(sorted_pred),color = 'red',\n",
    "#         label=rf'$y = {coefs_pred[0]:.3f}x^4 {coefs_pred[1]:.3f}x^3 + {coefs_pred[2]:.3f}x^2 {coefs_pred[3]:.3f}x + {coefs_pred[4]:.3f}$') #p(X) evaluates the polynomial at X\n",
    "\n",
    "#         plt.title(\"Predicted vs Actual\")\n",
    "#         plt.xlabel(r'STW [$knots$]', fontsize=13)\n",
    "#         plt.ylabel(r'Brake Power [$kW$]', fontsize=13)\n",
    "\n",
    "#         plt.title(\"Power Plot\")\n",
    "\n",
    "#         plt.axhline(y=slow_steam,linestyle = 'dotted',c='k',alpha=0.3)\n",
    "#         plt.axhline(y=normal,linestyle = 'dotted',c='k',alpha=0.3)\n",
    "#         plt.axhline(y=max_Pb,linestyle = 'dotted',c='k',alpha=0.3)\n",
    "\n",
    "#         plt.text(6.1,2100,'Slow Steaming',rotation=360,alpha=0.3)\n",
    "#         plt.text(6.1,6500,'Normal Crusing',rotation=360,alpha=0.3)\n",
    "#         plt.text(6.1,9900,'Max Power',rotation=360,alpha=0.3)\n",
    "\n",
    "\n",
    "#         plt.xlim(6,21)\n",
    "#         plt.ylim(0,12000)\n",
    "#         plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "#         plt.legend(loc=\"lower left\",bbox_to_anchor=(-.025, -0.34))\n",
    "\n",
    "#         # return coefs_act,coefs_pred\n",
    "\n",
    "#         # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharey=False)\n",
    "# plt.sca(axes[0])\n",
    "# coefs_act_yr,coefs_pred_et_yr = FOC_coeff_plot(stw_pred_et_yr,stw_act_yr,FOC_pred_yr,FOC_act_yr)\n",
    "# plt.sca(axes[1])\n",
    "# Pb_plot(stw_pred_et_yr,stw_act_yr,P_b_pred_yr,P_b_act_yr)\n",
    "# # plt.ylabel(\"\")\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharey=False)\n",
    "# plt.sca(axes[0])\n",
    "# coefs_act_s,coefs_pred_et_s = FOC_coeff_plot(stw_pred_et_s,stw_act_s,FOC_pred_s,FOC_act_s)\n",
    "# plt.sca(axes[1])\n",
    "# coefs_act_w,coefs_pred_et_w = FOC_coeff_plot(stw_pred_et_w,stw_act_w,FOC_pred_w,FOC_act_w)\n",
    "# # plt.ylabel(\"\")\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharey=False)\n",
    "# plt.sca(axes[0])\n",
    "# Pb_plot(stw_pred_et_s,stw_act_s,P_b_pred_s,P_b_act_s)\n",
    "# plt.sca(axes[1])\n",
    "# Pb_plot(stw_pred_et_w,stw_act_w,P_b_pred_w,P_b_act_w)\n",
    "# # plt.ylabel(\"\")\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharey=False)\n",
    "# plt.sca(axes[0])\n",
    "# coefs_act_s,coefs_pred_rf_s = FOC_coeff_plot(stw_pred_rf_s,stw_act_s,FOC_pred_rf_s,FOC_act_s)\n",
    "# plt.sca(axes[1])\n",
    "# coefs_act_w,coefs_pred_rf_w = FOC_coeff_plot(stw_pred_rf_w,stw_act_w,FOC_pred_rf_w,FOC_act_w)\n",
    "# # plt.ylabel(\"\")\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharey=False)\n",
    "# plt.sca(axes[0])\n",
    "# Pb_plot(stw_pred_rf_s,stw_act_s,P_b_pred_rf_s,P_b_act_s)\n",
    "# plt.sca(axes[1])\n",
    "# Pb_plot(stw_pred_rf_w,stw_act_w,P_b_pred_rf_w,P_b_act_w)\n",
    "# # plt.ylabel(\"\")\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharey=False)\n",
    "# plt.sca(axes[0])\n",
    "# coefs_act_s,coefs_pred_dt_s = FOC_coeff_plot(stw_pred_dt_s,stw_act_s,FOC_pred_dt_s,FOC_act_s)\n",
    "# plt.sca(axes[1])\n",
    "# coefs_act_w,coefs_pred_dt_w = FOC_coeff_plot(stw_pred_dt_w,stw_act_w,FOC_pred_dt_w,FOC_act_w)\n",
    "# # plt.ylabel(\"\")\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharey=False)\n",
    "# plt.sca(axes[0])\n",
    "# Pb_plot(stw_pred_dt_s,stw_act_s,P_b_pred_dt_s,P_b_act_s)\n",
    "# plt.sca(axes[1])\n",
    "# Pb_plot(stw_pred_dt_w,stw_act_w,P_b_pred_dt_w,P_b_act_w)\n",
    "# # plt.ylabel(\"\")\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharey=False)\n",
    "# plt.sca(axes[0])\n",
    "# coefs_act_s,coefs_pred_mlr_s = FOC_coeff_plot(stw_pred_mlr_s,stw_act_s,FOC_pred_mlr_s,FOC_act_s)\n",
    "# plt.sca(axes[1])\n",
    "# coefs_act_w,coefs_pred_mlr_w = FOC_coeff_plot(stw_pred_mlr_w,stw_act_w,FOC_pred_mlr_w,FOC_act_w)\n",
    "# # plt.ylabel(\"\")\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 5), sharey=False)\n",
    "# plt.sca(axes[0])\n",
    "# Pb_plot(stw_pred_mlr_s,stw_act_s,P_b_pred_mlr_s,P_b_act_s)\n",
    "# plt.sca(axes[1])\n",
    "# Pb_plot(stw_pred_mlr_w,stw_act_w,P_b_pred_mlr_w,P_b_act_w)\n",
    "# # plt.ylabel(\"\")\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # # Simulate model performance by inserting arbitrary STW in knots\n",
    "\n",
    "# # # Complete Random number with no distribution\n",
    "\n",
    "# # def generate_random_integers(min_value, max_value, n):\n",
    "# #     random_numbers = [random.uniform(min_value, max_value) for _ in range(n)]\n",
    "# #     return random_numbers\n",
    "# # # speed range, 5-21 kts, simulate 18h journey time in a day over a month so (18*31)\n",
    "# # v_stw_lst = generate_random_integers(19.5,21,558) \n",
    "# # v_stw = np.array(v_stw_lst)\n",
    "# # print(v_stw)\n",
    "\n",
    "# # # Simulate one way trip\n",
    "\n",
    "# # v_stw_lst_day = [17.27621092,18.9989056,19.33427835,18.91104651,13.85493274]\n",
    "# # v_stw = np.array(v_stw_lst_day)\n",
    "\n",
    "# # Skewed distribution of random numbers\n",
    "\n",
    "# # Parameters for the right-skewed normal distribution\n",
    "# mean = 17  # Mean of the distribution\n",
    "# std_dev = -3.176740  # Standard deviation of the distribution\n",
    "# skewness_factor = .3  # Skewness factor to control the skewness\n",
    "\n",
    "# # Range of the random numbers you want to generate\n",
    "# minimum = 5\n",
    "# maximum = 21\n",
    "\n",
    "# # Generate random numbers with a standard normal distribution\n",
    "# n = 558  # Number of random numbers to generate\n",
    "# standard_normal_values = np.random.randn(n)\n",
    "\n",
    "# # Transform the standard normal distribution into a right-skewed distribution\n",
    "# right_skewed_values = mean + std_dev * np.exp(skewness_factor * standard_normal_values)\n",
    "\n",
    "# # Scale and shift the values to the desired range\n",
    "# scaled_values = (right_skewed_values - np.min(right_skewed_values)) / (np.max(right_skewed_values) - np.min(right_skewed_values))\n",
    "# v_stw = minimum + scaled_values * (maximum - minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(v_stw,bins=25,color='black')\n",
    "# plt.grid(True,linestyle = '--', linewidth = 0.5)\n",
    "# plt.xlabel(r\"STW [$kts$]\",fontsize=10)\n",
    "# plt.title(r\"Profile of dummy STW\", fontsize=14)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Summer\n",
    "# FOC_poly_act_s = coefs_act_s[0]*v_stw**4 + coefs_act_s[1] * v_stw**3 + coefs_act_s[2] * v_stw**2 + coefs_act_s[3]*v_stw + coefs_act_s[4]\n",
    "# #Winter\n",
    "# FOC_poly_act_w = coefs_act_w[0]*v_stw**4 + coefs_act_w[1] * v_stw**3 + coefs_act_w[2] * v_stw**2 + coefs_act_w[3]*v_stw + coefs_act_s[4]\n",
    "# #Year\n",
    "# FOC_poly_act_yr = coefs_act_yr[0]*v_stw**4 + coefs_act_yr[1] * v_stw**3 + coefs_act_yr[2] * v_stw**2 + coefs_act_yr[3]*v_stw + coefs_act_yr[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Summer\n",
    "# FOC_poly_pred_s = coefs_pred_et_s[0]*v_stw**4 + coefs_pred_et_s[1] * v_stw**3 + coefs_pred_et_s[2] * v_stw**2 + coefs_pred_et_s[3]*v_stw + coefs_pred_et_s[4]\n",
    "# #Winter\n",
    "# FOC_poly_pred_w = coefs_pred_et_w[0]*v_stw**4 + coefs_pred_et_w[1] * v_stw**3 + coefs_pred_et_w[2] * v_stw**2 + coefs_pred_et_w[3]*v_stw + coefs_pred_et_w[4]\n",
    "# #Yearly\n",
    "# FOC_poly_pred_yr = coefs_pred_et_yr[0]*v_stw**4 + coefs_pred_et_yr[1] * v_stw**3 + coefs_pred_et_yr[2] * v_stw**2 + coefs_pred_et_yr[3]*v_stw + coefs_pred_et_yr[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Summer\n",
    "# FOC_poly_rf_s = coefs_pred_rf_s[0]*v_stw**4 + coefs_pred_rf_s[1] * v_stw**3 + coefs_pred_rf_s[2] * v_stw**2 + coefs_pred_rf_s[3]*v_stw + coefs_pred_rf_s[4]\n",
    "# #Winter\n",
    "# FOC_poly_rf_w = coefs_pred_rf_w[0]*v_stw**4 + coefs_pred_rf_w[1] * v_stw**3 + coefs_pred_rf_w[2] * v_stw**2 + coefs_pred_rf_w[3]*v_stw + coefs_pred_rf_w[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Summer\n",
    "# FOC_poly_dt_s = coefs_pred_dt_s[0]*v_stw**4 + coefs_pred_dt_s[1] * v_stw**3 + coefs_pred_dt_s[2] * v_stw**2 + coefs_pred_dt_s[3]*v_stw + coefs_pred_dt_s[4]\n",
    "# #Winter\n",
    "# FOC_poly_dt_w = coefs_pred_dt_w[0]*v_stw**4 + coefs_pred_dt_w[1] * v_stw**3 + coefs_pred_dt_w[2] * v_stw**2 + coefs_pred_dt_w[3]*v_stw + coefs_pred_dt_w[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Summer\n",
    "# FOC_poly_mlr_s = coefs_pred_mlr_s[0]*v_stw**4 + coefs_pred_mlr_s[1] * v_stw**3 + coefs_pred_mlr_s[2] * v_stw**2 + coefs_pred_mlr_s[3]*v_stw + coefs_pred_mlr_s[4]\n",
    "# #Winter\n",
    "# FOC_poly_mlr_w = coefs_pred_mlr_w[0]*v_stw**4 + coefs_pred_mlr_w[1] * v_stw**3 + coefs_pred_mlr_w[2] * v_stw**2 + coefs_pred_mlr_w[3]*v_stw + coefs_pred_mlr_w[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This model defines the performance of the generated polyfit line from the model\n",
    "\n",
    "# def evaluate_FOC_poly(model,FOC_poly_act,FOC_poly_pred):\n",
    "#     from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error,r2_score,explained_variance_score,median_absolute_error,mean_absolute_error\n",
    "    \n",
    "#     Rsquared_ply = r2_score(FOC_poly_act,FOC_poly_pred)\n",
    "#     expVar_ply = explained_variance_score(FOC_poly_act,FOC_poly_pred)\n",
    "#     MAE_ply = mean_absolute_error(FOC_poly_act,FOC_poly_pred)\n",
    "#     RMSE_ply = np.sqrt(mean_squared_error(FOC_poly_act,FOC_poly_pred))\n",
    "#     MAD_ply = median_absolute_error(FOC_poly_act,FOC_poly_pred)\n",
    "#     MAPE_ply = mean_absolute_percentage_error(FOC_poly_act,FOC_poly_pred)\n",
    "    \n",
    "#     n = len(FOC_poly_act)\n",
    "#     num_params = 4 + 1 # 4th order model is best fit\n",
    "#     aic_ply = n * np.log(mean_squared_error(FOC_poly_act,FOC_poly_pred)) + 2 * num_params\n",
    "\n",
    "#     print(f\"Model Performance of {model}\")\n",
    "#     print(f\"AIC {aic_ply:0.4f}\")\n",
    "#     print(f\"R^2 {Rsquared_ply:0.4f}\")\n",
    "#     print(f\"Explained Variance {expVar_ply:0.4f}\")\n",
    "#     print(f\"MAE {MAE_ply:0.4f} T/h\")    \n",
    "#     print(f\"RMSE FOC {RMSE_ply:0.4f} T/h\")\n",
    "#     print(f\"MAD {MAD_ply:0.4f} T/h\")    \n",
    "#     print(f\"MAPE FOC {MAPE_ply*100:0.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Yearly\n",
    "# evaluate_FOC_poly(model_etr_hpov,FOC_poly_act_yr,FOC_poly_pred_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summer\n",
    "# evaluate_FOC_poly(model_etr_hpov,FOC_poly_act_s,FOC_poly_pred_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Winter\n",
    "# evaluate_FOC_poly(model_etr_hpov,FOC_poly_act_w,FOC_poly_pred_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summer\n",
    "# evaluate_FOC_poly(model_rfr_hpov,FOC_poly_act_s,FOC_poly_rf_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Winter\n",
    "# evaluate_FOC_poly(model_rfr_hpov,FOC_poly_act_w,FOC_poly_rf_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summer\n",
    "# evaluate_FOC_poly(model_dtr_hpov,FOC_poly_act_s,FOC_poly_dt_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Winter\n",
    "# evaluate_FOC_poly(model_dtr_hpov,FOC_poly_act_w,FOC_poly_dt_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summer\n",
    "# evaluate_FOC_poly(model_mlr_ftr,FOC_poly_act_s,FOC_poly_mlr_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Winter\n",
    "# evaluate_FOC_poly(model_mlr_ftr,FOC_poly_act_w,FOC_poly_mlr_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best plots from GPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_Xreg = np.sort(stw_pred_et_yr)\n",
    "# sorted_Yreg = np.sort(FOC_pred_yr)\n",
    "\n",
    "# Xreg = sorted_Xreg.reshape(-1,1)\n",
    "# Yreg = sorted_Yreg\n",
    "\n",
    "# Xreg_train, Xreg_test, Yreg_train, Yreg_test = train_test_split(Xreg, Yreg, test_size=0.2, random_state=42)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# best_order = None\n",
    "# best_score = -np.inf\n",
    "# best_model = None\n",
    "\n",
    "# for order in range(1, 6):\n",
    "#     # Create polynomial features for the current order\n",
    "#     poly = PolynomialFeatures(degree=order)\n",
    "#     X_poly_train = poly.fit_transform(Xreg_train)\n",
    "    \n",
    "#     # Fit the linear regression model and perform cross-validation\n",
    "#     model = LinearRegression()\n",
    "#     scores = cross_val_score(model, X_poly_train, Yreg_train, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "#     # Calculate the mean cross-validation score\n",
    "#     mean_score = np.mean(scores)\n",
    "    \n",
    "#     # Check if the current model is the best model so far\n",
    "#     if mean_score > best_score:\n",
    "#         best_order = order\n",
    "#         best_score = mean_score\n",
    "#         best_model = model\n",
    "#     print(f\"Best order {best_order}\")\n",
    "\n",
    "# # Fit the best model on the entire training data\n",
    "# X_poly_train_best = PolynomialFeatures(degree=best_order).fit_transform(Xreg_train)\n",
    "# best_model.fit(X_poly_train_best, Yreg_train)\n",
    "\n",
    "# coefficients = best_model.coef_\n",
    "# intercept = best_model.intercept_\n",
    "\n",
    "# print(\"Best Model Coefficients:\", coefficients)\n",
    "# print(\"Best Model Intercept:\", intercept)\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# train_errors = []\n",
    "# test_errors = []\n",
    "\n",
    "# # Loop through different orders\n",
    "# for order in range(1, 6):\n",
    "#     # Create polynomial features for the current order\n",
    "#     poly = PolynomialFeatures(degree=order)\n",
    "#     X_poly_train = poly.fit_transform(Xreg_train)\n",
    "#     X_poly_test = poly.transform(Xreg_test)\n",
    "\n",
    "#     # Fit the linear regression model\n",
    "#     model = LinearRegression()\n",
    "#     model.fit(X_poly_train, Yreg_train)\n",
    "\n",
    "#     # Make predictions on training and test data\n",
    "#     y_pred_train = model.predict(X_poly_train)\n",
    "#     y_pred_test = model.predict(X_poly_test)\n",
    "\n",
    "#     # Calculate mean squared errors for training and test data\n",
    "#     train_error = mean_squared_error(Yreg_train, y_pred_train)\n",
    "#     test_error = mean_squared_error(Yreg_test, y_pred_test)\n",
    "\n",
    "#     # Append the errors to the lists\n",
    "#     train_errors.append(train_error)\n",
    "#     test_errors.append(test_error)\n",
    "\n",
    "# orders = range(1, 6)\n",
    "\n",
    "# plt.plot(orders, train_errors, label='Train Error')\n",
    "# plt.plot(orders, test_errors, label='Test Error')\n",
    "# plt.xlabel('Polynomial Order')\n",
    "# plt.ylabel('Mean Squared Error')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# # Create an empty list to store the fitted models and their scores\n",
    "# models_poly = []\n",
    "# scores_poly = []\n",
    "\n",
    "# # Loop through different orders\n",
    "# for order in range(1, 6):\n",
    "#     # Create polynomial features for the current order\n",
    "#     poly = PolynomialFeatures(degree=order)\n",
    "#     X_poly = poly.fit_transform(Xreg)\n",
    "    \n",
    "#     # Fit the linear regression model\n",
    "#     model = LinearRegression()\n",
    "#     model.fit(X_poly, Yreg)\n",
    "    \n",
    "#     # Calculate the score (R-squared) of the model\n",
    "#     score = model.score(X_poly, Yreg)\n",
    "    \n",
    "#     # Append the model and score to the lists\n",
    "#     models_poly.append(model)\n",
    "#     scores_poly.append(score)\n",
    "#     print(scores_poly)\n",
    "\n",
    "# best_order = np.argmax(scores_poly) + 1\n",
    "# best_model = models_poly[best_order - 1]\n",
    "\n",
    "# Ypoly_pred = best_model.predict(X_poly)\n",
    "\n",
    "# plt.scatter(Xreg, Yreg, label='Data Points')\n",
    "# plt.plot(Xreg, Ypoly_pred, color='red', label=f'Polynomial Regression (Order {best_order})')\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('y')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "\n",
    "    # # Create an empty list to store the fitted models and their scores\n",
    "    # models_poly = []\n",
    "    # scores_poly = []\n",
    "\n",
    "    # # Loop through different orders\n",
    "    # for order in range(1, 6):\n",
    "    #     # Create polynomial features for the current order\n",
    "    #     poly = PolynomialFeatures(degree=order)\n",
    "    #     X_poly = poly.fit_transform(Xreg)\n",
    "        \n",
    "    #     # Fit the linear regression model\n",
    "    #     model = LinearRegression()\n",
    "    #     model.fit(X_poly, Yreg)\n",
    "        \n",
    "    #     # Calculate the score (R-squared) of the model\n",
    "    #     score = model.score(X_poly, Yreg)\n",
    "        \n",
    "    #     # Append the model and score to the lists\n",
    "    #     models_poly.append(model)\n",
    "    #     scores_poly.append(score)\n",
    "    #     print(scores_poly)\n",
    "\n",
    "    # best_order = np.argmax(scores_poly) + 1\n",
    "    # best_model = models_poly[best_order - 1]\n",
    "    # Ypoly_pred = best_model.predict(X_poly)\n",
    "\n",
    "    # plt.scatter(Xreg, Yreg,marker='D',linewidths=.5,facecolors='none',edgecolors='blue', label='Data Points')\n",
    "    # plt.plot(Xreg, Ypoly_pred, color='red', label=f'Polynomial Regression (Order {best_order})')\n",
    "    # plt.xlabel('STW [kn]')\n",
    "    # plt.ylabel('FOC [T/h]')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # coefficients = best_model.coef_\n",
    "    # intercept = best_model.intercept_\n",
    "\n",
    "    \n",
    "\n",
    "    # print(\"Best Model Coefficients:\", coefficients)\n",
    "    # print(\"Best Model Intercept:\", intercept)\n",
    "\n",
    "    # return coefficients,intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old feature importances plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_feature_importance(model_type,names,model_name):\n",
    "\n",
    "#     importance = model_type.feature_importances_\n",
    "\n",
    "#     #Create arrays from feature importance and feature names\n",
    "#     feature_importance = np.array(importance)\n",
    "#     feature_names = np.array(names)\n",
    "\n",
    "#     #Create a DataFrame using a Dictionary\n",
    "#     data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "#     fi_df = pd.DataFrame(data)\n",
    "\n",
    "#     #Sort the DataFrame in order decreasing feature importance\n",
    "#     fi_sorted = fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "\n",
    "#     #Define size of bar plot\n",
    "#     plt.figure(figsize=(5,4))\n",
    "#     #Plot Searborn bar chart\n",
    "#     sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'],color='black')\n",
    "#     #Add chart labels\n",
    "#     plt.title(f\"Feature Importance of {model_name}\")\n",
    "#     plt.grid(linestyle = '--', linewidth = 0.4,axis='x')\n",
    "#     plt.xlabel('Feature Importance')\n",
    "#     plt.xlim(0,0.7)\n",
    "#     plt.ylabel('Feature Names')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Sample data (replace this with your actual feature names and importances)\n",
    "# feature_names = ['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4']\n",
    "# importance_values = [0.25, 0.35, 0.15, 0.25]\n",
    "\n",
    "# # Sort feature names and importance values in descending order\n",
    "# sorted_indices = sorted(range(len(importance_values)), key=lambda k: importance_values[k], reverse=True)\n",
    "# sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "# sorted_importance_values = [importance_values[i] for i in sorted_indices]\n",
    "\n",
    "# # Create a bar plot for sorted feature importances\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.barh(range(len(sorted_feature_names)), sorted_importance_values, color='skyblue')\n",
    "# plt.yticks(range(len(sorted_feature_names)), sorted_feature_names)\n",
    "# plt.xlabel('Importance Value')\n",
    "# plt.ylabel('Feature Name')\n",
    "# plt.title('Feature Importances (Sorted)')\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Show importance values on the plot\n",
    "# for i, v in enumerate(sorted_importance_values):\n",
    "#     plt.text(v + 0.01, i, str(round(v, 2)), color='black', fontweight='bold')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "\n",
    "# frames_summer = [dfMay,dfJune,dfJuly,dfAug,dfSep,dfOct]\n",
    "# df3months = pd.concat(frames_summer)\n",
    "\n",
    "# df_test_set = df3months\n",
    "\n",
    "###############################################################################\n",
    "# df_test_set = test_set\n",
    "\n",
    "# dfdate6 = df[df['Time'].dt.strftime('%Y-%m') == '2021-06']\n",
    "# dfJuly = df[df['Time'].dt.strftime('%Y-%m') == '2021-07']\n",
    "# dfAug = df[df['Time'].dt.strftime('%Y-%m') == '2021-08']\n",
    "# frames_3mth = [dfdate6,dfJuly,dfAug]\n",
    "# df_test_set = pd.concat(frames_3mth)\n",
    "\n",
    "\n",
    "\n",
    "# dfdate6 = df[df['Time'].dt.strftime('%Y-%m') == '2021-06']\n",
    "# df = df[df['Time'].dt.strftime('%Y-%m') != '2021-06']\n",
    "\n",
    "#############################################################################\n",
    "# # Extract information of a journey from a single day\n",
    "\n",
    "# koege_dep = df[df['Time'].dt.strftime('%Y-%m-%d %H:%M:%S') == '2021-05-31 23:00:00']\n",
    "# koege_arr = dfdate6[dfdate6['Time'].dt.strftime('%Y-%m-%d') == '2021-06-01']\n",
    "\n",
    "# # Drop the departure time for next trip\n",
    "\n",
    "# koege_arr2 = koege_arr.drop(koege_arr.index[14])\n",
    "\n",
    "# # To gain information for a single trip from Koege to Ronne\n",
    "\n",
    "# ronne_arr = koege_arr.drop(koege_arr.index[4:15])\n",
    "\n",
    "# dfday6 = pd.concat([koege_dep,koege_arr2])\n",
    "# dfone6 = pd.concat([koege_dep,ronne_arr])\n",
    "\n",
    "# # For Qgis Single Journey\n",
    "# dfday6.to_csv(\"AIS_01_06_21_koegeronne.csv\")\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Number generator with skew and limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters for the right-skewed normal distribution\n",
    "# mean = 17  # Mean of the distribution\n",
    "# std_dev = -0.2  # Standard deviation of the distribution\n",
    "# skewness_factor = .1  # Skewness factor to control the skewness\n",
    "\n",
    "# # Range of the random numbers you want to generate\n",
    "# minimum = 5\n",
    "# maximum = 21\n",
    "\n",
    "# # Generate random numbers with a standard normal distribution\n",
    "# n = 558  # Number of random numbers to generate\n",
    "# standard_normal_values = np.random.randn(n)\n",
    "\n",
    "# # Transform the standard normal distribution into a right-skewed distribution\n",
    "# right_skewed_values = mean + std_dev * np.exp(skewness_factor * standard_normal_values)\n",
    "\n",
    "# # Scale and shift the values to the desired range\n",
    "# scaled_values = (right_skewed_values - np.min(right_skewed_values)) / (np.max(right_skewed_values) - np.min(right_skewed_values))\n",
    "# random_numbers = minimum + scaled_values * (maximum - minimum)\n",
    "\n",
    "# print(random_numbers)\n",
    "\n",
    "# # print(right_skewed_values)\n",
    "# plt.hist(random_numbers,bins=50)\n",
    "# plt.show()\n",
    "\n",
    "#############################################\n",
    "\n",
    "# from scipy.stats import skewnorm\n",
    "\n",
    "# a, loc, scale = -1.3, -17, 2.2\n",
    "# v_test_stw= skewnorm.rvs(a,loc,scale, size=558)\n",
    "# plt.hist(v_test_stw)\n",
    "# plt.show()\n",
    "\n",
    "##################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twin axis plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot with twin axis\n",
    "\n",
    "# def FOC_coeff_plot(stw_predicted,stw_actual,FOC_predicted,FOC_actual,Pb_predicted,Pb_actual):\n",
    "\n",
    "#         slow_steam = 0.2*9760*(169.4/1e6)\n",
    "#         normal = 0.65*9760*(169.4/1e6)\n",
    "#         max_Pb = 9760*(169.4/1e6)\n",
    "\n",
    "#         # To predict FOC\n",
    "\n",
    "#         Xp = stw_predicted\n",
    "#         Yp = FOC_predicted\n",
    "\n",
    "#         Xa = stw_actual\n",
    "#         Ya = FOC_actual\n",
    "\n",
    "#         coefs_pred = np.polyfit(Xp, Yp, 4)\n",
    "#         coefs_act = np.polyfit(Xa, Ya, 4)\n",
    "\n",
    "#         print(coefs_pred)\n",
    "#         print(coefs_act)\n",
    "\n",
    "#         p_pred = np.poly1d(coefs_pred)\n",
    "#         p_act = np.poly1d(coefs_act)\n",
    "\n",
    "#         fig,ax1 = plt.subplots()\n",
    "\n",
    "#         ax1.scatter(Xa, Ya,marker='o',linewidths=.8,edgecolors='orange',facecolor='none', label = 'Actual STW',s=12 )\n",
    "#         ax1.scatter(Xp, Yp,marker='x',linewidths=.5,c='black',label = 'Predicted STW',s=12 )\n",
    "\n",
    "#         sorted_pred= np.sort(Xp)\n",
    "#         sorted_act= np.sort(Xa)\n",
    "\n",
    "#         ax1.plot(sorted_pred, p_pred(sorted_pred), linestyle = '-',color = 'b',\n",
    "#                 label=rf'$y = ({coefs_pred[0]:.3G})x^4 {coefs_pred[1]:.3G}x^3 + {coefs_pred[2]:.3G}x^2 {coefs_pred[3]:.3G}x + {coefs_pred[4]:.3G}$') #p(X) evaluates the polynomial at X\n",
    "#         ax1.plot(sorted_act, p_act(sorted_act), linestyle = \"-.\" , color = 'red',\n",
    "#                 label=rf'$y = ({coefs_act[0]:.3G})x^4 {coefs_act[1]:.3G}x^3 + {coefs_act[2]:.3G}x^2 {coefs_act[3]:.3G}x + {coefs_act[4]:.3G}$') #p(X) evaluates the polynomial at X\n",
    "#         plt.title(\"Predicted vs Actual\")\n",
    "#         ax1.set_xlabel(r'STW [$knots$]', fontsize=13)\n",
    "#         ax1.set_ylabel(r'FOC [$T/h$]', fontsize=13)\n",
    "\n",
    "#         ax2 = ax1.twinx()\n",
    "\n",
    "#         ax2.scatter(Xa, Pb_actual,marker='o',linewidths=.8,edgecolors='blue',facecolor='none', label = 'Actual STW',s=12 )\n",
    "#         ax2.scatter(Xp, Pb_predicted,marker='x',linewidths=.5,c='red',label = 'Predicted STW',s=12 )\n",
    "#         ax2.set_ylabel(r'Brake Power [$kW$]', fontsize=13)\n",
    "\n",
    "\n",
    "#         ax1.axhline(y=slow_steam,linestyle = 'dotted',c='k')\n",
    "#         ax1.axhline(y=normal,linestyle = 'dotted',c='k')\n",
    "#         ax1.axhline(y=max_Pb,linestyle = 'dotted',c='k')\n",
    "\n",
    "#         ax1.text(6.1,1.1*slow_steam,'Slow Steaming',rotation=360)\n",
    "#         ax1.text(6.1,1.03*normal,'Normal Crusing',rotation=360)\n",
    "#         ax1.text(6.1,1.01*max_Pb,'Max Power',rotation=360)\n",
    "\n",
    "\n",
    "#         # plt.xlim(6,21)\n",
    "#         # plt.ylim(0,2)\n",
    "#         plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "#         plt.legend(bbox_to_anchor=(0,-.4),loc=\"lower left\")\n",
    "#         # plt.show()\n",
    "\n",
    "#         return coefs_act,coefs_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old PB curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def P_b_curve\n",
    "# slow_steam = 0.2*9760\n",
    "# normal = 0.65*9760\n",
    "# max_Pb = 9760\n",
    "\n",
    "# # plt.scatter(dfprog['stw_act'],FOC_act,edgecolors='black',c='grey',label=rf\"Actual\")\n",
    "# # plt.scatter(dfprog['stw_pred_rf'],P_b_pred_rf,c='black',edgecolors='grey',marker='x',label=rf\"RFR\")\n",
    "# # plt.scatter(dfprog['stw_pred_dt'],P_b_pred_dt,c='orange',edgecolors='black',marker='+')\n",
    "# # plt.scatter(dfprog['stw_pred_mlr'],P_b_pred_mlr,edgecolors='black',marker='^',facecolors='none')\n",
    "# plt.scatter(dfprog['stw_pred'],P_b_pred,c='#1f77b4',edgecolors='black')\n",
    "# plt.scatter(dfprog['stw_act'],P_b_act,edgecolors='black',c='grey',label=rf\"Actual\")\n",
    "# # plt.scatter(dfprog['stw_pred'],FOC_pred,edgecolors='black',c='#1f77b4',marker='^',label=rf\"ETR\")\n",
    "# # plt.scatter(dfprog['stw_act'],P_b_act,edgecolors='black',c='orange')\n",
    "# # Here: add a new tick with the required value\n",
    "# plt.axhline(y=slow_steam,linestyle = 'dotted',c='k')\n",
    "# plt.axhline(y=normal,linestyle = 'dotted',c='k')\n",
    "# plt.axhline(y=max_Pb,linestyle = 'dotted',c='k')\n",
    "# plt.text(6.1,2100,'Slow Steaming',rotation=360)\n",
    "# plt.text(6.1,6444,'Normal Crusing',rotation=360)\n",
    "# plt.text(10.1,9900,'Max Power',rotation=360)\n",
    "# plt.xlim(6,22)\n",
    "# plt.xlabel(r'Speed Through Water [$STW$]', fontsize=13)\n",
    "# plt.ylabel(r'Power [$kW$]', fontsize=13)\n",
    "# plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_predict(model,test_features):\n",
    "#     predictions = model.predict(test_features)\n",
    "#     return predictions\n",
    "\n",
    "# ydate_pred = label_predict(model_etr_hpov,x_date_c)\n",
    " \n",
    "# dfprog_sog = pd.Series(y_date_c,name=\"sog_act\").to_frame()\n",
    "# dfprog_sog[\"sog_pred\"] = ydate_pred\n",
    "# dfprog_sog[\"gamma\"] = pd.concat([dfdate6tr[\"truenorthcurrentdir\"],df_months6[\"True North Current Direction\"]])\n",
    "# dfprog_sog[\"nwinddir\"] = pd.concat([dfdate6tr[\"truenorthwinddir\"],df_months6[\"True North Wind Direction\"]])\n",
    "\n",
    "# print(dfprog_sog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfprog = pd.concat([x_date_c,dfprog_sog],axis=1)\n",
    "# dfprog.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch to function based, so all the prevoius code will be archived here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ship Information, that are readily available in ship specification\n",
    "\n",
    "# loa = 158 # ship overall length\n",
    "# lwl = 144.8 # ship waterline length, m\n",
    "# lpp = 0.97*lwl # ship perpendicular length , m, according to information\n",
    "# B = 24.5 # Ship breadth, m\n",
    "# depth = 13.8 # Ship depth. m\n",
    "# T_n = 5.85 # Nominal max draught , m\n",
    "# # T_n = 5.7 # Nominal design draught , m\n",
    "# dwt = 5110 # ship dead weight , t\n",
    "# V_n = 17.7 # ship design speed, knots\n",
    "\n",
    "# # Environmental Constants\n",
    "\n",
    "# g = 9.805 # gravity, kg/ms^2 \n",
    "# rho_sea = 1025 # kg/m3\n",
    "# nu_sea = 0.00000118 # Kinematics viscosity of sea m^2/s\n",
    "# rho_air = 1.25 # density air \n",
    "\n",
    "# # Any other additional ship parameters beyond here are approximated based on literature review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch between actual and predicted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert STW to m/s, stw with only current correction\n",
    "\n",
    "# # dfprog['stw_pred_ms'] = dfprog['stw_pred'] / 1.94384\n",
    "\n",
    "# # Switch between actual and predicted here \n",
    "\n",
    "# dfprog['stw_pred_ms'] = dfprog['stw_act'] / 1.94384\n",
    "\n",
    "# # Calculation for Block coefficient,C_b, according to Schneekluth and Bertram 1998\n",
    "# # Then Froude number is required\n",
    "\n",
    "# V_n = 17.7/1.94384\n",
    "# Fr_n = V_n / math.sqrt(g*lwl)\n",
    "# print(f\"Froude Number {Fr_n:0.4f}\")\n",
    "\n",
    "# C_b = -4.22 + 27.8*math.sqrt(Fr_n) - 39.1*Fr_n + 46.6*(Fr_n)**3\n",
    "# print(f\"C_b {C_b:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculation for midship section coefficient, C_m according to Jensen from Birk\n",
    "\n",
    "# C_m = 1 / (1+(1-C_b)**3.5)\n",
    "# # C_m = 0.977 + 0.085*(C_b-0.6) # Charcalis\n",
    "# print(f\"C_m {C_m:0.4f}\")\n",
    "\n",
    "# # prismatic coefficient C_p can be calculated according to Biran\n",
    "\n",
    "# C_p = C_b/C_m \n",
    "# print(f\"C_p {C_p:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Displacement calculation according to Barras \n",
    "# # Use approximate value cd=0.35 according to Barras\n",
    "# # Readjust displacement to standard formula\n",
    "\n",
    "# # C_d = 0.35\n",
    "# # dsp = dwt/C_d # m^3\n",
    "\n",
    "# dsp = C_b * lwl * B * T_n\n",
    "\n",
    "# print(f\"dsp {dsp:0.4f} m^3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # coefficient c14 to account for stern shape according to holtrop mennen\n",
    "\n",
    "# C_stern = 10 # assume u shaped stern\n",
    "# c14 = 1 + 0.011*C_stern \n",
    "# print(f\"c14 {c14:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate length of run according to holtrop mennen\n",
    "\n",
    "# # lcb = -2/100 # according to Barras\n",
    "# lcb = -(0.44*Fr_n - 0.094) # according to Guldhammer and Harvald\n",
    "\n",
    "# # L in holtrop mennen is lwl\n",
    "\n",
    "# lr = lwl*(1-C_p+(0.06*C_p*lcb/(4*C_p-1)))\n",
    "# print(f\"lr {lr:0.4f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now the (1+k1) can be calculated\n",
    "\n",
    "# k1a = 0.487118*c14*(B/lwl)**1.06806\n",
    "# k1b = (dfprog['draught']/lwl)**0.46106\n",
    "# k1c = (lwl/lr)**0.121563\n",
    "# k1d = (lwl**3/dsp)**0.36486\n",
    "# k1e = (1-C_p)**-0.604247\n",
    "\n",
    "# dfprog['k1_const'] = 0.93 + k1a*k1b*k1c*k1d*k1e\n",
    "# print(f\"k1_const {dfprog['k1_const'].mean():0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate Reynold number and Coefficient of Friction C_f. Here, the C_f will be dynamic and depend on the velocity of the ship\n",
    "\n",
    "# dfprog['Re'] =( dfprog['stw_pred_ms'] * lwl ) / nu_sea\n",
    "# dfprog['C_f'] = 0.075 / (np.log10(dfprog['Re']-2)**2)\n",
    "\n",
    "# print(f\"Mean Reynold Number {dfprog['Re'].mean():0.2f}\")\n",
    "# print(f\"Mean Coefficient of friction {dfprog['C_f'].mean():0.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the appendage area of bare hull S_bh\n",
    "# # Formula according to Holtrop Mennen\n",
    "\n",
    "# # Calculate the waterplane area coefficient \n",
    "# # Formula according to Schneekluth and Bertram\n",
    "\n",
    "# C_wp = (1+2*C_b)/3\n",
    "# print(f\"C_wp {C_wp:0.4f}\")\n",
    "\n",
    "# # Calculate transverse bulb area A_bt, Transom area A_t and immersed midship section area A_m according to Kim 2019 (depreceated)\n",
    "\n",
    "# # dfprog['A_m'] = B*dfprog['draught']*C_m\n",
    "# # Borrow from Guldhammer and Harvald\n",
    "# dfprog['A_m'] = dsp/(lpp*C_p)\n",
    "# dfprog['A_t'] = 0.051 * dfprog['A_m']\n",
    "# dfprog['A_bt'] = 0.085*dfprog['A_m'] # From approximation of Kracht78, Similar to Charcalis\n",
    "# # dfprog['A_bt'] = 0.08*dfprog['A_m'] # From approximation of Kracht78, Similar to Charcalis\n",
    "# print(f\"A_m {dfprog['A_m'].mean():0.2f} m^2\")\n",
    "# print(f\"A_t {dfprog['A_t'].mean():0.2f} m^2\")\n",
    "# print(f\"A_bt {dfprog['A_bt'].mean():0.2f} m^2\")\n",
    "\n",
    "# dfprog['sbh_a'] = lwl*(2*dfprog['draught']+B)*math.sqrt(C_m)\n",
    "# sbh_b = 0.453\n",
    "# sbh_c = 0.4425*C_b\n",
    "# sbh_d = 0.2862*C_m\n",
    "# dfprog['sbh_e'] = 0.003467*(B/dfprog['draught'])\n",
    "# sbh_f = 0.3696*C_wp\n",
    "# dfprog['sbh_g'] = 2.38*dfprog['A_bt']/C_b\n",
    "\n",
    "# dfprog['S_bh'] = dfprog['sbh_a']*(sbh_b+sbh_c-sbh_d+dfprog['sbh_e']+sbh_f)+dfprog['sbh_g']\n",
    "\n",
    "# print(f\"S_bh {dfprog['S_bh'].mean():0.2f} m^2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate R_f\n",
    "\n",
    "# dfprog['R_f'] = 0.5 * rho_sea * (dfprog['stw_pred_ms'])**2 * dfprog['C_f'] * dfprog['S_bh'] * dfprog['k1_const']\n",
    "\n",
    "# print(f\"Mean STW {dfprog['stw_pred'].mean():0.2f} kt\")\n",
    "# print(f\"Frictional Resistance {dfprog['R_f'].mean():0.2f} N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate resistance due to appendage\n",
    "\n",
    "# # Assume S_app\n",
    "# # Taken from Holtrop Mennen worked example\n",
    "# # S_app = 50 # m^2 \n",
    "\n",
    "# # Calculation of appendage area according to Hollenbach method, the formula is for twin screw ship\n",
    "\n",
    "# # Lower limit\n",
    "# S_app_lo = dfprog['S_bh'].mean()*(0.028+0.01*math.exp(-(lpp*T_n)/1000))\n",
    "\n",
    "# # Upper limit\n",
    "# S_app_hi = dfprog['S_bh'].mean()*(0.0325+0.045*math.exp(-(lpp*T_n)/1000))\n",
    "\n",
    "# print(f\"S_app between {S_app_lo:.02f} and {S_app_hi:.02f} according to Hollenbach\")\n",
    "\n",
    "# # The following appendage area are scaled from the picture of the ship\n",
    "# # Constant k here means (1+k_2) !\n",
    "\n",
    "# D_shaft = 0.55 # m, approx\n",
    "# l_shaft = 13.54 # m, approx\n",
    "\n",
    "# S_app_shaft = math.pi * D_shaft * l_shaft\n",
    "# k2_shaft = 2   \n",
    "\n",
    "# h_rudder = 4.06 #m, approx\n",
    "# B_rudder = 1.99 #m, approx\n",
    "# S_app_rudder = 2 * h_rudder * B_rudder #m, two side\n",
    "# k2_rudder = 3\n",
    "\n",
    "# h_skeg = 4.41 #m, approx\n",
    "# l_skeg = 26.23 #m, approx\n",
    "# S_app_skeg =  h_skeg * l_skeg #two side\n",
    "# k2_skeg = 0.75\n",
    "\n",
    "# S_app = S_app_shaft + S_app_rudder + S_app_skeg\n",
    "\n",
    "# k2_const = (k2_shaft*S_app_shaft + k2_rudder*S_app_rudder + k2_skeg*S_app_skeg)/S_app\n",
    "\n",
    "# print(f\"Wetted area of hull is {dfprog['S_bh'].mean():0.2f}\")\n",
    "# print(f\"(1+k2) equivalent {k2_const:0.2f}\")\n",
    "# print(f\"Total Appendage area is {S_app:0.2f}\")\n",
    "# # from holtrop mennen, take case of twin screw\n",
    "# # k2_const = 2.8\n",
    "\n",
    "# # Add resistance due to Bow Thrusters\n",
    "\n",
    "# d_th = 2.15 #m, approx\n",
    "\n",
    "# # Use formula from Hollenach\n",
    "# C_dth = 0.003 + 0.003*((10*d_th/T_n)-1)\n",
    "# # C_dth = 0.003 # The picture shows that the thruster are fairly parallel to midship area\n",
    "# # There are two bow thruster in this ship\n",
    "# dfprog['R_th'] = rho_sea*dfprog['stw_pred_ms']**2*math.pi*d_th**2*C_dth\n",
    "# print(f\"Friction due to bow thrusters {dfprog['R_th'].mean():0.2f} N\")\n",
    "# dfprog['R_app'] = (0.5 * rho_sea * (dfprog['stw_pred_ms'])**2 * (dfprog['C_f']) * S_app *k2_const) + 2*dfprog['R_th']\n",
    "# print(f\"Mean STW {dfprog['stw_pred'].mean():0.2f} kt\")\n",
    "# print(f\"Appendage Friction {dfprog['R_app'].mean():0.2f} N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate wave-making and wave-breaking resistance\n",
    "\n",
    "# print(\"Ratio check to ensure correct formula is used for Holtrop Mennen\")\n",
    "# print(f\"B/lwl = {B/lwl:0.2f}\")\n",
    "# print(f\"lwl^3/displacemt = {lwl**3/dsp:0.2f}\")\n",
    "# print(f\"C_p = {C_p:0.4f}\")\n",
    "\n",
    "\n",
    "# c7 = B/lwl\n",
    "# T_fwd = dfprog['draught'] # See reasoning from Rakke16 \n",
    "# h_b = 0.4*T_n # must not exceed 0.6 T_f, here T_n = T_f (design), reasong and coefficient value taken from Rakke\n",
    "\n",
    "# # All formulas here are listed by Holtrop Mennen\n",
    "\n",
    "# dfprog['c3'] = 0.56 * dfprog['A_bt']**1.5 / (B*dfprog['draught']*(0.31*np.sqrt(dfprog['A_bt'])+T_fwd-h_b))\n",
    "# c2 = np.exp(-1.89*np.sqrt(dfprog['c3']))\n",
    "# dfprog['c5'] = 1 - 0.8*(dfprog['A_t']/(B*dfprog['draught']*C_m))\n",
    "# lambda_const = (1.446 * C_p) - 0.03*(lwl/B)\n",
    "# c16 = 8.07981*C_p - 13.8673*C_p**2 + 6.984388*C_p**3\n",
    "# dfprog['m_1'] = 0.0140407 * (lwl/dfprog['draught']) - 1.75254*(dsp**(1/3)/lwl) -  4.79323*(B/lwl) - c16\n",
    "# c15 = -1.69385\n",
    "\n",
    "# # Use dynamic Froude here to refect the actual resistance due to ship movement \n",
    "\n",
    "# dfprog['Fr_n'] = dfprog['stw_pred_ms'] / math.sqrt(g*lwl)\n",
    "# # Updated formula use m_4\n",
    "# dfprog['m_4'] = 0.4 * c15 * np.exp(-0.034*dfprog['Fr_n']**-3.29)\n",
    "\n",
    "# i_e = 1 + 89*math.exp(-(lwl/B)**0.80856*(1-C_wp)**0.30484*(1-C_p-0.0225*lcb)**0.6367*(lr/B)**0.34574*((100*dsp)/lwl**3)**0.16302)\n",
    "# dfprog['c1'] = 2223105 * c7**3.78613 * (dfprog['draught']/B)**1.07961*(90-i_e)**-1.37565\n",
    "# d = -0.9\n",
    "\n",
    "# # Use updated formula with m4\n",
    "\n",
    "# dfprog['R_w'] = dfprog['c1']*c2*dfprog['c5']*dsp*g*rho_sea*np.exp(dfprog['m_1']*dfprog['Fr_n']**d+dfprog['m_4']*np.cos(lambda_const*dfprog['Fr_n']**-2))\n",
    "\n",
    "# print(f\"Mean STW {dfprog['stw_pred'].mean():0.2f} kt\")\n",
    "# print(f\"Wave resistance {dfprog['R_w'].mean():0.2f} N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Resistance due to Bulbous Bow\n",
    "# Calculate effect of forward sinkage h_f and local wave height at bow h_w\n",
    "\n",
    "\n",
    "\n",
    "# Holtrop Mennen according to Molland11\n",
    "\n",
    "# P_b = 0.56*np.sqrt(dfprog['A_bt'])/(T_fwd-1.5*h_b)\n",
    "# dfprog['Fn_i'] = dfprog['stw_pred_ms'] / np.sqrt(g*(T_fwd-h_b-0.25*np.sqrt(dfprog['A_bt']))+0.15*dfprog['stw_pred_ms']**2)\n",
    "# dfprog['R_b'] = 0.11 * np.exp(-3*P_b**-2)*dfprog['Fn_i']**3*dfprog['A_bt']**1.5*rho_sea*g/(1+dfprog['Fn_i']**2)\n",
    "\n",
    "\n",
    "# # P_b = 0.56*np.sqrt(dfprog['A_bt'])/(T_fwd-1.5*h_b+h_f)\n",
    "# dfprog['Fn_i'] = dfprog['stw_pred_ms'] / np.sqrt(g*(T_fwd-h_b-0.25*np.sqrt(dfprog['A_bt']))+0.15*dfprog['stw_pred_ms']**2)\n",
    "# # Updated Fn_i\n",
    "# # dfprog['Fn_i'] = dfprog['stw_pred_ms'] / np.sqrt(g*(T_fwd-h_b-0.25*np.sqrt(dfprog['A_bt'])+h_f+h_w))\n",
    "# dfprog['R_b'] = 0.11 * np.exp(-3*P_b**-2)*dfprog['Fn_i']**3*dfprog['A_bt']**1.5*rho_sea*g/(1+dfprog['Fn_i']**2)\n",
    "# # Update R_b\n",
    "# # dfprog['R_b'] = 0.11 * rho_sea * g * np.sqrt(dfprog['A_bt'])**3 * ((dfprog['Fn_i']**3)/(1 + dfprog['Fn_i']**2)) * np.exp(-3*P_b**-2)\n",
    "\n",
    "\n",
    "# # Holtrop Mennen according to Molland11\n",
    "\n",
    "# P_b = 0.56*np.sqrt(dfprog['A_bt'])/(T_fwd-1.5*h_b)\n",
    "# dfprog['Fn_i'] = dfprog['stw_pred_ms'] / np.sqrt(g*(T_fwd-h_b-0.25*np.sqrt(dfprog['A_bt']))+0.15*dfprog['stw_pred_ms']**2)\n",
    "# dfprog['R_b'] = 0.11 * np.exp(-3*P_b**-2)*dfprog['Fn_i']**3*dfprog['A_bt']**1.5*rho_sea*g/(1+dfprog['Fn_i']**2)\n",
    "\n",
    "\n",
    "# # Updated Holtrop Mennen 1984 according to Birk\n",
    "# # Seems to yield wrong results \n",
    "\n",
    "# h_f = C_p * C_m * ((B*T_n)/lwl) * (136-316-3*dfprog['Fr_n'])*dfprog['Fr_n']**3\n",
    "# h_w = (i_e * dfprog['stw_pred_ms']**2) / (400*g)\n",
    "# P_b = 0.56*np.sqrt(dfprog['A_bt'])/(T_fwd-1.5*h_b+h_f)\n",
    "# dfprog['Fn_i'] = dfprog['stw_pred_ms'] / np.sqrt(g*(T_fwd-h_b-0.25*np.sqrt(dfprog['A_bt'])+h_f+h_w))\n",
    "# dfprog['R_b'] = 0.11 * rho_sea * g * np.sqrt(dfprog['A_bt'])**3 * ((dfprog['Fn_i']**3)/(1 + dfprog['Fn_i']**2)) * np.exp(-3*P_b**-2)\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"Emergence of bulb Pb {P_b.mean():0.4f}\")\n",
    "# print(f\"Immersion Froude number Fn_i {dfprog['Fn_i'].mean():0.2f}\")\n",
    "# print(f\"Mean STW {dfprog['stw_pred'].mean():0.2f} kt\")\n",
    "# print(f\"Resistance due to bulbous bow {dfprog['R_b'].mean():0.2f} N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculate Transom Resistance \n",
    "\n",
    "# dfprog['Fn_tr'] = dfprog['stw_pred_ms'] / np.sqrt(2*g*dfprog['A_t']/(B+(B*C_wp)))\n",
    "# print(f\"Froude for Transom {dfprog['Fn_tr'].mean():0.4f}\")\n",
    "\n",
    "# # Use condition to calculate Froude due to transom\n",
    "\n",
    "# cond_Fn_tr = [dfprog['Fn_tr'] < 5 ]\n",
    "# cond_c6 = [0.2*(1-0.2*dfprog[('Fn_tr')])]\n",
    "\n",
    "# dfprog['c6'] = np.select(cond_Fn_tr,cond_c6,0)\n",
    "# dfprog['R_tr'] = 0.5*rho_sea*10**2*dfprog['A_t']*dfprog['c6']\n",
    "# print(f\"Mean STW {dfprog['stw_pred'].mean():0.2f} kt\")\n",
    "# print(f\"Resistance due to transom {dfprog['R_tr'].mean():0.2f} N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_tr = dfprog['R_tr']\n",
    "# R_tr.hist()\n",
    "# plt.title(\"Transom Resistance\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model ship correlation resistance\n",
    "\n",
    "# # print(\"Ratio check to ensure correct formula is used for Holtrop Mennen\")\n",
    "\n",
    "# cond_Tf_lwl = [(T_fwd/lwl) <= 0.04 ]\n",
    "# cond_c4 = [T_fwd/lwl]\n",
    "# dfprog['c4'] = np.select(cond_Tf_lwl,cond_c4,0.04)\n",
    "\n",
    "# C_a = 0.00546*(lwl+100)**-0.16 - 0.002 + 0.003*math.sqrt(lwl/7.5)*C_b**4*c2*(0.04-dfprog ['c4'])\n",
    "\n",
    "# dfprog['R_a'] = 0.5*rho_sea*dfprog['stw_pred_ms']**2*C_a*(dfprog['S_bh']+S_app)\n",
    "\n",
    "# print(f\"C_a {C_a.mean()}\")\n",
    "# print(f\"Mean STW {dfprog['stw_pred'].mean():0.2f} kt\")\n",
    "# print(f\"Correlation allowance resistance {dfprog['R_a'].mean():0.2f} N\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of Wind speed <br>\n",
    "- There are two formula, one from Holtrop another one from Blendermann, Blendermann seems to be more reasonable as it considers the wind direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate using Holtrop Mennen\n",
    "# A_f = 325.3\n",
    "# dfprog['R_aa_hm'] = 0.5 * rho_air * dfprog['stw_pred_ms']**2 * 0.8 * A_f\n",
    "# print(f\"Mean wind resistance Holtrop Mennen {dfprog['R_aa_hm'].mean():0.2f} N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate Additional Resistance, consist of wind resistance and wave resistance\n",
    "# # Calculate Apparent velocities and Apparent Angle \n",
    "\n",
    "# dfprog['V_aw'] = np.sqrt(dfprog['windspeed']**2 + dfprog['stw_pred_ms']**2 + 2*dfprog['windspeed']*dfprog['stw_pred_ms']*np.cos(np.deg2rad(dfprog['truewinddir'])))\n",
    "\n",
    "# dfprog['awa_c1'] = (dfprog['windspeed']/dfprog['V_aw'])*np.sin(np.deg2rad(dfprog['truewinddir']))\n",
    "\n",
    "# # Epsilon is Apparent Wind Angle AWA\n",
    "\n",
    "# dfprog['epsilon'] = np.rad2deg(np.arcsin(dfprog['awa_c1']))\n",
    "\n",
    "# # Values and method from Blendermann\n",
    "\n",
    "# C_DlAf = 0.45\n",
    "# A_f = 325.3\n",
    "# A_l = 2125.8\n",
    "# C_Dt = 0.9\n",
    "# delta = 0.8\n",
    "# C_Dl = C_DlAf * A_f / A_l\n",
    "# L_bwl = 43.75 # m, acquired from picture\n",
    "\n",
    "# dfprog['Raa_const1'] = (rho_air/2) * dfprog['V_aw']**2 * A_l * C_Dl\n",
    "# dfprog['Raa_const2'] = np.cos(np.deg2rad(dfprog['epsilon']))\n",
    "# dfprog['Raa_const3'] = 1 - (delta/2) * ((1-(C_Dl/C_Dt))*(np.sin(np.deg2rad(2*dfprog['epsilon'])))**2)\n",
    "\n",
    "# dfprog['R_aa'] = dfprog['Raa_const1'] * dfprog['Raa_const2'] / dfprog['Raa_const3'] \n",
    "\n",
    "# print(f\"Mean wind resistance Blendermann {dfprog['R_aa'].mean():0.2f}N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "# plt.sca(axes[0])\n",
    "# R_aa_hm = dfprog['R_aa_hm']\n",
    "# R_aa_hm.hist()\n",
    "# plt.title(\"Holtrop Mennen Wind Resistance\")\n",
    "# plt.sca(axes[1])\n",
    "# R_aa = dfprog['R_aa']\n",
    "# R_aa.hist()\n",
    "# plt.title(\"Blendermann Wind Resistance\")\n",
    "# plt.ylabel(\"\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate Wave Resistance according to STAWAVE-1\n",
    "\n",
    "# Rawl = 1/16 * rho_sea * g * dfprog['windwaveswellheight']**2 * math.sqrt(B/L_bwl)\n",
    "\n",
    "# condwave = [dfprog['truewavedir']<=45]\n",
    "# choicewave = [Rawl]\n",
    "\n",
    "# dfprog['R_awl'] = np.select(condwave,choicewave,0)\n",
    "\n",
    "# print(f\"Mean wave resistance STAWAVE-1 {dfprog['R_awl'].mean():0.2f} N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate total Resistance\n",
    "\n",
    "# dfprog['R_tot'] = dfprog['R_f'] + dfprog['R_app'] + dfprog['R_w'] + dfprog['R_b'] + dfprog['R_a']  + dfprog['R_aa'] + dfprog['R_awl'] + dfprog['R_tr']\n",
    "# # dfprog['R_tot'] = dfprog['R_f'] + dfprog['R_app'] + dfprog['R_w'] + dfprog['R_b'] + dfprog['R_a']  + dfprog['R_aa_hm'] + dfprog['R_awl']\n",
    "\n",
    "# dfprog['R_tot'] = dfprog['R_tot'] / 1e3\n",
    "\n",
    "# df_prog=dfprog.drop(['C_f','Re','Fn_i'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate Efficiencies\n",
    "\n",
    "# # Diameter value for ship estimated from Bertram \n",
    "\n",
    "# # D = 0.215*16 #m \n",
    "# # Revised D, 08.07.23\n",
    "# D = 4 # m, from flyer\n",
    "\n",
    "# # Update C_v formula\n",
    "\n",
    "# dfprog['C_v'] = (dfprog['k1_const']*dfprog['R_f'] + dfprog['R_app'] + dfprog['R_a']) / (0.5*rho_sea*dfprog['stw_pred_ms']**2*(dfprog['S_bh']+S_app))\n",
    "# dfprog['w'] = 0.3095 * C_b + 10*dfprog['C_v']*C_b - (0.23*D)/np.sqrt(B*dfprog['draught']) \n",
    "# dfprog['t'] = 0.325*C_b - 0.1885*D/np.sqrt(B*dfprog['draught'])\n",
    "# dfprog['eff_h'] = (1-dfprog['t']) / (1-dfprog['w'])\n",
    "# dfprog['eff_r'] = 0.9737 + 0.111*(C_p - 0.225*lcb) - 0.06325*lcb\n",
    "# dfprog['eff_s'] = 0.99 # Set according to holtrop mennen and man\n",
    "# dfprog['eff_o'] = 0.75 # Approximation from Wageningen Line from Breslin94, since Holtrop perform their measurement in Wageningen basin \n",
    "\n",
    "# dfprog['eff_tot'] = dfprog['eff_h']* dfprog['eff_r']* dfprog['eff_s']*dfprog['eff_o'] # consider sea margin\n",
    "\n",
    "# print(f\"Total Efficiency {dfprog['eff_tot'].mean():0.3f} N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate power and FOC\n",
    "\n",
    "# dfprog['P_b'] = (dfprog['R_tot'] * dfprog['stw_pred_ms'])/dfprog['eff_tot'] # in kW\n",
    "# SFOC = 169.4 # g/kWh, taken from datasheet Waertsilla 8V31\n",
    "# dfprog['FOC'] = (dfprog['P_b'] * SFOC)/1e6 # get FOC t/h\n",
    "\n",
    "# print(f\"Average Power {dfprog['P_b'].mean():0.3f} kW\")\n",
    "# print(f\"Max Power {dfprog['P_b'].max():0.3f} kW\")\n",
    "# print(f\"Average Fuel Consumption per hour {dfprog['FOC'].mean():0.3f} T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfprog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop All intermediate Constants\n",
    "\n",
    "# dfprog_f = dfprog.drop(['Re','C_f','Fr_n','m_4','Fn_i',\n",
    "#                         'C_v','w','t','sog_act','stw_pred_ms','V_aw','awa_c1','epsilon','Raa_const1','Raa_const2','Raa_const3',\n",
    "#                         'k1_const','A_m','A_t','A_bt','sbh_a','sbh_e','sbh_g',\n",
    "#                         'S_bh','c3','c5','m_1','c1','Fn_tr','R_tr','c6','c4'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfprog_f.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dfprog_f.describe())\n",
    "# stats = dfprog_f.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is repurposed as FOC calculator, nothing wrong here !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Xp = stw_pred_et\n",
    "# # Yp = P_b_pred\n",
    "\n",
    "# # Xa = stw_act\n",
    "# # Ya = P_b_act\n",
    "\n",
    "# # To predict FOC\n",
    "\n",
    "# Xp = stw_pred_et\n",
    "# Yp = FOC_pred\n",
    "\n",
    "# Xa = stw_act\n",
    "# Ya = FOC_act\n",
    "\n",
    "# coefs_pred = np.polyfit(Xp, Yp, 4)\n",
    "# coefs_act = np.polyfit(Xa, Ya, 4)\n",
    "\n",
    "# print(coefs_pred)\n",
    "# print(coefs_act)\n",
    "\n",
    "# p_pred = np.poly1d(coefs_pred)\n",
    "# p_act = np.poly1d(coefs_act)\n",
    "\n",
    "# plt.scatter(Xp, Yp,marker='x',linewidths=.5,c='gray',label = 'Predicted STW',s=12 )\n",
    "# plt.scatter(Xa, Ya,marker='o',linewidths=.8,edgecolors='gray',facecolor='none', label = 'Actual STW',s=12 )\n",
    "\n",
    "# sorted_pred= np.sort(Xp)\n",
    "# sorted_act= np.sort(Xa)\n",
    "\n",
    "# plt.plot(sorted_pred, p_pred(sorted_pred), linestyle = '-',color = 'b',\n",
    "#          label=rf'$y = {coefs_pred[0]:.1f}x^4 {coefs_pred[1]:.1f}x^3 + {coefs_pred[2]:.1f}x^2 {coefs_pred[3]:.1f}x + {coefs_pred[4]:.1f}$') #p(X) evaluates the polynomial at X\n",
    "# plt.plot(sorted_act, p_act(sorted_act), linestyle = \"-.\" , color = 'red',\n",
    "#          label=rf'$y = {coefs_act[0]:.1f}x^4 {coefs_act[1]:.1f}x^3 + {coefs_act[2]:.1f}x^2 {coefs_act[3]:.1f}x + {coefs_act[4]:.1f}$') #p(X) evaluates the polynomial at X\n",
    "# plt.title(\"Predicted vs Actual\")\n",
    "# plt.xlabel(r'STW [$knots$]', fontsize=13)\n",
    "# plt.ylabel(r'Power [$kW$]', fontsize=13)\n",
    "\n",
    "# # plt.axhline(y=slow_steam,linestyle = 'dotted',c='k')\n",
    "# # plt.axhline(y=normal,linestyle = 'dotted',c='k')\n",
    "# # plt.axhline(y=max_Pb,linestyle = 'dotted',c='k')\n",
    "\n",
    "# # plt.text(6.1,2100,'Slow Steaming',rotation=360)\n",
    "# # plt.text(6.1,6500,'Normal Crusing',rotation=360)\n",
    "# # plt.text(6.1,9900,'Max Power',rotation=360)\n",
    "\n",
    "\n",
    "# plt.xlim(6,21)\n",
    "# # plt.ylim(200,5000)\n",
    "# plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# plt.legend(bbox_to_anchor=(0,-.4),loc=\"lower left\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried Kwon again, but still weird, the equation make sense if we use container ship case , but this aint container ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stw_pred_ms_test = dfprog[\"stw_pred\"]/1.9438\n",
    "# dfprog['Fr_n_test'] = (dfprog[\"stw_pred\"]/1.9438)/ math.sqrt(g*lpp)\n",
    "# # dfprog['Fr_n'] = 0.233\n",
    "# # dsp test\n",
    "# dsp = 13388.812\n",
    "\n",
    "# # Formula according to Kwon\n",
    "\n",
    "# # # Get Beaufort scale based on formula from Schneekluth Bertram\n",
    "\n",
    "# # dfprog['B_n'] = (dfprog['windspeed']/0.836)**(2/3) \n",
    "\n",
    "# # Convert wind speed to fuzzy scale\n",
    "\n",
    "# cond_windspeed = [(dfprog['windspeed']>0.3) & (dfprog['windspeed']<1.6),\n",
    "#                 (dfprog['windspeed']>1.6) & (dfprog['windspeed']<3.4),\n",
    "#                 (dfprog['windspeed']>3.4) & (dfprog['windspeed']<5.5),\n",
    "#                 (dfprog['windspeed']>5.5) & (dfprog['windspeed']<8),\n",
    "#                 (dfprog['windspeed']>8) & (dfprog['windspeed']<10.8),\n",
    "#                 (dfprog['windspeed']>10.8) & (dfprog['windspeed']<13.9),\n",
    "#                 (dfprog['windspeed']>13.9) & (dfprog['windspeed']<17.2),\n",
    "#                 (dfprog['windspeed']>17.2) & (dfprog['windspeed']<20.7),\n",
    "#                 (dfprog['windspeed']>20.8) & (dfprog['windspeed']<24.5),\n",
    "#                 (dfprog['windspeed']>24.5) & (dfprog['windspeed']<28.5),\n",
    "#                 (dfprog['windspeed']>28.5) & (dfprog['windspeed']<32.7),\n",
    "#                 (dfprog['windspeed']>32.7)]\n",
    "\n",
    "# cond_BN = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "# dfprog['B_n'] = np.select(cond_windspeed, cond_BN,0)\n",
    "\n",
    "\n",
    "# #  C_alpha is direction reduction coefficient \n",
    "\n",
    "# cond_winddir = [(dfprog['truewinddir']>=30) & (dfprog['truewinddir']<=60),\n",
    "#                 (dfprog['truewinddir']>=60) & (dfprog['truewinddir']<=150),\n",
    "#                 (dfprog['truewinddir']>=150) & (dfprog['truewinddir']<=180)]\n",
    "\n",
    "# cond_windcoeff = [(1.7 - 0.03*(dfprog['B_n']-4)**2)/2,\n",
    "#                   (0.9 - 0.06*(dfprog['B_n']-6)**2)/2,\n",
    "#                   (0.4 - 0.03*(dfprog['B_n']-8)**2)/2]\n",
    "\n",
    "# dfprog['C_alpha'] = np.select(cond_winddir, cond_windcoeff,1)\n",
    "\n",
    "# # C_mu is speed reduction coefficient\n",
    "\n",
    "# dfprog['C_mu'] = 2.6 - 3.7*dfprog['Fr_n_test'] - 11.6*dfprog['Fr_n_test']**2\n",
    "# # dfprog['C_mu'] = 2.2 - 2.5*dfprog['Fr_n_test'] - 9.7*dfprog['Fr_n_test']**2\n",
    "\n",
    "\n",
    "\n",
    "# # C_form is ship form coefficient \n",
    "\n",
    "# dfprog['C_form'] = 0.5*dfprog['B_n'] + (dfprog['B_n']**6.5 / (22*dsp**(2/3)))\n",
    "\n",
    "\n",
    "# dfprog['delta_stw_100'] = dfprog['C_alpha'] * dfprog['C_mu'] * dfprog['C_form'] # in percent %\n",
    "\n",
    "# dfprog['delta_stw_ms'] = (dfprog['delta_stw_100']/100)*stw_pred_ms_test # Speed loss in m/s\n",
    "# dfprog['delta_stw'] = (dfprog['delta_stw_100']/100)*stw_pred_ms_test*1.9438 # Speed loss in Knots\n",
    "\n",
    "# dfprog['delta_stw_100'].hist()\n",
    "# # plt.ylim(0,10)\n",
    "# plt.xlim(-1,5)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapped Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# degree=3\n",
    "# # X_seq_pred = np.linspace(min(stw_pred), max(stw_pred), 322).reshape(-1, 1)\n",
    "# # X_seq = np.linspace(min(stw), max(stw), 322).reshape(-1, 1)\n",
    "# polyreg=make_pipeline(PolynomialFeatures(degree),LinearRegression())\n",
    "\n",
    "# polyreg.fit(stw_pred.values.reshape(-1,1),P_b_pred)\n",
    "# polyreg.fit(stw_act.values.reshape(-1,1),P_b_act)\n",
    "\n",
    "# X_seq_pred = np.linspace(min(stw_pred), max(stw_pred), 322).reshape(-1, 1)\n",
    "# X_seq_act = np.linspace(min(stw_act), max(stw_act), 322).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "# poly_reg_model = LinearRegression()\n",
    "\n",
    "# poly_features_pred = poly.fit_transform(stw_pred.values.reshape(-1,1))\n",
    "# poly_model_pred = poly_reg_model.fit(poly_features, P_b_pred)\n",
    "# coef_3p = poly_model_pred.coef_[2]\n",
    "# coef_2p = poly_model_pred.coef_[1]\n",
    "# coef_1p = poly_model_pred.coef_[0]\n",
    "# coef_0p = poly_model_pred.intercept_\n",
    "# print(poly_model_pred.coef_)\n",
    "# print(poly_model_pred.intercept_)\n",
    "\n",
    "# poly_features_act = poly.fit_transform(stw_act.values.reshape(-1,1))\n",
    "# poly_model_act  = poly_reg_model.fit(poly_features, P_b_act)\n",
    "# coef_3a = poly_model_act.coef_[2]\n",
    "# coef_2a = poly_model_act.coef_[1]\n",
    "# coef_1a = poly_model_act.coef_[0]\n",
    "# coef_0a = poly_model_act.intercept_\n",
    "# print(poly_model_act.coef_)\n",
    "# print(poly_model_act.intercept_)\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure()\n",
    "# plt.scatter(stw_pred,P_b_pred,edgecolors='black')\n",
    "# plt.scatter(stw_act,P_b_act,edgecolors='black')\n",
    "# plt.plot(stw_pred,polyreg.predict(stw_pred.values.reshape(-1,1)),color=\"green\",ls='--',label=rf'$y = {coef_3p:.1f}x^3 + {coef_2p:.1f}x^2 +{coef_1p:.1f}x  {coef_0p:.1f}$')\n",
    "# plt.plot(stw_act,polyreg.predict(stw_act.values.reshape(-1,1)),color=\"red\",ls='--',label=rf'$y = {coef_3a:.1f}x^3 + {coef_2a:.1f}x^2 +{coef_1a:.1f}x  {coef_0a:.1f}$')\n",
    "# plt.axhline(y=slow_steam,linestyle = 'dotted',c='k')\n",
    "# plt.axhline(y=normal,linestyle = 'dotted',c='k')\n",
    "# plt.axhline(y=max_Pb,linestyle = 'dotted',c='k')\n",
    "# plt.text(6.1,2100,'Slow Steaming',rotation=360)\n",
    "# plt.text(6.1,6444,'Normal Crusing',rotation=360)\n",
    "# plt.text(6.1,9900,'Max Power',rotation=360)\n",
    "# plt.title(\"Power vs speed curve\")\n",
    "# plt.xlabel(r'Speed Through Water, STW [$m/s$]', fontsize=13)\n",
    "# plt.ylabel(r'Power [$kW$]', fontsize=13)\n",
    "# # plt.xlim(6,17.7)\n",
    "# # plt.ylim(200,5000)\n",
    "# plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated CV for effciencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate Efficiencies\n",
    "\n",
    "# # Diameter value for ship estimated from Bertram \n",
    "\n",
    "# # D = 0.215*16 #m \n",
    "# # Revised D, 08.07.23\n",
    "# D = 4 # m, from flyer\n",
    "\n",
    "# dfprog['C_v'] = dfprog['k1_const']*dfprog['C_f'] + C_a\n",
    "# dfprog['w'] = 0.3095 * C_b + 10*dfprog['C_v']*C_b - (0.23*D)/np.sqrt(B*dfprog['draught']) \n",
    "# dfprog['t'] = 0.325*C_b - 0.1885*D/np.sqrt(B*dfprog['draught'])\n",
    "# dfprog['eff_h'] = (1-dfprog['t']) / (1-dfprog['w'])\n",
    "# dfprog['eff_r'] = 0.9737 + 0.111*(C_p - 0.225*lcb) - 0.06325*lcb\n",
    "# dfprog['eff_s'] = 0.99 # Set according to holtrop mennen and man\n",
    "# dfprog['eff_o'] = 0.75 # Approximation from Wageningen Line from Breslin94\n",
    "\n",
    "# dfprog['eff_tot'] = dfprog['eff_h']* dfprog['eff_r']* dfprog['eff_s']*dfprog['eff_o']*0.85 # consider sea margin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rw formula updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate wave-making and wave-breaking resistance\n",
    "\n",
    "# print(\"Ratio check to ensure correct formula is used for Holtrop Mennen\")\n",
    "# print(f\"B/lwl = {B/lwl:0.2f}\")\n",
    "# print(f\"lwl^3/displacemt = {lwl**3/dsp:0.2f}\")\n",
    "# print(f\"C_p = {C_p:0.4f}\")\n",
    "\n",
    "\n",
    "# c7 = B/lwl\n",
    "# T_fwd = dfprog['draught'] # See reasoning from Rakke16 \n",
    "# h_b = 0.4*T_n # must not exceed 0.6 T_f, here T_n = T_f (design), reasong and coefficient value taken from Rakke\n",
    "\n",
    "# # All formulas here are listed by Holtrop Mennen\n",
    "\n",
    "# dfprog['c3'] = 0.56 * dfprog['A_bt']**1.5 / (B*dfprog['draught']*(0.31*np.sqrt(dfprog['A_bt'])+T_fwd-h_b))\n",
    "# c2 = np.exp(-1.89*np.sqrt(dfprog['c3']))\n",
    "# dfprog['c5'] = 1 - 0.8*(dfprog['A_t']/(B*dfprog['draught']*C_m))\n",
    "# lambda_const = (1.446 * C_p) - 0.03*(lwl/B)\n",
    "# c16 = 8.07981*C_p - 13.8673*C_p**2 + 6.984388*C_p**3\n",
    "# dfprog['m_1'] = 0.0140407 * (lwl/dfprog['draught']) - 1.75254*(dsp**(1/3)/lwl) -  4.79323*(B/lwl) - c16\n",
    "# c15 = -1.69385\n",
    "\n",
    "# # Use dynamic Froude here to refect the actual resistance due to ship movement \n",
    "\n",
    "# dfprog['Fr_n'] = dfprog['stw_pred_ms'] / math.sqrt(g*lpp)\n",
    "# # dfprog['m_2'] = c15 * C_p**2 * np.exp(-0.1*dfprog['Fr_n']**-2)\n",
    "# # Updated formula use m_4\n",
    "# dfprog['m_4'] = 0.4 * c15 * np.exp(-0.034*dfprog['Fr_n']**-3.29)\n",
    "\n",
    "\n",
    "\n",
    "# i_e = 1 + 89*math.exp(-(lwl/B)**0.80856*(1-C_wp)**0.30484*(1-C_p-0.0225*lcb)**0.6367*(lr/B)**0.34574*((100*dsp)/lwl**3)**0.16302)\n",
    "# dfprog['c1'] = 2223105 * c7**3.78613 * (dfprog['draught']/B)**1.07961*(90-i_e)**-1.37565\n",
    "# d = -0.9\n",
    "\n",
    "# # dfprog['R_w'] = dfprog['c1']*c2*dfprog['c5']*dsp*g*rho_sea*np.exp(dfprog['m_1']*dfprog['Fr_n']**d+dfprog['m_2']*np.cos(lambda_const*dfprog['Fr_n']**-2))\n",
    "\n",
    "# # Use updated formula \n",
    "\n",
    "# dfprog['R_w'] = dfprog['c1']*c2*dfprog['c5']*dsp*g*rho_sea*np.exp(dfprog['m_1']*dfprog['Fr_n']**d+dfprog['m_4']*np.cos(lambda_const*dfprog['Fr_n']**-2))\n",
    "\n",
    "# print(f\"Mean STW {dfprog['stw_pred'].mean():0.2f} kt\")\n",
    "# print(f\"Wave resistance {dfprog['R_w'].mean():0.2f} N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rb formula updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate Resistance due to Bulbous Bow\n",
    "# h_f = C_p * C_m * (136-316-3*dfprog['Fr_n'])*dfprog['Fr_n']**3\n",
    "# h_w = (i_e * dfprog['stw_pred_ms']**2) / (400*g)\n",
    "\n",
    "# P_b = 0.56*np.sqrt(dfprog['A_bt'])/(T_fwd-1.5*h_b)\n",
    "# # dfprog['F_ni'] = dfprog['stw_pred_ms'] / np.sqrt(g*(T_fwd-h_b-0.25*np.sqrt(dfprog['A_bt']))+0.15*dfprog['stw_pred_ms']**2)\n",
    "# # Updated F_ni\n",
    "# dfprog['F_ni'] = dfprog['stw_pred_ms'] / np.sqrt(g*(T_fwd-h_b-0.25*np.sqrt(dfprog['A_bt'])+h_f+h_w))\n",
    "\n",
    "# # dfprog['R_b'] = 0.11 * np.exp(-3*P_b**-2)*dfprog['F_ni']**3*dfprog['A_bt']**1.5*rho_sea*g/(1+dfprog['F_ni']**2)\n",
    "# # Updatef R_b\n",
    "# dfprog['R_b'] = 0.11 * rho_sea * g * np.sqrt(dfprog['A_bt'])**3 * ((dfprog['F_ni']**3)/(1 + dfprog['F_ni']**2)) * np.exp(-3*P_b**-2)\n",
    "\n",
    "# print(f\"Mean STW {dfprog['stw_pred'].mean():0.2f} kt\")\n",
    "# print(f\"Resistance due to bulbous bow {dfprog['R_b'].mean():0.2f} N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kwon Formula Scrapped due to unsuitability to our case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disp = 36.5e3\n",
    "\n",
    "\n",
    "# # dfprog['Fr_n'] = dfprog['stw_pred_msc'] / math.sqrt(g*lpp)\n",
    "# dfprog['Fr_n'] = 0.233\n",
    "\n",
    "\n",
    "# # Formula according to Kwon\n",
    "\n",
    "# # Get Beaufort scale based on formula from Schneekluth Bertram\n",
    "\n",
    "# dfprog['B_n'] = (dfprog['windspeed']/0.836)**(2/3) \n",
    "\n",
    "# #  C_alpha is direction reduction coefficient \n",
    "\n",
    "# cond_winddir = [(dfprog['truewinddir']>=30) & (dfprog['truewinddir']<=60),\n",
    "#                 (dfprog['truewinddir']>=90) & (dfprog['truewinddir']<=150),\n",
    "#                 (dfprog['truewinddir']>=150) & (dfprog['truewinddir']<=180)]\n",
    "\n",
    "# cond_windcoeff = [(1.7 - 0.03*(dfprog['B_n']-4)**2)/2,\n",
    "#                   (0.9 - 0.06*(dfprog['B_n']-6)**2)/2,\n",
    "#                   (0.4 - 0.03*(dfprog['B_n']-8)**2)/2]\n",
    "\n",
    "# dfprog['C_alpha'] = np.select(cond_winddir, cond_windcoeff,1)\n",
    "\n",
    "# # C_mu is speed reduction coefficient\n",
    "\n",
    "# dfprog['C_mu'] = 2.6 - 3.7*dfprog['Fr_n'] - 11.6*dfprog['Fr_n']**2\n",
    "\n",
    "# # C_form is ship form coefficient \n",
    "\n",
    "# dfprog['C_form'] = 0.5*dfprog['B_n'] + (dfprog['B_n']**6.5 / (22*dsp**(2/3)))\n",
    "\n",
    "\n",
    "# dfprog['delta_stw_100'] = dfprog['C_alpha'] * dfprog['C_mu'] * dfprog['C_form'] # in percent %\n",
    "\n",
    "# dfprog['delta_stw_ms'] = (dfprog['delta_stw_100']/100)*dfprog['stw_pred_msc'] # Speed loss in m/s\n",
    "# dfprog['delta_stw'] = (dfprog['delta_stw_100']/100)*dfprog['stw_pred_msc'] # Speed loss in Knots\n",
    "\n",
    "# dfprog['stw_pred_ms'] = dfprog['stw_pred_msc'] - dfprog['delta_stw_ms']\n",
    "\n",
    "# dfprog['stw_pred'] = dfprog['stw_pred_ms'] * 1.9438\n",
    "\n",
    "# delta_stw = dfprog['delta_stw']\n",
    "# delta_stw.hist()\n",
    "# plt.show\n",
    "# print(dfprog['delta_stw_100'].mean())\n",
    "# print(dfprog['delta_stw_100'].max())\n",
    "# print(dfprog['delta_stw'].max())\n",
    "# print(dfprog['C_alpha'].max())\n",
    "# print(dfprog['B_n'].max())\n",
    "\n",
    "# print(dfprog['B_n'].mean())\n",
    "\n",
    "# print(dfprog['C_mu'].mean())\n",
    "# print(dfprog['C_alpha'].mean())\n",
    "# print(dfprog['C_form'].mean())\n",
    "\n",
    "# dfprog_anom = dfprog[dfprog['B_n'] > 6]\n",
    "# print(dfprog_anom)\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "# plt.sca(axes[0])\n",
    "# stw_predc = dfprog['stw_predc']\n",
    "# stw_predc.hist()\n",
    "# plt.title(\"Speed due to current [kt]\")\n",
    "# plt.sca(axes[1])\n",
    "# stw_pred = dfprog['stw_pred']\n",
    "# stw_pred.hist()\n",
    "# plt.title(\"Speed after wind correction [kr]\")\n",
    "# plt.ylabel(\"\")\n",
    "# plt.show()\n",
    "\n",
    "# print(dfprog['stw_predc'].mean())\n",
    "# print(dfprog['stw_pred'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=3, figsize=(14, 4), sharey=True)\n",
    "# plt.sca(axes[0])\n",
    "# dfprog_f['stw_pred'].hist(bins=25,color='black')\n",
    "# # Parameter to plot in nice latex fonts\n",
    "# plt.rcParams.update(plt.rcParamsDefault)\n",
    "# plt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t\"font.serif\": \"bookman\",\n",
    "# \t})\n",
    "# plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# plt.xlabel(r\"STW $[Knots]$\")\n",
    "# plt.title(r\"Speed Through Water (STW)\", fontsize=13)\n",
    "# plt.sca(axes[1])\n",
    "# dfprog_f['R_tot'].hist(bins=25,color='black')\n",
    "# # Parameter to plot in nice latex fonts\n",
    "# plt.rcParams.update(plt.rcParamsDefault)\n",
    "# plt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t\"font.sans-serif\": \"bookman\",\n",
    "# \t})\n",
    "# plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# plt.xlabel(r\"Force $[kN]$\")\n",
    "# plt.title(r\"Total Resistance\", fontsize=13)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.sca(axes[2])\n",
    "# dfprog_f['P_b'].hist(bins=25,color='black')\n",
    "# # Parameter to plot in nice latex fonts\n",
    "# plt.rcParams.update(plt.rcParamsDefault)\n",
    "# plt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t\"font.sans-serif\": \"bookman\",\n",
    "# \t})\n",
    "# plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# plt.xlabel(r\"Power $[kW]$\")\n",
    "# plt.title(r\"Power\", fontsize=13)\n",
    "# plt.ylabel(\"\")\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phased Out Fuel Calc Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOC in T/d\n",
    "# dfprog[\"foc_act_d\"] = ((dfprog[\"stw_act\"]/17.7)**3) * 21.14112\n",
    "# dfprog[\"foc_pred_d\"] = ((dfprog[\"stw_pred\"]/17.7)**3) * 21.14112\n",
    "\n",
    "# # FOC in T/h\n",
    "# dfprog[\"foc_act_h\"] = ((dfprog[\"stw_act\"]/17.7)**3) * 0.8808\n",
    "# dfprog[\"foc_pred_h\"] = ((dfprog[\"stw_pred\"]/17.7)**3) * 0.8808\n",
    "\n",
    "# dfprog.head(n=10)\n",
    "\n",
    "# # df_foc=dfprog.drop(['oceantemperature','waveheight','swellperiod','windwaveperiod','waveperiod',\n",
    "# #                     'surftemp','windwaveswellheight','swellheight','windwaveheight','swellheight',\n",
    "# #                     'windwaveheight','draught','cog','heading','windspeed','curspeed','truewinddir','truecurrentdir','trueswelldir','truewindwavedir',\n",
    "# #                     'truewavedir','gamma','nwinddir'],axis=1)\n",
    "\n",
    "# df_foc=dfprog.drop(['oceantemperature','waveperiod',\n",
    "#                     'surftemp','windwaveswellheight',\n",
    "#                     'draught','cog','heading','windspeed','curspeed',\n",
    "#                     'truewavedir'],axis=1)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams.update(plt.rcParamsDefault)\n",
    "# df_foc.hist(bins=100,figsize=(20,15))\n",
    "# plt.show()\n",
    "\n",
    "# foc_month = dfprog['foc_pred_d'].mean()\n",
    "# foc_month_a = dfprog['foc_act_d'].mean()\n",
    "# print(f\"Mean of predicted FOC in a month is {foc_month} T/d\")\n",
    "# print(f\"Mean of actual FOC in a month is {foc_month_a} T/d\")\n",
    "\n",
    "# foc_trip = dfprog['foc_pred_h'].mean() * 4\n",
    "# foc_trip_a = dfprog['foc_act_h'].mean() * 4\n",
    "# foc_trip_err = foc_trip_a - foc_trip\n",
    "# print(f\"Mean of predicted FOC in a 4h trip is {foc_trip} T\")\n",
    "# print(f\"Mean of actual FOC in a trip in a 4h trip is {foc_trip_a} T\")\n",
    "# print(f\"Error is {foc_trip_err} T\")\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.scatter(dfprog['stw_pred'], dfprog['stw_act'], c='crimson')\n",
    "# # plt.yscale('log')\n",
    "# # plt.xscale('log')\n",
    "\n",
    "# p1 = max(max(dfprog['stw_pred']), max(dfprog['stw_act']))\n",
    "# p2 = min(min(dfprog['stw_pred']), min(dfprog['stw_act']))\n",
    "# plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "# # plt.xscale('linear')\n",
    "# plt.xlim(0,25)\n",
    "# plt.ylim(0,30)\n",
    "# plt.xlabel('STW', fontsize=15)\n",
    "# plt.ylabel('FOC', fontsize=15)\n",
    "# # plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "# # define the true objective function\n",
    "# def objective(x, a, b, c):\n",
    "# \treturn a * x + b * x**2 + c\n",
    "\n",
    "# # fit a second degree polynomial to the economic data\n",
    "# from numpy import arange\n",
    "# from scipy.optimize import curve_fit\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "\n",
    "# # define the true objective function\n",
    "# def objective(x, a, b ):\n",
    "# \treturn a * x + b\n",
    "\n",
    "# # choose the input and output variables\n",
    "# x, y = dfprog.stw_pred, dfprog.stw_act\n",
    "# # x2, y2 = dfprog.stw_pred, dfprog.foc_pred_d\n",
    "# # curve fit\n",
    "# popt, _ = curve_fit(objective, x, y)\n",
    "# # popt2, _ = curve_fit(objective, x2, y2)\n",
    "\n",
    "# # summarize the parameter values\n",
    "# a, b = popt\n",
    "# print('y = %.5f * x + %.5f' % (a, b ))\n",
    "# # a2, b2, c2 = popt2\n",
    "# # print('y = %.5f * x + %.5f * x^2 + %.5f' % (a2, b2, c2))\n",
    "# # plot input vs output\n",
    "# plt.scatter(x, y)\n",
    "# # plt.scatter(x2, y2)\n",
    "# # define a sequence of inputs between the smallest and largest known inputs\n",
    "# x_line = arange(min(x), max(x), 1)\n",
    "# # x_line2 = arange(min(x), max(x), 1)\n",
    "# # calculate the output for the range\n",
    "# y_line = objective(x_line, a, b)\n",
    "# # y_line2 = objective(x_line2, a2, b2, c2)\n",
    "# # create a line plot for the mapping function\n",
    "# plt.plot(x_line, y_line, color='red')\n",
    "# # plt\t.plot(x_line2, y_line2, color='green')\n",
    "# plt.xlabel('STWpred', fontsize=15)\n",
    "# plt.ylabel('STWact', fontsize=15)\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holtrop Mennen Messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import math\n",
    "# import pandas as pd\n",
    "\n",
    "# # List of basic ship information\n",
    "\n",
    "# loa = 158 # ship overall length\n",
    "# lwl = 144.8 # ship waterline length, m\n",
    "# lpp = 0.97*lwl # ship perpendicular length , m, according to information\n",
    "# B = 24.5 # Ship B, m\n",
    "# depth = 13.8 # Ship depth. m\n",
    "# T_n = 5.85 # Nominal max draught , m\n",
    "# dwt = 5110 # ship dead weight , t\n",
    "# V_n = 17.7 # ship design speed, knots\n",
    "\n",
    "# #list of constants\n",
    "\n",
    "# g = 9.805 # gravity, kg/ms^2 \n",
    "# rho_sea = 1025 # kg/m3\n",
    "# nu_sea = 0.00000118 # Dynamic viscosity of sea m^2/s\n",
    "# dfprog['stw_pred_ms'] = dfprog['stw_pred'] / 1.94384\n",
    "# # Calculation for C_b according to Schneekluth and Bertram 1998\n",
    "# # first calculate froude number Fn\n",
    "# V_n = 17.7/1.94384\n",
    "# Fr_n = V_n / math.sqrt(g*lpp)\n",
    "# print(f\"Froude Number {Fr_n:0.4f}\")\n",
    "# C_b = -4.22 + 27.8*math.sqrt(Fr_n) - 39.1*Fr_n + 46.6*(Fr_n)**3\n",
    "# print(f\"C_b {C_b:0.4f}\")\n",
    "# ########################################################\n",
    "\n",
    "\n",
    "# # calculation for C_m according to charchalis 2017\n",
    "\n",
    "# C_m = 0.977 + 0.085*(C_b-0.6)\n",
    "# print(f\"C_m {C_m:0.4f}\")\n",
    "\n",
    "# # prismatic coefficient C_p can be calculated\n",
    "\n",
    "# C_p = C_b/C_m \n",
    "# print(f\"C_p {Fr_n:0.4f}\")\n",
    "\n",
    "# # Displacement calculation according to barras\n",
    "# # Use approximate value cd=0.35 according to Barras\n",
    "\n",
    "# dsp = dwt/0.35 # m^3\n",
    "# print(f\"dsp {dsp:0.4f} m^3\")\n",
    "\n",
    "# # coefficient c14 to account for stern shape according to holtrop mennen\n",
    "\n",
    "# C_stern = 10 # assume u shaped stern\n",
    "# c14 = 1 + 0.011*C_stern \n",
    "# print(f\"c14 {c14:0.4f}\")\n",
    "\n",
    "# # Calculate length of run according to holtrop mennen\n",
    "\n",
    "# lcb = -2/100 # ratio lcb wrt to amidship ford, value from barras\n",
    "# # L in holtrop mennen is lwl\n",
    "# lr = lwl*(1-C_p+(0.06*C_p*lcb/(4*C_p-1)))\n",
    "# print(f\"lr {lr:0.4f} m\")\n",
    "\n",
    "# # now the (1+k1) can be calculated\n",
    "\n",
    "# k1a = 0.487118*c14*(B/lwl)**1.06806\n",
    "# k1b = (T_n/lwl)**0.46106\n",
    "# k1c = (lwl/lr)**0.121563\n",
    "# k1d = (lwl**3/dsp)**0.36486\n",
    "# k1e = (1-C_p)**-0.604247\n",
    "\n",
    "# k1_const = 0.93 + k1a*k1b*k1c*k1d*k1e\n",
    "\n",
    "# print(f\"k1_const {k1_const:0.4f}\")\n",
    "# # we can then calculate R_f\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# # use v mean = 10 for Reynold\n",
    "\n",
    "\n",
    "# dfprog['Re'] =( dfprog['stw_pred_ms'] * lwl ) / nu_sea\n",
    "# dfprog['C_f'] = 0.075 / (np.log10(dfprog['Re']-2)**2)\n",
    "# # print(dfprog['Re'])\n",
    "# # print(dfprog['C_f'])\n",
    "\n",
    "# # calculate appendage area of bare hull S_bh\n",
    "\n",
    "# # first calculate C_wp acc. to schneekluth and bertram\n",
    "# C_wp = (1+2*C_b)/3\n",
    "# print(f\"C_wp {C_wp:0.4f}\")\n",
    "\n",
    "# # A_bt according to kim\n",
    "\n",
    "# A_m = B*T_n*C_m\n",
    "# A_t = 0.051 * A_m\n",
    "# A_bt = 0.08*A_m\n",
    "# print(f\"A_m {A_m:0.4f} m^2\")\n",
    "# print(f\"A_t {A_t:0.4f} m^2\")\n",
    "# print(f\"A_bt {A_bt:0.4f} m^2\")\n",
    "\n",
    "\n",
    "# sbh_a = lwl*(2*T_n+B)*math.sqrt(C_m)\n",
    "# sbh_b = 0.453\n",
    "# sbh_c = 0.4425*C_b\n",
    "# sbh_d = 0.2862*C_m\n",
    "# sbh_e = 0.003467*(B/T_n)\n",
    "# sbh_f = 0.3696*C_wp\n",
    "# sbh_g = 2.38*A_bt/C_b\n",
    "\n",
    "# S_bh = sbh_a*(sbh_b+sbh_c-sbh_d+sbh_e+sbh_f)+sbh_g\n",
    "# print(f\"S_bh {S_bh:0.4f} m^2\")\n",
    "\n",
    "# dfprog['R_f'] = 0.5 * rho_sea * (dfprog['stw_pred_ms'])**2 * dfprog['C_f'] * S_bh * k1_const\n",
    "# # print(dfprog['R_f'])\n",
    "\n",
    "\n",
    "# # Now resistance due to appendage\n",
    "# # Assume Sapp\n",
    "\n",
    "# S_app = 50 # m^2 \n",
    "# # from holtrop mennen\n",
    "# k2_const = 2.8\n",
    "\n",
    "# dfprog['R_app'] = 0.5 * rho_sea * (dfprog['stw_pred_ms'])**2 * (dfprog['C_f']) * S_app *k2_const\n",
    "# # print(dfprog['R_app'])\n",
    "\n",
    "# # print(B/lwl)\n",
    "# # print(lwl/B)\n",
    "# # print(lwl**3/dsp)\n",
    "# # calculate wave resistance\n",
    "# c7 = B/lwl\n",
    "# T_fwd = T_n # assume !\n",
    "# h_b = 0.6*T_fwd\n",
    "# c3 = 0.56 * A_bt**1.5 / (B*T_n*(0.31*math.sqrt(A_bt)+T_fwd-h_b))\n",
    "# # print(c3)\n",
    "# c2 = math.exp(-1.89*math.sqrt(c3))\n",
    "# # print(c2)\n",
    "# c5 = 1 - 0.8*(A_t/(B*T_n*C_m))\n",
    "# # print(c5)\n",
    "# lambda_const = (1.446 * C_p) - 0.03*(lwl/B)\n",
    "# # print(lambda_const)\n",
    "# c16 = 8.07981*C_p - 13.8673*C_p**2 + 6.984388*C_p**3\n",
    "# # print(c16)\n",
    "# m_1 = 0.0140407 * (lwl/T_n) - 1.75254*(dsp**(1/3)/lwl) -  4.79323*(B/lwl) - c16\n",
    "# # print(m_1)\n",
    "# c15 = -1.69385\n",
    "# # Use dynamic Froude here\n",
    "# dfprog['Fr_n'] = dfprog['stw_pred_ms'] / math.sqrt(g*lpp)\n",
    "# dfprog['m_2'] = c15 * C_p**2 * np.exp(-0.1*dfprog['Fr_n']**-2)\n",
    "# # print(dfprog['m_2'])\n",
    "# i_e = 1 + 89*math.exp(-(lwl/B)**0.80856*(1-C_wp)**0.30484*(1-C_p-0.0225*lcb)**0.6367*(lr/B)**0.34574*((100*dsp)/lwl**3)**0.16302)\n",
    "# # print(i_e)\n",
    "# c1 = 2223105 * c7**3.78613 * (T_n/B)**1.07961*(90-i_e)**-1.37565\n",
    "# # print(c1)\n",
    "# d = -0.9\n",
    "\n",
    "# dfprog['R_w'] = c1*c2*c5*dsp*g*rho_sea*np.exp(m_1*dfprog['Fr_n']**d+dfprog['m_2']*np.cos(lambda_const*dfprog['Fr_n']**-2))\n",
    "# # print(dfprog['R_w'])\n",
    "\n",
    "# # resistance due to bulbous bow\n",
    "# # use V = 10 m/s\n",
    "\n",
    "# P_b = 0.56*math.sqrt(A_bt)/(T_fwd-1.5*h_b)\n",
    "# # print(P_b)\n",
    "# dfprog['F_ni'] = dfprog['stw_pred_ms'] / np.sqrt(g*(T_fwd-h_b-0.25*math.sqrt(A_bt))+0.15*dfprog['stw_pred_ms']**2)\n",
    "# dfprog['R_b'] = 0.11 * np.exp(-3*P_b**-2)*dfprog['F_ni']**3*A_bt**1.5*rho_sea*g/(1+dfprog['F_ni']**2)\n",
    "# # print(dfprog['R_b'])\n",
    "# #Calculate Transom Resistance \n",
    "# # use v = 10\n",
    "\n",
    "# F_nT = 10 / math.sqrt(2*g*A_t/(B+B*C_wp))\n",
    "# print(f\"Froude for Transom {F_nT:0.4f}\")\n",
    "# # Due to condition set by holtrop mennen c6 = 0\n",
    "# c6 = 0\n",
    "# R_tr = 0.5*rho_sea*10**2*A_t*c6\n",
    "# print(f\"R_tr = {R_tr:0.4f} N\")\n",
    "# # Model ship correlation resistance\n",
    "\n",
    "# c4 = T_fwd/lwl\n",
    "# # print(c4)\n",
    "# # since c4>0.04, use c4 as c4 = 0.04\n",
    "# c4 = 0.04\n",
    "# C_a = 0.006*(lwl+100)**-0.16 - 0.00205 + 0.003*math.sqrt(lwl/7.5)*C_b**4*c2*(0.04-c4)\n",
    "# # print(C_a)\n",
    "# dfprog['R_a'] = 0.5*rho_sea*dfprog['stw_pred_ms']**2*S_bh*C_a\n",
    "# # print(dfprog['R_a'])\n",
    "\n",
    "# # Calculate Additional Resistance\n",
    "\n",
    "# # Calculate first apparent velocity \n",
    "\n",
    "# dfprog['V_aw'] = np.sqrt(dfprog['windspeed']**2 + dfprog['stw_pred_ms']**2 + 2*dfprog['windspeed']*dfprog['stw_pred_ms']*np.cos(np.deg2rad(dfprog['truewinddir'])))\n",
    "\n",
    "# dfprog['awa_c1'] = (dfprog['windspeed']/dfprog['V_aw'])*np.sin(np.deg2rad(dfprog['truewinddir']))\n",
    "\n",
    "# # Epsilon is Apparent Wind Angle AWA\n",
    "\n",
    "# dfprog['epsilon'] = np.rad2deg(np.arcsin(dfprog['awa_c1']))\n",
    "\n",
    "# C_DlAf = 0.45\n",
    "# # Values from Blendermann\n",
    "# A_f = 325.3\n",
    "# A_l = 2125.8\n",
    "# C_Dt = 0.9\n",
    "# delta = 0.8\n",
    "# C_Dl = C_DlAf * A_f / A_l\n",
    "# rho_air = 1.25\n",
    "# L_bwl = 30 # ASSUME FROM PICTURE\n",
    "\n",
    "# dfprog['Raa_const1'] = (rho_air/2) * dfprog['V_aw']**2 * A_l * C_Dl\n",
    "# dfprog['Raa_const2'] = np.cos(np.deg2rad(dfprog['epsilon']))\n",
    "# dfprog['Raa_const3'] = 1 - (delta/2) * ((1-(C_Dl/C_Dt))*(np.sin(np.deg2rad(2*dfprog['epsilon'])))**2)\n",
    "\n",
    "# dfprog['R_aa'] = dfprog['Raa_const1'] * dfprog['Raa_const2'] / dfprog['Raa_const3'] \n",
    "\n",
    "# # Calculate Wave Resistance\n",
    "\n",
    "# Rawl = 1/16 * rho_sea * g * dfprog['windwaveswellheight']**2 * math.sqrt(B/L_bwl)\n",
    "\n",
    "# condwave = [dfprog['truewavedir']<=45]\n",
    "# choicewave = [Rawl]\n",
    "\n",
    "# dfprog['R_awl'] = np.select(condwave,choicewave,0)\n",
    "\n",
    "\n",
    "\n",
    "# print(dfprog)\n",
    "\n",
    "\n",
    "# # Calculate Rtot\n",
    "\n",
    "# dfprog['R_tot'] = dfprog['R_f'] + dfprog['R_app'] + dfprog['R_w'] + dfprog['R_b'] + dfprog['R_a']  + dfprog['R_aa'] + dfprog['R_awl']\n",
    "# dfprog['R_tot'] = dfprog['R_tot'] / 1e3\n",
    "# # dfprog.head()\n",
    "# df_prog=dfprog.drop(['C_f','Re','F_ni'],axis=1)\n",
    "# # Calculate Efficiencies\n",
    "\n",
    "# # First n_h\n",
    "# # Approximate diameter size from Bertram\n",
    "\n",
    "# D = 0.215*16 #m\n",
    "# # D = 8\n",
    "# # print(D)\n",
    "# # print(C_b)\n",
    "# # print(k1_const)\n",
    "# # print(B)\n",
    "# # print(T_n)\n",
    "\n",
    "# dfprog['C_v'] = k1_const*dfprog['C_f'] + C_a\n",
    "# # print(dfprog['C_v'])\n",
    "# dfprog['w'] = 0.3095 * C_b + 10*dfprog['C_v']*C_b - (0.23*D)/math.sqrt(B*T_n) \n",
    "# # print(dfprog['w'])\n",
    "# dfprog['t'] = 0.325*C_b - 0.1885*D/math.sqrt(B*T_n)\n",
    "# # print(dfprog['t'])\n",
    "# dfprog['eff_h'] = (1-dfprog['t']) / (1-dfprog['w'])\n",
    "# # print(dfprog['eff_h'])\n",
    "\n",
    "# # Then n_r\n",
    "# dfprog['eff_r'] = 0.9737 + 0.111*(C_p - 0.225*lcb) - 0.06325*lcb\n",
    "# # print(dfprog['eff_r'])\n",
    "\n",
    "# dfprog['eff_s'] = 0.99 # Set according to holtrop mennen and man\n",
    "# dfprog['eff_o'] = 0.75 # Approx wageningen kristensen\n",
    "\n",
    "# dfprog['eff_tot'] = dfprog['eff_h']* dfprog['eff_r']* dfprog['eff_s']*dfprog['eff_o']*0.85 # consider sea margin\n",
    "# # print(df_prog)\n",
    "# dfprog['P_b'] = (dfprog['R_tot'] * dfprog['stw_pred_ms'])/dfprog['eff_tot'] # in kW\n",
    "# # dfprog.head()\n",
    "# # Unit P_b is KW\n",
    "# SFOC = 169.4 # g/kWh\n",
    "# dfprog['FOC'] = (dfprog['P_b'] * SFOC)/1e6 # get FOC t/h\n",
    "# dfprog.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest number of features effect on performance\n",
    "# import numpy\n",
    "# from numpy import mean\n",
    "# from numpy import std\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from matplotlib import pyplot\n",
    "# pyplot.rcParams.update({\n",
    "#     \"font.family\": \"serif\",\n",
    "#     \"text.usetex\": True,\n",
    "#     'text.latex.preamble': [\n",
    "#         r'\\usepackage{amsmath}',\n",
    "#         r'\\usepackage{amssymb}',\n",
    "#         r\"\\usepackage{siunitx}\",\n",
    "#         r\"\\usepackage[notextcomp]{kpfonts}\",\n",
    "#      ]\n",
    "# })\n",
    "\n",
    "\n",
    "# # get the dataset\n",
    "# def get_dataset():\n",
    "# \tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# \treturn X, y\n",
    "\n",
    "# # get a list of models to evaluate\n",
    "# def get_models():\n",
    "# \tmodels = dict()\n",
    "# \t# explore number of features from 1 to 7\n",
    "# \tfor i in range(1,8):\n",
    "# \t\tmodels[str(i)] = RandomForestClassifier(max_features=i)\n",
    "# \treturn models\n",
    "\n",
    "# # evaluate a given model using cross-validation\n",
    "# def evaluate_model(model, X, y):\n",
    "# \t# define the evaluation procedure\n",
    "# \tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# \t# evaluate the model and collect the results\n",
    "# \tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# \treturn scores\n",
    "\n",
    "# # define dataset\n",
    "# X, y = get_dataset()\n",
    "# # get the models to evaluate\n",
    "# models = get_models()\n",
    "# # evaluate the models and store results\n",
    "# # results, names , means = list(), list(), list()\n",
    "# results, names  = list(), list()\n",
    "\n",
    "# for name, model in models.items():\n",
    "# \t# evaluate the model\n",
    "# \tscores = evaluate_model(model, X, y)\n",
    "# \t# store the results\n",
    "# \tresults.append(scores)\n",
    "# \tnames.append(name)\n",
    "# \t# means.append(a)\n",
    "# \t# summarize the performance along the way\n",
    "# \tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# \ta = numpy.mean(results,axis=1)\n",
    "# # plot model performance for comparison\n",
    "# a = numpy.mean(results,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# pyplot.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"serif\",\n",
    "#     \"font.sans-serif\": \"kpfonts\",\n",
    "# })\n",
    "\n",
    "# pyplot.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"serif\",\n",
    "#     \"font.sans-serif\": \"charter\",\n",
    "# })\n",
    "\n",
    "# pyplot.boxplot(results, labels=names,positions=range(len(names)) , showmeans=True)\n",
    "# plt.xlabel(r\"Number of features\",fontsize=18)\n",
    "# plt.ylabel(r\"RMSE\",fontsize=18)\n",
    "# plt.title(r\"Validation error\", fontsize=18)\n",
    "# pyplot.plot(names,a,\"b.-\")\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(5,3))\n",
    "# fig,ax = plt.subplots() \n",
    "# # ax.boxplot(results,showmeans=True)\n",
    "# # ax.set_xticklabels(names)\n",
    "# ax.plot(names,a,\"b.-\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.preprocessing import StandardScaler \n",
    "# ann_regr = MLPRegressor(activation='relu',alpha=10e-8,hidden_layer_sizes=(10,100),random_state=1,early_stopping=False,max_iter=500)\n",
    "# # ann_regr = MLPRegressor(random_state=1,max_iter=500)\n",
    "# # regressor_mlsc = linear_model.LinearRegression()\n",
    "# start_ann = time.time()\n",
    "# # model_ann = TransformedTargetRegressor(regressor= ann_regr,\n",
    "# #                                         transformer = StandardScaler()\n",
    "# #                                         ).fit(x_train,y_train)\n",
    "# model_ann = ann_regr.fit(x_train,y_train)\n",
    "# end_ann = time.time()\n",
    "# print(f\"Training time: {end_ann-start_ann:0.4}s \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, test_features, test_labels):\n",
    "#     from sklearn.metrics import explained_variance_score,mean_absolute_error,mean_squared_error,median_absolute_error\n",
    "\n",
    "#     predictions = model.predict(test_features)\n",
    "#     rsquared = model.score(test_features,test_labels)\n",
    "#     expVar = explained_variance_score(test_labels,predictions)\n",
    "#     MAE = mean_absolute_error(test_labels,predictions)\n",
    "#     MAD = median_absolute_error(test_labels,predictions)\n",
    "#     RMSE = np.sqrt(mean_squared_error(test_labels,predictions))\n",
    "\n",
    "#     print(f\"Model Performance of {model}\")\n",
    "#     print(f\"R^2: {rsquared:0.4f}\")\n",
    "#     print(f\"explained Variance = {expVar:0.4f}\")\n",
    "#     print(f\"MAE = {MAE:0.4f}\")\n",
    "#     print(f\"RMSE = {RMSE:0.4f}\")\n",
    "#     print(f\"MAD = {MAD:0.4f}\\n\")\n",
    "    \n",
    "#     # return rsquared,expVar,MAE,RMSE,MAD,predictions\n",
    "# rf_rsquared,rf_expVar,rf_MAE,rf_RMSE,rf_MAD,y_pred_rfr = evaluate(model_rfr_ftr,x_date,y_date)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE Error Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_rf_hpo = cross_val_score(model_rfr_ftr_hpov,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_et_hpo = cross_val_score(model_etr_hpov,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_dt_hpo = cross_val_score(model_dtr_hpov,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# score_rf = cross_val_score(model_rfr_ftr,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_et = cross_val_score(model_etr,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_dt = cross_val_score(model_dtr,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mlr = cross_val_score(model_mlr,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_sqr = score_rf.mean()\n",
    "# print(r_sqr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit a second degree polynomial to the economic data\n",
    "# from numpy import arange\n",
    "# from scipy.optimize import curve_fit\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "\n",
    "# # define the true objective function\n",
    "# def objective(x, a, b ):\n",
    "# \treturn a * x + b\n",
    "\n",
    "# def label_predict(model,test_features):\n",
    "#     predictions = model.predict(test_features)\n",
    "#     return predictions\n",
    "\n",
    "# y = label_predict(model_mlr_ftr,x_date)\n",
    "# y2 = label_predict(model_rfr_hpov,x_date)\n",
    "\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# print(r2_score(y_date, y))\n",
    "# print(r2_score(y_date,y2))\n",
    "\n",
    "# # choose the input and output variables\n",
    "# x, y = y_date, y\n",
    "# x2, y2 = y_date, y2\n",
    "# # curve fit\n",
    "# popt, _ = curve_fit(objective, x, y)\n",
    "# popt2, _ = curve_fit(objective, x2, y2)\n",
    "\n",
    "# # summarize the parameter values\n",
    "# a, b = popt\n",
    "# print('y = %.5f * x + %.5f' % (a, b ))\n",
    "# a2, b2 = popt2\n",
    "# print('y = %.5f * x + %.5f' % (a2, b2))\n",
    "# # plot input vs output\n",
    "# plt.scatter(x, y)\n",
    "# plt.scatter(x2, y2)\n",
    "# # define a sequence of inputs between the smallest and largest known inputs\n",
    "# x_line = arange(min(x), max(x), 1)\n",
    "# x_line2 = arange(min(x), max(x), 1)\n",
    "# # calculate the output for the range\n",
    "# y_line = objective(x_line, a, b)\n",
    "# y_line2 = objective(x_line2, a2, b2)\n",
    "# # create a line plot for the mapping function\n",
    "# plt.plot(x_line, y_line, color='red')\n",
    "# plt\t.plot(x_line2, y_line2, color='green')\n",
    "# plt.xlabel('STWpred', fontsize=15)\n",
    "# plt.ylabel('STWact', fontsize=15)\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# plt.scatter(y_test, y_predicted, c='crimson')\n",
    "# # plt.yscale('log')\n",
    "# # plt.xscale('log')\n",
    "\n",
    "# p1 = max(max(y_predicted), max(y_test))\n",
    "# p2 = min(min(y_predicted), min(y_test))\n",
    "# plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "# plt.xlabel('True Values', fontsize=15)\n",
    "# plt.ylabel('Predictions', fontsize=15)\n",
    "# plt.axis('equal')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth vs n estimator plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest number of trees effect on performance\n",
    "# from numpy import mean\n",
    "# from numpy import std\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# # get the dataset\n",
    "# # def get_dataset():\n",
    "# # \tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# # \treturn X, y\n",
    "\n",
    "# # get a list of models to evaluate\n",
    "# def get_models():\n",
    "# \tmodels_m2 = dict()\n",
    "# \t# define number of trees to consider\n",
    "# \t# n_trees = [1,2,3,4,5,10,50,100,250,500,750,1000]\n",
    "# \tn_trees = [1,2,3,4,5,6,7,8,9,10,100,200,300,400,500,600,700,800,900,1000]\n",
    "\n",
    "# \tfor n in n_trees:\n",
    "# \t\t# models[str(n)] = RandomForestRegressor(n_estimators=n)\n",
    "# \t\tmodels_m2[str(n)] = RandomForestRegressor(n_estimators=n,max_depth=1)\n",
    "# \treturn models_m2\n",
    "\n",
    "# # get a list of models to evaluate\n",
    "# def get_models6():\n",
    "# \tmodels_m6 = dict()\n",
    "# \t# define number of trees to consider\n",
    "# \t# n_trees = [1,2,3,4,5,10,50,100,250,500,750,1000]\n",
    "# \tn_trees = [1,2,3,4,5,6,7,8,9,10,100,200,300,400,500,600,700,800,900,1000]\n",
    "\n",
    "# \tfor n in n_trees:\n",
    "# \t\t# models[str(n)] = RandomForestRegressor(n_estimators=n)\n",
    "# \t\tmodels_m6[str(n)] = RandomForestRegressor(n_estimators=n,max_depth=6)\n",
    "# \treturn models_m6\n",
    "\n",
    "\n",
    "# # evaluate a given model using cross-validation\n",
    "# def evaluate_model(model, X, y):\n",
    "# \t# define the evaluation procedure\n",
    "# \t# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# \t# evaluate the model and collect the results\n",
    "# \tscores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n",
    "# \treturn -scores\n",
    "\n",
    "# # # define dataset\n",
    "# # X, y = get_dataset()\n",
    "# # get the models to evaluate\n",
    "# # models = get_models()\n",
    "# models_m2 = get_models()\n",
    "# models_m6 = get_models6()\n",
    "# # evaluate the models and store results\n",
    "# results, names = list(), list()\n",
    "# for name, model_m2 in models_m2.items():\n",
    "# \t# evaluate the model\n",
    "# \tscores = evaluate_model(model_m2, x_date, y_date)\n",
    "# \t# store the results\n",
    "# \tresults.append(scores)\n",
    "# \tnames.append(name)\n",
    "# \t# summarize the performance along the way\n",
    "# \tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "# results6, names6 = list(), list()\n",
    "# for name6, model_m6 in models_m6.items():\n",
    "# \t# evaluate the model\n",
    "# \tscores6 = evaluate_model(model_m6, x_date, y_date)\n",
    "# \t# store the results\n",
    "# \tresults6.append(scores6)\n",
    "# \tnames6.append(name6)\n",
    "# \t# summarize the performance along the way\n",
    "# \tprint('>%s %.3f (%.3f)' % (name6, mean(scores6), std(scores6)))\n",
    "\n",
    "\n",
    "# # plot model performance for comparison\n",
    "# # pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "# # pyplot.show()\n",
    "\n",
    "# a = np.mean(results,axis=1)\n",
    "# b = np.mean(results6,axis=1)\n",
    "# min_error = np.min(a)\n",
    "# min_error6 = np.min(b)\n",
    "# print(min_error)\n",
    "\n",
    "# plt.plot(names,a,\"b.-\")\n",
    "# bst_n_estimators = np.argmin(a) \n",
    "# print(bst_n_estimators)\n",
    "# # plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\",linewidth=1)\n",
    "# # plt.plot([-1, 12], [min_error, min_error], \"k--\",linewidth=1)\n",
    "# # plt.plot(bst_n_estimators, min_error, \"ko\",linewidth = 1)\n",
    "# # plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=12)\n",
    "\n",
    "\n",
    "# plt.plot(names,b,\"r.-\")\n",
    "# bst_n_estimators6 = np.argmin(b) \n",
    "# print(bst_n_estimators6)\n",
    "# # plt.plot([bst_n_estimators6, bst_n_estimators6], [0, min_error6], \"k--\",linewidth=1)\n",
    "# # plt.plot([-1, 12], [min_error6, min_error6], \"k--\",linewidth=1)\n",
    "# # plt.plot(bst_n_estimators6, min_error6, \"ko\",linewidth = 1)\n",
    "# # plt.text(bst_n_estimators6, min_error6*1.2, \"Minimum\", ha=\"center\", fontsize=12)\n",
    "\n",
    "# # plt.plot([names, max_score], \"k--\")\n",
    "# # plt.plot(names, max_score, \"ko\")\n",
    "# # plt.axis([0, 12, 0, 1])\n",
    "# plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# # plt.xlim(0,12-1)\n",
    "# plt.xlim([0,1000])\n",
    "# # plt.ylim(0,3)\n",
    "# plt.xlabel(\"Number of Trees\")\n",
    "# plt.ylabel(\"RMSE\")\n",
    "# plt.title(\"Validation error\", fontsize=13)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "\n",
    "# # def tree_visualization(width,height,model_type):\n",
    "# #     fn=x_train.columns\n",
    "# #     fig, axes = plt.subplots(figsize = (width,height), dpi=800)\n",
    "# #     tree.plot_tree(model_type.estimators_[0],\n",
    "# #                max_depth=3,\n",
    "# #                fontsize=2,\n",
    "# #                feature_names = fn);\n",
    "# #     plt.show()\n",
    "# #     #fig.savefig('rf_individualtree.png')\n",
    "\n",
    "# # tree_visualization(5.5,3,model_rfr_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hist plot depreciated\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams.update(plt.rcParamsDefault)\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"serif\",\n",
    "# })\n",
    "# df_ship2.hist(bins=50,figsize=(15,10))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest number of features effect on performance v1 no boxplot\n",
    "# def feature_curve(x,y,regressor):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "# \tfrom sklearn.model_selection import KFold\n",
    "\n",
    "# \tplt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t})\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_modelsftr():\n",
    "# \t\tmodels_ftr = dict()\n",
    "# \t\t# explore number of features from 1 to 13\n",
    "# \t\tfor n in range(1,13):\n",
    "# \t\t\tif regressor == 'dt':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = DecisionTreeRegressor(max_features=n)\n",
    "# \t\t\telif regressor == 'rf':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = RandomForestRegressor(max_features=n)\n",
    "# \t\t\telif regressor == 'et':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = ExtraTreesRegressor(max_features=n)\t\n",
    "# \t\treturn models_ftr\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model, x, y):\n",
    "# \t\t# define the evaluation procedure\n",
    "# \t\tcv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# \t\t# evaluate the model and collect the results\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "# \t\t# negative scores due to scoring mechanism of sklearn\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_ftr = get_modelsftr()\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_ftr, names_ftr = list(), list()\n",
    "# \tfor name, model in models_ftr.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_ftr = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_ftr.append(scores_ftr)\n",
    "# \t\tnames_ftr.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_ftr), std(scores_ftr)))\n",
    "\t\n",
    "# \t# Calculate mean for the x value of the plot\n",
    "\n",
    "# \tmean_ftr = np.mean(results_ftr,axis=1)\n",
    "# \tmin_error_ftr = np.min(mean_ftr)\n",
    "# \tbst_n_estimators = np.argmin(mean_ftr) \n",
    "\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_ftr:.3f}\")\n",
    "\n",
    "# \tplt.plot(names_ftr,mean_ftr,\"b.-\")\n",
    "# \tplt.plot([bst_n_estimators, bst_n_estimators], [0, min_error_ftr], \"k--\",linewidth=1)\n",
    "# \tplt.plot([-1, 12], [min_error_ftr, min_error_ftr], \"k--\",linewidth=1)\n",
    "# \tplt.plot(bst_n_estimators, min_error_ftr, \"ko\",linewidth = 1)\n",
    "# \tplt.text(bst_n_estimators, min_error_ftr*1.3, \"Minimum\", ha=\"center\", fontsize=14,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \tplt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,3)\n",
    "# \tplt.xlabel(r\"Number of features\",fontsize=18)\n",
    "# \tplt.ylabel(r\"RMSE\",fontsize=18)\n",
    "# \tplt.title(r\"Validation error\", fontsize=18)\n",
    "# \t# plt.text(4,2.5, r'def_param : \\tt{max_features = n_features}', bbox={'facecolor' : 'white','alpha':1},fontsize=18)\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest number of features effect on performance v1 no boxplot\n",
    "# def feature_curve(x,y,regressor):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "# \tfrom sklearn.model_selection import KFold\n",
    "\n",
    "# \tplt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t})\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_modelsftr():\n",
    "# \t\tmodels_ftr = dict()\n",
    "# \t\t# explore number of features from 1 to 13\n",
    "# \t\tfor n in range(1,13):\n",
    "# \t\t\tif regressor == 'dt':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = DecisionTreeRegressor(max_features=n)\n",
    "# \t\t\telif regressor == 'rf':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = RandomForestRegressor(max_features=n)\n",
    "# \t\t\telif regressor == 'et':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = ExtraTreesRegressor(max_features=n)\t\n",
    "# \t\treturn models_ftr\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model, x, y):\n",
    "# \t\t# define the evaluation procedure\n",
    "# \t\tcv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# \t\t# evaluate the model and collect the results\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "# \t\t# negative scores due to scoring mechanism of sklearn\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_ftr = get_modelsftr()\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_ftr, names_ftr = list(), list()\n",
    "# \tfor name, model in models_ftr.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_ftr = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_ftr.append(scores_ftr)\n",
    "# \t\tnames_ftr.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_ftr), std(scores_ftr)))\n",
    "\t\n",
    "# \t# Calculate mean for the x value of the plot\n",
    "\n",
    "# \tmean_ftr = np.mean(results_ftr,axis=1)\n",
    "# \tmin_error_ftr = np.min(mean_ftr)\n",
    "# \tbst_n_estimators = np.argmin(mean_ftr) \n",
    "\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_ftr:.3f}\")\n",
    "\n",
    "# \tplt.plot(names_ftr,mean_ftr,\"b.-\")\n",
    "# \tplt.plot([bst_n_estimators, bst_n_estimators], [0, min_error_ftr], \"k--\",linewidth=1)\n",
    "# \tplt.plot([-1, 12], [min_error_ftr, min_error_ftr], \"k--\",linewidth=1)\n",
    "# \tplt.plot(bst_n_estimators, min_error_ftr, \"ko\",linewidth = 1)\n",
    "# \tplt.text(bst_n_estimators, min_error_ftr*1.3, \"Minimum\", ha=\"center\", fontsize=14,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \t# plt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,4)\n",
    "# \tplt.xlabel(r\"Number of features\",fontsize=18)\n",
    "# \tplt.ylabel(r\"RMSE\",fontsize=18)\n",
    "# \tplt.title(r\"Validation error\", fontsize=18)\n",
    "# \tplt.boxplot(results_ftr, labels=names_ftr,positions=range(len(names_ftr)), showmeans=True)\n",
    "# \t# plt.text(4,2.5, r'def_param : \\tt{max_features = n_features}', bbox={'facecolor' : 'white','alpha':1},fontsize=18)\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instead , we will invesitagte the effect of setting the minimal samples leaf\n",
    "# def leaf_curve(x,y,regressor,regname):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "\n",
    "# \tplt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t\"font.sans-serif\": \"bookman\",\n",
    "# \t})\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_models_leaf():\n",
    "# \t\tmodels_leaf = dict()\n",
    "# \t\t# define number of trees to consider\n",
    "# \t\tn_samples_leaf = [1,2,3,4,5,6,7,8,9,10,50,100]\n",
    "# \t\tfor n in n_samples_leaf:\n",
    "# \t\t\tif regressor == 'dt':\n",
    "# \t\t\t\tmodels_leaf[str(n)] = RandomForestRegressor(n_estimators = n)\n",
    "# \t\treturn models_leaf\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model, x, y):\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# # define dataset\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_leaf = get_models_leaf()\n",
    "\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_leaf, names_leaf = list(), list()\n",
    "# \tfor name, model in models_leaf.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_leaf = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_leaf.append(scores_leaf)\n",
    "# \t\tnames_leaf.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_leaf), std(scores_leaf)))\n",
    "\n",
    "\n",
    "# \tmean_leaf = np.mean(results_leaf,axis=1)\n",
    "# \tmin_error_leaf = np.min(mean_leaf)\n",
    "# \tprint(min_error_leaf)\n",
    "# \tbst_n_estimators_leaf = np.argmin(mean_leaf)\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_leaf:.3f}\")\n",
    "# \tplt.plot(names_leaf,mean_leaf,\"b.-\")\n",
    "# \tplt.plot([bst_n_estimators_leaf, bst_n_estimators_leaf], [0, min_error_leaf], \"k--\",linewidth=1)\n",
    "# \tplt.plot([-1, 12-1], [min_error_leaf, min_error_leaf], \"k--\",linewidth=1)\n",
    "# \tplt.plot(bst_n_estimators_leaf, min_error_leaf, \"ko\",linewidth = 1)\n",
    "# \tplt.text(bst_n_estimators_leaf, min_error_leaf*1.2, \"Minimum\", ha=\"center\", fontsize=12,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \tplt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,3)\n",
    "# \tplt.xlabel(\"Number of Samples in Leaf\")\n",
    "# \tplt.ylabel(\"RMSE\")\n",
    "# \tplt.title(rf\"{regname}\", fontsize=13)\n",
    "# \tplt.text(4,2.5, r'def_param : \\tt{min_samples_leaf = 1}', bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.boxplot(results_leaf, labels=names_leaf,positions=range(len(names_leaf)), showmeans=True)\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest and extra tree number of trees effect on performance\n",
    "# def trees_curve(x,y,regressor):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "\n",
    "# \tplt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t\"font.sans-serif\": \"bookman\",\n",
    "# \t})\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_models_tree():\n",
    "# \t\tmodels_tree = dict()\n",
    "# \t\t# define number of trees to consider\n",
    "# \t\tn_trees = [1,10,100,200,300,400,500,600,700,800,900,1000]\n",
    "# \t\tfor n in n_trees:\n",
    "# \t\t\tif regressor == 'rf':\n",
    "# \t\t\t\tmodels_tree[str(n)] = RandomForestRegressor(n_estimators = n)\n",
    "# \t\t\telif regressor == 'et':\n",
    "# \t\t\t\tmodels_tree[str(n)] = ExtraTreesRegressor(n_estimators = n)\t\n",
    "# \t\treturn models_tree\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model_tree, x, y):\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# # define dataset\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_tree = get_models_tree()\n",
    "\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_tree, names_tree = list(), list()\n",
    "# \tfor name, model in models_tree.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_tree = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_tree.append(scores_tree)\n",
    "# \t\tnames_tree.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_tree), std(scores_tree)))\n",
    "\n",
    "\n",
    "# \tmean_tree = np.mean(results_tree,axis=1)\n",
    "# \tmin_error_tree = np.min(mean_tree)\n",
    "# \tprint(min_error_tree)\n",
    "# \tbst_n_estimators_tree = np.argmin(mean_tree)\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_tree:.3f}\")\n",
    "# \tplt.plot(names_tree,mean_tree,\"b.-\")\n",
    "# \tplt.plot([bst_n_estimators_tree, bst_n_estimators_tree], [0, min_error_tree], \"k--\",linewidth=1)\n",
    "# \tplt.plot([-1, 12-1], [min_error_tree, min_error_tree], \"k--\",linewidth=1)\n",
    "# \tplt.plot(bst_n_estimators_tree, min_error_tree, \"ko\",linewidth = 1)\n",
    "# \tplt.text(bst_n_estimators_tree, min_error_tree*1.2, \"Minimum\", ha=\"center\", fontsize=12,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \tplt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,3)\n",
    "# \tplt.xlabel(\"Number of Trees\")\n",
    "# \tplt.ylabel(\"RMSE\")\n",
    "# \tplt.title(\"Validation error\", fontsize=13)\n",
    "# \tplt.text(4,2.5, r'def_param : \\tt{n_estimators = 100}', bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tpyplot.boxplot(results_tree, labels=names_tree, showmeans=True)\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore tree based , tree depth. effect on performance\n",
    "# def depth_curve(x,y,regressor):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "# \tfrom sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_models_dp():\n",
    "# \t\tmodels_dp = dict()\n",
    "# \t\t# consider tree depths from 1 to 7 and None=full\n",
    "# \t\tdepths = [1,2,3,4,5,6,7,8,9,10,100] + [None]\n",
    "# \t\tfor n in depths:\n",
    "# \t\t\tif regressor == 'dt':\n",
    "# \t\t\t\tmodels_dp[str(n)] = DecisionTreeRegressor(max_depth=n)\n",
    "# \t\t\telif regressor == 'rf':\n",
    "# \t\t\t\tmodels_dp[str(n)] = RandomForestRegressor(max_depth=n)\n",
    "# \t\t\telif regressor == 'et':\n",
    "# \t\t\t\tmodels_dp[str(n)] = ExtraTreesRegressor(max_depth=n)\t\n",
    "# \t\treturn models_dp\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model, x, y):\n",
    "# \t\t# define the evaluation procedure\n",
    "# \t\tcv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# \t\t# evaluate the model and collect the results\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "# \t\t# negative scores due to scoring mechanism of sklearn\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_dp = get_models_dp()\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_dp, names_dp = list(), list()\n",
    "# \tfor name, model in models_dp.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_dp = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_dp.append(scores_dp)\n",
    "# \t\tnames_dp.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_dp), std(scores_dp)))\n",
    "\n",
    "\n",
    "# \tmean_dp = np.mean(results_dp,axis=1)\n",
    "# \tmin_error_dp = np.min(mean_dp)\n",
    "# \tprint(min_error_dp)\n",
    "# \tbst_n_estimators_dp= np.argmin(mean_dp)\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_dp:.3f}\")\n",
    "# \tplt.plot(names_dp,mean_dp,\"b.-\")\n",
    "# \t# Test\n",
    "# \t# plt.errorbar(names_dp,mean_dp,results_dp)\n",
    "# \t# plt.plot([bst_n_estimators_dp, bst_n_estimators_dp], [0, min_error_dp], \"k--\",linewidth=1)\n",
    "# \t# plt.plot([-1, 12-1], [min_error_dp, min_error_dp], \"k--\",linewidth=1)\n",
    "# \t# plt.plot(bst_n_estimators_dp, min_error_dp, \"ko\",linewidth = 1)\n",
    "# \t# plt.text(bst_n_estimators_dp, min_error_dp*1.2, \"Minimum\", ha=\"center\", fontsize=14,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \t# plt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,4)\n",
    "# \tplt.xlabel(r\"Tree Depth\",fontsize=18)\n",
    "# \tplt.ylabel(r\"RMSE\",fontsize=18)\n",
    "# \tplt.title(r\"Validation error\", fontsize=18)\n",
    "# \tplt.text(4,2.5, r'def_param : \\tt{max_depth = None}', bbox={'facecolor' : 'white','alpha':1},fontsize=12)\n",
    "# \tplt.boxplot(results_dp, labels=names_dp,positions=range(len(names_dp)), showmeans=True)\n",
    "# \tplt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROM JUNE AIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# score_rf = cross_val_score(model_rfr_ftr,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_et = cross_val_score(model_etr,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_dt = cross_val_score(model_dtr_ftr,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_mlr = cross_val_score(model_mlr_ftr,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_rfopt = cross_val_score(model_rfr_hpov,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_etopt = cross_val_score(model_etr_hpov,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_dtopt = cross_val_score(model_dtr_hpov,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse_rf = cross_val_score(model_rfr_ftr,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_et = cross_val_score(model_etr,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_dt = cross_val_score(model_dtr_ftr,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_rfopt = cross_val_score(model_rfr_hpov,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_etopt = cross_val_score(model_etr_hpov,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_dtopt = cross_val_score(model_dtr_hpov,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)Check STW and SOG\n",
    "# Keep this part for SOG and STW check\n",
    "\n",
    "# df[\"vgms\"] = df[\"SOG\"]/1.9438\n",
    "# rad_gamma = np.deg2rad(df[\"True North Current Direction\"])\n",
    "# rad_cog = np.deg2rad(df[\"COG\"])\n",
    "# df[\"vgx\"] = df[\"vgms\"] * np.sin(rad_cog)\n",
    "# df[\"vcx\"] = df[\"Current Speed\"] * np.sin(rad_gamma)\n",
    "# df[\"stw_x\"] = (df[\"vgx\"] - df[\"vcx\"])\n",
    "# df[\"vgy\"] = df[\"vgms\"] * np.cos(rad_cog)\n",
    "# df[\"vcy\"] = df[\"Current Speed\"] * np.cos(rad_gamma)\n",
    "# df[\"stw_y\"] = (df[\"vgy\"] - df[\"vcy\"])\n",
    "# df[\"vwms_a\"] = np.sqrt(df[\"stw_x\"]**2 + df[\"stw_y\"]**2)\n",
    "# df[\"stw_act\"] = df[\"vwms_a\"]*1.9438\n",
    "\n",
    "# dfsog= df[df[\"SOG\"] > df[\"stw_act\"] ]\n",
    "# dfsog.head(n=10)\n",
    "# dfsog.to_csv(\"SOG_june_update.csv\")\n",
    "\n",
    "# dfstw= df[df[\"SOG\"] < df[\"stw_act\"] ]\n",
    "# dfstw.head(n=10)\n",
    "# dfstw.to_csv(\"STW_june_update.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import scipy.stats as stats\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# stw = dfprog.stw_act\n",
    "# foc = dfprog.foc_act_d\n",
    "\n",
    "# stw = stw.to_numpy()\n",
    "# foc = foc.to_numpy()\n",
    "\n",
    "# #specify degree of 3 for polynomial regression model\n",
    "# #include bias=False means don't force y-intercept to equal zero\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# #reshape data to work properly with sklearn\n",
    "# poly_features = poly.fit_transform(stw.reshape(-1,1))\n",
    "# # poly_features = lat\n",
    "\n",
    "\n",
    "# #fit polynomial regression model\n",
    "# poly_reg_model = LinearRegression()\n",
    "# poly_reg_model.fit(poly_features, stw)\n",
    "\n",
    "# #display model coefficients\n",
    "# print(poly_reg_model.intercept_, poly_reg_model.coef_)\n",
    "\n",
    "# y_predicted = label_predict(poly_reg_model,poly_features)\n",
    "# y_line \n",
    "\n",
    "# #create scatterplot of x vs. y\n",
    "# plt.scatter(stw, foc)\n",
    "# # myline = np.linspace(55.1, 55.36, 100)\n",
    "\n",
    "# # add line to show fitted polynomial regression model\n",
    "# # plt.plot(poly_features, y_predicted, color='purple')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize Single Day Journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ga_df = dfone6.drop(['Unnamed: 0','Time','Air density above oceans',\n",
    "#                     'Surface pressure','Width','Length'],axis=1)\n",
    "# ga_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ga_df = ga_df.rename({'Max wave height': 'waveheight', 'Draught': 'draught',\n",
    "#                            'SOG': 'sog', 'Wind Speed': 'windspeed', \n",
    "#                            'True Wind Direction': 'truewinddir','Temperature above oceans' : 'oceantemperature',\n",
    "#                            'COG': 'cog', 'Current Speed' : 'curspeed','True Wave Direction' : 'truewavedir',\n",
    "#                             'Swell period': 'swellperiod','Wind wave period': 'windwaveperiod','Sea surface temperature': 'surftemp',\n",
    "#                             'Combined wind waves and swell height': 'windwaveswellheight','Swell height': 'swellheight','Wind wave height': 'windwaveheight',\n",
    "#                             'Heading': 'heading','True Current Direction': 'truecurrentdir','True Swell Direction': 'trueswelldir',\n",
    "#                             'True Wind Wave Direction': 'truewindwavedir','Wave period': 'waveperiod',\n",
    "#                             'True North Wind Direction' : 'truenorthwinddir' , 'True North Current Direction' : 'truenorthcurrentdir'\n",
    "#                            }, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ga_df = ga_df[['oceantemperature','waveheight','swellperiod','windwaveperiod','waveperiod','surftemp','windwaveswellheight','swellheight','windwaveheight','draught','sog','cog','heading','windspeed','curspeed','truewinddir','truecurrentdir','trueswelldir','truewindwavedir','truewavedir', 'truenorthwinddir' , 'truenorthcurrentdir']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.stats as stats\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# lat = ga_df.LAT\n",
    "# sog = ga_df.sog\n",
    "\n",
    "# #create scatterplot\n",
    "# plt.scatter(lat, sog)\n",
    "# plt.show()\n",
    "\n",
    "# lat = lat.to_numpy()\n",
    "# sog = sog.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.signal import savgol_filter,savgol_coeffs\n",
    "\n",
    "# yhat = savgol_filter(sog, 5, 3) # window size 5, polynomial order 3\n",
    "# a = savgol_coeffs(5,3)\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(lat,sog)\n",
    "# plt.plot(lat,yhat, color='red')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# #specify degree of 3 for polynomial regression model\n",
    "# #include bias=False means don't force y-intercept to equal zero\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# #reshape data to work properly with sklearn\n",
    "# poly_features = poly.fit_transform(lat.reshape(-1, 1))\n",
    "# # poly_features = lat\n",
    "\n",
    "\n",
    "# #fit polynomial regression model\n",
    "# poly_reg_model = LinearRegression()\n",
    "# poly_reg_model.fit(poly_features, sog)\n",
    "\n",
    "# #display model coefficients\n",
    "# print(poly_reg_model.intercept_, poly_reg_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use model to make predictions on response variable\n",
    "# myline = np.linspace(55.1, 55.36, 100)\n",
    "# poly_features2 = poly.fit_transform(myline.reshape(-1, 1))\n",
    "# y_predicted = label_predict(poly_reg_model,poly_features2)\n",
    "\n",
    "\n",
    "# #create scatterplot of x vs. y\n",
    "# plt.scatter(lat, sog)\n",
    "# myline = np.linspace(55.1, 55.36, 100)\n",
    "\n",
    "# # add line to show fitted polynomial regression model\n",
    "# plt.plot(myline, y_predicted, color='purple')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(model_rfr_ftr.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 10, stop = 300, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = [int(x) for x in np.linspace(1, 12, num = 11)]\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 200, num = 10)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]# Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "# pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random = RandomizedSearchCV(estimator = model_rfr_hpov, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "# rf_random.fit(x_date, y_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, test_features, test_labels):\n",
    "#     from sklearn.metrics import explained_variance_score,mean_absolute_error,mean_squared_error,median_absolute_error\n",
    "\n",
    "#     predictions = model.predict(test_features)\n",
    "#     rsquared = model.score(test_features,test_labels)\n",
    "#     expVar = explained_variance_score(test_labels,predictions)\n",
    "#     MAE = mean_absolute_error(test_labels,predictions)\n",
    "#     MAD = median_absolute_error(test_labels,predictions)\n",
    "#     RMSE = np.sqrt(mean_squared_error(test_labels,predictions))\n",
    "\n",
    "#     print(f\"Model Performance of {model}\")\n",
    "#     print(f\"R^2: {rsquared:0.4f}\")\n",
    "#     print(f\"explained Variance = {expVar:0.4f}\")\n",
    "#     print(f\"MAE = {MAE:0.4f}\")\n",
    "#     print(f\"RMSE = {RMSE:0.4f}\")\n",
    "#     print(f\"MAD = {MAD:0.4f}\\n\")\n",
    "    \n",
    "#     return rsquared,expVar,MAE,RMSE,MAD\n",
    "\n",
    "# base_model = model_rfr_hpov\n",
    "# base_model.fit(x_date, y_date)\n",
    "# rsquared_base,expVar_base,MAE_base,RMSE_base,MAD_base = evaluate(base_model, x_date, y_date)\n",
    "\n",
    "# best_random = rf_random.best_estimator_\n",
    "# rsquared_random,expVar_random,MAE_random,RMSE_random,MAD_random = evaluate(best_random, x_date, y_date)\n",
    "# print('Improvement of Rsquared {:0.3f}%.'.format( 100 * (rsquared_random - rsquared_base) / rsquared_base))\n",
    "# print('Improvement of explainedVariance {:0.3f}%.'.format( 100 * (expVar_random - expVar_base) / expVar_base))\n",
    "# print('Improvement of MAE {:0.3f}%.'.format( 100 * (MAE_base - MAE_random) / MAE_base)) # MAE is other way around since best score is 0\n",
    "# print('Improvement of RMSE {:0.3f}%.'.format( 100 * (RMSE_base - RMSE_random) / RMSE_base)) # RMSE is other way around since best score is 0\n",
    "# print('Improvement of MAD {:0.3f}%.'.format( 100 * (MAD_base - MAD_random) / MAD_base)) # MAD is other way around since best score is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit a second degree polynomial to the economic data\n",
    "# from numpy import arange\n",
    "# from scipy.optimize import curve_fit\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "\n",
    "# # define the true objective function\n",
    "# def objective(x, a, b, c):\n",
    "# \treturn a * x + b * x**2 + c\n",
    "\n",
    "# # choose the input and output variables\n",
    "# x, y = dfprog.stw_act, dfprog.foc_act_d\n",
    "# x2, y2 = dfprog.stw_pred, dfprog.foc_pred_d\n",
    "# # curve fit\n",
    "# popt, _ = curve_fit(objective, x, y)\n",
    "# popt2, _ = curve_fit(objective, x2, y2)\n",
    "\n",
    "# # summarize the parameter values\n",
    "# a, b, c = popt\n",
    "# print('y = %.5f * x + %.5f * x^2 + %.5f' % (a, b, c))\n",
    "# a2, b2, c2 = popt2\n",
    "# print('y = %.5f * x + %.5f * x^2 + %.5f' % (a2, b2, c2))\n",
    "# # plot input vs output\n",
    "# # plt.scatter(x, y)\n",
    "# # plt.scatter(x2, y2)\n",
    "# # define a sequence of inputs between the smallest and largest known inputs\n",
    "# x_line = arange(min(x), max(x), 1)\n",
    "# x_line2 = arange(min(x), max(x), 1)\n",
    "# # calculate the output for the range\n",
    "# y_line = objective(x_line, a, b, c)\n",
    "# y_line2 = objective(x_line2, a2, b2, c2)\n",
    "# # create a line plot for the mapping function\n",
    "# plt.plot(x_line, y_line, color='red')\n",
    "# plt\t.plot(x_line2, y_line2, color='green')\n",
    "# plt.xlabel('STW', fontsize=15)\n",
    "# plt.ylabel('FOC', fontsize=15)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conversion of predicted SOG to m/s\n",
    "\n",
    "# dfpre2[\"sog_act\"] = dfpre2[\"sog\"]/1.9438\n",
    "\n",
    "# # Conversion of the angles to radian\n",
    "\n",
    "# rad_cog = np.deg2rad(dfpre2[\"heading\"])\n",
    "# # Calculation of the actual x-component of SOG\n",
    "\n",
    "# dfpre2[\"sog_x_act\"] = dfpre2[\"sog_act\"] * np.sin(rad_cog)\n",
    "# dfpre2[\"stw_x_act\"] = (dfpre2[\"sog_x_act\"] - dfpre2[\"eastcurrent\"])\n",
    "\n",
    "# # Calculation of the actual y-component of SOG\n",
    "\n",
    "# dfpre2[\"sog_y_act\"] = dfpre2[\"sog_act\"] * np.cos(rad_cog)\n",
    "# dfpre2[\"stw_y_act\"] = (dfpre2[\"sog_y_act\"] - dfpre2[\"northcurrent\"])\n",
    "\n",
    "# # For the actual data\n",
    "# dfpre2[\"vwms_a\"] = np.sqrt(dfpre2[\"stw_x_act\"]**2 + dfpre2[\"stw_y_act\"]**2)\n",
    "# dfpre2[\"stw_act\"] = dfpre2[\"vwms_a\"]*1.9438\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # dfprog = dfp.drop(['vgms','vgx','vcx','stw_x',\n",
    "# #                       'vgy','vcy','stw_y',\n",
    "# #                       'vgms_act','vgx_act','stw_x_act',\n",
    "# #                       'vgy_act','stw_y_act',\n",
    "# #                       'vwms_p','vwms_a'],axis=1)\n",
    "# # #df_ship.head(n=5)\n",
    "# dfpre2.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geron1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
