{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest number of features effect on performance\n",
    "# import numpy\n",
    "# from numpy import mean\n",
    "# from numpy import std\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from matplotlib import pyplot\n",
    "# pyplot.rcParams.update({\n",
    "#     \"font.family\": \"serif\",\n",
    "#     \"text.usetex\": True,\n",
    "#     'text.latex.preamble': [\n",
    "#         r'\\usepackage{amsmath}',\n",
    "#         r'\\usepackage{amssymb}',\n",
    "#         r\"\\usepackage{siunitx}\",\n",
    "#         r\"\\usepackage[notextcomp]{kpfonts}\",\n",
    "#      ]\n",
    "# })\n",
    "\n",
    "\n",
    "# # get the dataset\n",
    "# def get_dataset():\n",
    "# \tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# \treturn X, y\n",
    "\n",
    "# # get a list of models to evaluate\n",
    "# def get_models():\n",
    "# \tmodels = dict()\n",
    "# \t# explore number of features from 1 to 7\n",
    "# \tfor i in range(1,8):\n",
    "# \t\tmodels[str(i)] = RandomForestClassifier(max_features=i)\n",
    "# \treturn models\n",
    "\n",
    "# # evaluate a given model using cross-validation\n",
    "# def evaluate_model(model, X, y):\n",
    "# \t# define the evaluation procedure\n",
    "# \tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# \t# evaluate the model and collect the results\n",
    "# \tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# \treturn scores\n",
    "\n",
    "# # define dataset\n",
    "# X, y = get_dataset()\n",
    "# # get the models to evaluate\n",
    "# models = get_models()\n",
    "# # evaluate the models and store results\n",
    "# # results, names , means = list(), list(), list()\n",
    "# results, names  = list(), list()\n",
    "\n",
    "# for name, model in models.items():\n",
    "# \t# evaluate the model\n",
    "# \tscores = evaluate_model(model, X, y)\n",
    "# \t# store the results\n",
    "# \tresults.append(scores)\n",
    "# \tnames.append(name)\n",
    "# \t# means.append(a)\n",
    "# \t# summarize the performance along the way\n",
    "# \tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# \ta = numpy.mean(results,axis=1)\n",
    "# # plot model performance for comparison\n",
    "# a = numpy.mean(results,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# pyplot.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"serif\",\n",
    "#     \"font.sans-serif\": \"kpfonts\",\n",
    "# })\n",
    "\n",
    "# pyplot.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"serif\",\n",
    "#     \"font.sans-serif\": \"charter\",\n",
    "# })\n",
    "\n",
    "# pyplot.boxplot(results, labels=names,positions=range(len(names)) , showmeans=True)\n",
    "# plt.xlabel(r\"Number of features\",fontsize=18)\n",
    "# plt.ylabel(r\"RMSE\",fontsize=18)\n",
    "# plt.title(r\"Validation error\", fontsize=18)\n",
    "# pyplot.plot(names,a,\"b.-\")\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(5,3))\n",
    "# fig,ax = plt.subplots() \n",
    "# # ax.boxplot(results,showmeans=True)\n",
    "# # ax.set_xticklabels(names)\n",
    "# ax.plot(names,a,\"b.-\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.preprocessing import StandardScaler \n",
    "# ann_regr = MLPRegressor(activation='relu',alpha=10e-8,hidden_layer_sizes=(10,100),random_state=1,early_stopping=False,max_iter=500)\n",
    "# # ann_regr = MLPRegressor(random_state=1,max_iter=500)\n",
    "# # regressor_mlsc = linear_model.LinearRegression()\n",
    "# start_ann = time.time()\n",
    "# # model_ann = TransformedTargetRegressor(regressor= ann_regr,\n",
    "# #                                         transformer = StandardScaler()\n",
    "# #                                         ).fit(x_train,y_train)\n",
    "# model_ann = ann_regr.fit(x_train,y_train)\n",
    "# end_ann = time.time()\n",
    "# print(f\"Training time: {end_ann-start_ann:0.4}s \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, test_features, test_labels):\n",
    "#     from sklearn.metrics import explained_variance_score,mean_absolute_error,mean_squared_error,median_absolute_error\n",
    "\n",
    "#     predictions = model.predict(test_features)\n",
    "#     rsquared = model.score(test_features,test_labels)\n",
    "#     expVar = explained_variance_score(test_labels,predictions)\n",
    "#     MAE = mean_absolute_error(test_labels,predictions)\n",
    "#     MAD = median_absolute_error(test_labels,predictions)\n",
    "#     RMSE = np.sqrt(mean_squared_error(test_labels,predictions))\n",
    "\n",
    "#     print(f\"Model Performance of {model}\")\n",
    "#     print(f\"R^2: {rsquared:0.4f}\")\n",
    "#     print(f\"explained Variance = {expVar:0.4f}\")\n",
    "#     print(f\"MAE = {MAE:0.4f}\")\n",
    "#     print(f\"RMSE = {RMSE:0.4f}\")\n",
    "#     print(f\"MAD = {MAD:0.4f}\\n\")\n",
    "    \n",
    "#     # return rsquared,expVar,MAE,RMSE,MAD,predictions\n",
    "# rf_rsquared,rf_expVar,rf_MAE,rf_RMSE,rf_MAD,y_pred_rfr = evaluate(model_rfr_ftr,x_date,y_date)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE Error Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_rf_hpo = cross_val_score(model_rfr_ftr_hpov,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_et_hpo = cross_val_score(model_etr_hpov,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_dt_hpo = cross_val_score(model_dtr_hpov,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# score_rf = cross_val_score(model_rfr_ftr,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_et = cross_val_score(model_etr,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_dt = cross_val_score(model_dtr,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mlr = cross_val_score(model_mlr,x_train,y_train,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_sqr = score_rf.mean()\n",
    "# print(r_sqr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit a second degree polynomial to the economic data\n",
    "# from numpy import arange\n",
    "# from scipy.optimize import curve_fit\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "\n",
    "# # define the true objective function\n",
    "# def objective(x, a, b ):\n",
    "# \treturn a * x + b\n",
    "\n",
    "# def label_predict(model,test_features):\n",
    "#     predictions = model.predict(test_features)\n",
    "#     return predictions\n",
    "\n",
    "# y = label_predict(model_mlr_ftr,x_date)\n",
    "# y2 = label_predict(model_rfr_hpov,x_date)\n",
    "\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# print(r2_score(y_date, y))\n",
    "# print(r2_score(y_date,y2))\n",
    "\n",
    "# # choose the input and output variables\n",
    "# x, y = y_date, y\n",
    "# x2, y2 = y_date, y2\n",
    "# # curve fit\n",
    "# popt, _ = curve_fit(objective, x, y)\n",
    "# popt2, _ = curve_fit(objective, x2, y2)\n",
    "\n",
    "# # summarize the parameter values\n",
    "# a, b = popt\n",
    "# print('y = %.5f * x + %.5f' % (a, b ))\n",
    "# a2, b2 = popt2\n",
    "# print('y = %.5f * x + %.5f' % (a2, b2))\n",
    "# # plot input vs output\n",
    "# plt.scatter(x, y)\n",
    "# plt.scatter(x2, y2)\n",
    "# # define a sequence of inputs between the smallest and largest known inputs\n",
    "# x_line = arange(min(x), max(x), 1)\n",
    "# x_line2 = arange(min(x), max(x), 1)\n",
    "# # calculate the output for the range\n",
    "# y_line = objective(x_line, a, b)\n",
    "# y_line2 = objective(x_line2, a2, b2)\n",
    "# # create a line plot for the mapping function\n",
    "# plt.plot(x_line, y_line, color='red')\n",
    "# plt\t.plot(x_line2, y_line2, color='green')\n",
    "# plt.xlabel('STWpred', fontsize=15)\n",
    "# plt.ylabel('STWact', fontsize=15)\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# plt.scatter(y_test, y_predicted, c='crimson')\n",
    "# # plt.yscale('log')\n",
    "# # plt.xscale('log')\n",
    "\n",
    "# p1 = max(max(y_predicted), max(y_test))\n",
    "# p2 = min(min(y_predicted), min(y_test))\n",
    "# plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "# plt.xlabel('True Values', fontsize=15)\n",
    "# plt.ylabel('Predictions', fontsize=15)\n",
    "# plt.axis('equal')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth vs n estimator plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest number of trees effect on performance\n",
    "# from numpy import mean\n",
    "# from numpy import std\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# # get the dataset\n",
    "# # def get_dataset():\n",
    "# # \tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# # \treturn X, y\n",
    "\n",
    "# # get a list of models to evaluate\n",
    "# def get_models():\n",
    "# \tmodels_m2 = dict()\n",
    "# \t# define number of trees to consider\n",
    "# \t# n_trees = [1,2,3,4,5,10,50,100,250,500,750,1000]\n",
    "# \tn_trees = [1,2,3,4,5,6,7,8,9,10,100,200,300,400,500,600,700,800,900,1000]\n",
    "\n",
    "# \tfor n in n_trees:\n",
    "# \t\t# models[str(n)] = RandomForestRegressor(n_estimators=n)\n",
    "# \t\tmodels_m2[str(n)] = RandomForestRegressor(n_estimators=n,max_depth=1)\n",
    "# \treturn models_m2\n",
    "\n",
    "# # get a list of models to evaluate\n",
    "# def get_models6():\n",
    "# \tmodels_m6 = dict()\n",
    "# \t# define number of trees to consider\n",
    "# \t# n_trees = [1,2,3,4,5,10,50,100,250,500,750,1000]\n",
    "# \tn_trees = [1,2,3,4,5,6,7,8,9,10,100,200,300,400,500,600,700,800,900,1000]\n",
    "\n",
    "# \tfor n in n_trees:\n",
    "# \t\t# models[str(n)] = RandomForestRegressor(n_estimators=n)\n",
    "# \t\tmodels_m6[str(n)] = RandomForestRegressor(n_estimators=n,max_depth=6)\n",
    "# \treturn models_m6\n",
    "\n",
    "\n",
    "# # evaluate a given model using cross-validation\n",
    "# def evaluate_model(model, X, y):\n",
    "# \t# define the evaluation procedure\n",
    "# \t# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# \t# evaluate the model and collect the results\n",
    "# \tscores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n",
    "# \treturn -scores\n",
    "\n",
    "# # # define dataset\n",
    "# # X, y = get_dataset()\n",
    "# # get the models to evaluate\n",
    "# # models = get_models()\n",
    "# models_m2 = get_models()\n",
    "# models_m6 = get_models6()\n",
    "# # evaluate the models and store results\n",
    "# results, names = list(), list()\n",
    "# for name, model_m2 in models_m2.items():\n",
    "# \t# evaluate the model\n",
    "# \tscores = evaluate_model(model_m2, x_date, y_date)\n",
    "# \t# store the results\n",
    "# \tresults.append(scores)\n",
    "# \tnames.append(name)\n",
    "# \t# summarize the performance along the way\n",
    "# \tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "\n",
    "# results6, names6 = list(), list()\n",
    "# for name6, model_m6 in models_m6.items():\n",
    "# \t# evaluate the model\n",
    "# \tscores6 = evaluate_model(model_m6, x_date, y_date)\n",
    "# \t# store the results\n",
    "# \tresults6.append(scores6)\n",
    "# \tnames6.append(name6)\n",
    "# \t# summarize the performance along the way\n",
    "# \tprint('>%s %.3f (%.3f)' % (name6, mean(scores6), std(scores6)))\n",
    "\n",
    "\n",
    "# # plot model performance for comparison\n",
    "# # pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "# # pyplot.show()\n",
    "\n",
    "# a = np.mean(results,axis=1)\n",
    "# b = np.mean(results6,axis=1)\n",
    "# min_error = np.min(a)\n",
    "# min_error6 = np.min(b)\n",
    "# print(min_error)\n",
    "\n",
    "# plt.plot(names,a,\"b.-\")\n",
    "# bst_n_estimators = np.argmin(a) \n",
    "# print(bst_n_estimators)\n",
    "# # plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\",linewidth=1)\n",
    "# # plt.plot([-1, 12], [min_error, min_error], \"k--\",linewidth=1)\n",
    "# # plt.plot(bst_n_estimators, min_error, \"ko\",linewidth = 1)\n",
    "# # plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=12)\n",
    "\n",
    "\n",
    "# plt.plot(names,b,\"r.-\")\n",
    "# bst_n_estimators6 = np.argmin(b) \n",
    "# print(bst_n_estimators6)\n",
    "# # plt.plot([bst_n_estimators6, bst_n_estimators6], [0, min_error6], \"k--\",linewidth=1)\n",
    "# # plt.plot([-1, 12], [min_error6, min_error6], \"k--\",linewidth=1)\n",
    "# # plt.plot(bst_n_estimators6, min_error6, \"ko\",linewidth = 1)\n",
    "# # plt.text(bst_n_estimators6, min_error6*1.2, \"Minimum\", ha=\"center\", fontsize=12)\n",
    "\n",
    "# # plt.plot([names, max_score], \"k--\")\n",
    "# # plt.plot(names, max_score, \"ko\")\n",
    "# # plt.axis([0, 12, 0, 1])\n",
    "# plt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# # plt.xlim(0,12-1)\n",
    "# plt.xlim([0,1000])\n",
    "# # plt.ylim(0,3)\n",
    "# plt.xlabel(\"Number of Trees\")\n",
    "# plt.ylabel(\"RMSE\")\n",
    "# plt.title(\"Validation error\", fontsize=13)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "\n",
    "# # def tree_visualization(width,height,model_type):\n",
    "# #     fn=x_train.columns\n",
    "# #     fig, axes = plt.subplots(figsize = (width,height), dpi=800)\n",
    "# #     tree.plot_tree(model_type.estimators_[0],\n",
    "# #                max_depth=3,\n",
    "# #                fontsize=2,\n",
    "# #                feature_names = fn);\n",
    "# #     plt.show()\n",
    "# #     #fig.savefig('rf_individualtree.png')\n",
    "\n",
    "# # tree_visualization(5.5,3,model_rfr_ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hist plot depreciated\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams.update(plt.rcParamsDefault)\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"serif\",\n",
    "# })\n",
    "# df_ship2.hist(bins=50,figsize=(15,10))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest number of features effect on performance v1 no boxplot\n",
    "# def feature_curve(x,y,regressor):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "# \tfrom sklearn.model_selection import KFold\n",
    "\n",
    "# \tplt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t})\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_modelsftr():\n",
    "# \t\tmodels_ftr = dict()\n",
    "# \t\t# explore number of features from 1 to 13\n",
    "# \t\tfor n in range(1,13):\n",
    "# \t\t\tif regressor == 'dt':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = DecisionTreeRegressor(max_features=n)\n",
    "# \t\t\telif regressor == 'rf':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = RandomForestRegressor(max_features=n)\n",
    "# \t\t\telif regressor == 'et':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = ExtraTreesRegressor(max_features=n)\t\n",
    "# \t\treturn models_ftr\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model, x, y):\n",
    "# \t\t# define the evaluation procedure\n",
    "# \t\tcv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# \t\t# evaluate the model and collect the results\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "# \t\t# negative scores due to scoring mechanism of sklearn\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_ftr = get_modelsftr()\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_ftr, names_ftr = list(), list()\n",
    "# \tfor name, model in models_ftr.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_ftr = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_ftr.append(scores_ftr)\n",
    "# \t\tnames_ftr.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_ftr), std(scores_ftr)))\n",
    "\t\n",
    "# \t# Calculate mean for the x value of the plot\n",
    "\n",
    "# \tmean_ftr = np.mean(results_ftr,axis=1)\n",
    "# \tmin_error_ftr = np.min(mean_ftr)\n",
    "# \tbst_n_estimators = np.argmin(mean_ftr) \n",
    "\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_ftr:.3f}\")\n",
    "\n",
    "# \tplt.plot(names_ftr,mean_ftr,\"b.-\")\n",
    "# \tplt.plot([bst_n_estimators, bst_n_estimators], [0, min_error_ftr], \"k--\",linewidth=1)\n",
    "# \tplt.plot([-1, 12], [min_error_ftr, min_error_ftr], \"k--\",linewidth=1)\n",
    "# \tplt.plot(bst_n_estimators, min_error_ftr, \"ko\",linewidth = 1)\n",
    "# \tplt.text(bst_n_estimators, min_error_ftr*1.3, \"Minimum\", ha=\"center\", fontsize=14,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \tplt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,3)\n",
    "# \tplt.xlabel(r\"Number of features\",fontsize=18)\n",
    "# \tplt.ylabel(r\"RMSE\",fontsize=18)\n",
    "# \tplt.title(r\"Validation error\", fontsize=18)\n",
    "# \t# plt.text(4,2.5, r'def_param : \\tt{max_features = n_features}', bbox={'facecolor' : 'white','alpha':1},fontsize=18)\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest number of features effect on performance v1 no boxplot\n",
    "# def feature_curve(x,y,regressor):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "# \tfrom sklearn.model_selection import KFold\n",
    "\n",
    "# \tplt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t})\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_modelsftr():\n",
    "# \t\tmodels_ftr = dict()\n",
    "# \t\t# explore number of features from 1 to 13\n",
    "# \t\tfor n in range(1,13):\n",
    "# \t\t\tif regressor == 'dt':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = DecisionTreeRegressor(max_features=n)\n",
    "# \t\t\telif regressor == 'rf':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = RandomForestRegressor(max_features=n)\n",
    "# \t\t\telif regressor == 'et':\n",
    "# \t\t\t\tmodels_ftr[str(n)] = ExtraTreesRegressor(max_features=n)\t\n",
    "# \t\treturn models_ftr\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model, x, y):\n",
    "# \t\t# define the evaluation procedure\n",
    "# \t\tcv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# \t\t# evaluate the model and collect the results\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "# \t\t# negative scores due to scoring mechanism of sklearn\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_ftr = get_modelsftr()\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_ftr, names_ftr = list(), list()\n",
    "# \tfor name, model in models_ftr.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_ftr = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_ftr.append(scores_ftr)\n",
    "# \t\tnames_ftr.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_ftr), std(scores_ftr)))\n",
    "\t\n",
    "# \t# Calculate mean for the x value of the plot\n",
    "\n",
    "# \tmean_ftr = np.mean(results_ftr,axis=1)\n",
    "# \tmin_error_ftr = np.min(mean_ftr)\n",
    "# \tbst_n_estimators = np.argmin(mean_ftr) \n",
    "\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_ftr:.3f}\")\n",
    "\n",
    "# \tplt.plot(names_ftr,mean_ftr,\"b.-\")\n",
    "# \tplt.plot([bst_n_estimators, bst_n_estimators], [0, min_error_ftr], \"k--\",linewidth=1)\n",
    "# \tplt.plot([-1, 12], [min_error_ftr, min_error_ftr], \"k--\",linewidth=1)\n",
    "# \tplt.plot(bst_n_estimators, min_error_ftr, \"ko\",linewidth = 1)\n",
    "# \tplt.text(bst_n_estimators, min_error_ftr*1.3, \"Minimum\", ha=\"center\", fontsize=14,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \t# plt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,4)\n",
    "# \tplt.xlabel(r\"Number of features\",fontsize=18)\n",
    "# \tplt.ylabel(r\"RMSE\",fontsize=18)\n",
    "# \tplt.title(r\"Validation error\", fontsize=18)\n",
    "# \tplt.boxplot(results_ftr, labels=names_ftr,positions=range(len(names_ftr)), showmeans=True)\n",
    "# \t# plt.text(4,2.5, r'def_param : \\tt{max_features = n_features}', bbox={'facecolor' : 'white','alpha':1},fontsize=18)\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instead , we will invesitagte the effect of setting the minimal samples leaf\n",
    "# def leaf_curve(x,y,regressor,regname):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "\n",
    "# \tplt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t\"font.sans-serif\": \"bookman\",\n",
    "# \t})\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_models_leaf():\n",
    "# \t\tmodels_leaf = dict()\n",
    "# \t\t# define number of trees to consider\n",
    "# \t\tn_samples_leaf = [1,2,3,4,5,6,7,8,9,10,50,100]\n",
    "# \t\tfor n in n_samples_leaf:\n",
    "# \t\t\tif regressor == 'dt':\n",
    "# \t\t\t\tmodels_leaf[str(n)] = RandomForestRegressor(n_estimators = n)\n",
    "# \t\treturn models_leaf\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model, x, y):\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# # define dataset\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_leaf = get_models_leaf()\n",
    "\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_leaf, names_leaf = list(), list()\n",
    "# \tfor name, model in models_leaf.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_leaf = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_leaf.append(scores_leaf)\n",
    "# \t\tnames_leaf.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_leaf), std(scores_leaf)))\n",
    "\n",
    "\n",
    "# \tmean_leaf = np.mean(results_leaf,axis=1)\n",
    "# \tmin_error_leaf = np.min(mean_leaf)\n",
    "# \tprint(min_error_leaf)\n",
    "# \tbst_n_estimators_leaf = np.argmin(mean_leaf)\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_leaf:.3f}\")\n",
    "# \tplt.plot(names_leaf,mean_leaf,\"b.-\")\n",
    "# \tplt.plot([bst_n_estimators_leaf, bst_n_estimators_leaf], [0, min_error_leaf], \"k--\",linewidth=1)\n",
    "# \tplt.plot([-1, 12-1], [min_error_leaf, min_error_leaf], \"k--\",linewidth=1)\n",
    "# \tplt.plot(bst_n_estimators_leaf, min_error_leaf, \"ko\",linewidth = 1)\n",
    "# \tplt.text(bst_n_estimators_leaf, min_error_leaf*1.2, \"Minimum\", ha=\"center\", fontsize=12,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \tplt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,3)\n",
    "# \tplt.xlabel(\"Number of Samples in Leaf\")\n",
    "# \tplt.ylabel(\"RMSE\")\n",
    "# \tplt.title(rf\"{regname}\", fontsize=13)\n",
    "# \tplt.text(4,2.5, r'def_param : \\tt{min_samples_leaf = 1}', bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.boxplot(results_leaf, labels=names_leaf,positions=range(len(names_leaf)), showmeans=True)\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore random forest and extra tree number of trees effect on performance\n",
    "# def trees_curve(x,y,regressor):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "\n",
    "# \tplt.rcParams.update({\n",
    "# \t\"text.usetex\": True,\n",
    "# \t\"font.family\": \"serif\",\n",
    "# \t\"font.sans-serif\": \"bookman\",\n",
    "# \t})\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_models_tree():\n",
    "# \t\tmodels_tree = dict()\n",
    "# \t\t# define number of trees to consider\n",
    "# \t\tn_trees = [1,10,100,200,300,400,500,600,700,800,900,1000]\n",
    "# \t\tfor n in n_trees:\n",
    "# \t\t\tif regressor == 'rf':\n",
    "# \t\t\t\tmodels_tree[str(n)] = RandomForestRegressor(n_estimators = n)\n",
    "# \t\t\telif regressor == 'et':\n",
    "# \t\t\t\tmodels_tree[str(n)] = ExtraTreesRegressor(n_estimators = n)\t\n",
    "# \t\treturn models_tree\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model_tree, x, y):\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# # define dataset\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_tree = get_models_tree()\n",
    "\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_tree, names_tree = list(), list()\n",
    "# \tfor name, model in models_tree.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_tree = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_tree.append(scores_tree)\n",
    "# \t\tnames_tree.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_tree), std(scores_tree)))\n",
    "\n",
    "\n",
    "# \tmean_tree = np.mean(results_tree,axis=1)\n",
    "# \tmin_error_tree = np.min(mean_tree)\n",
    "# \tprint(min_error_tree)\n",
    "# \tbst_n_estimators_tree = np.argmin(mean_tree)\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_tree:.3f}\")\n",
    "# \tplt.plot(names_tree,mean_tree,\"b.-\")\n",
    "# \tplt.plot([bst_n_estimators_tree, bst_n_estimators_tree], [0, min_error_tree], \"k--\",linewidth=1)\n",
    "# \tplt.plot([-1, 12-1], [min_error_tree, min_error_tree], \"k--\",linewidth=1)\n",
    "# \tplt.plot(bst_n_estimators_tree, min_error_tree, \"ko\",linewidth = 1)\n",
    "# \tplt.text(bst_n_estimators_tree, min_error_tree*1.2, \"Minimum\", ha=\"center\", fontsize=12,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \tplt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,3)\n",
    "# \tplt.xlabel(\"Number of Trees\")\n",
    "# \tplt.ylabel(\"RMSE\")\n",
    "# \tplt.title(\"Validation error\", fontsize=13)\n",
    "# \tplt.text(4,2.5, r'def_param : \\tt{n_estimators = 100}', bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tpyplot.boxplot(results_tree, labels=names_tree, showmeans=True)\n",
    "# \tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # explore tree based , tree depth. effect on performance\n",
    "# def depth_curve(x,y,regressor):\n",
    "# \tfrom numpy import mean\n",
    "# \tfrom numpy import std\n",
    "# \tfrom sklearn.model_selection import cross_val_score\n",
    "# \tfrom sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# \t# get a list of models to evaluate\n",
    "# \tdef get_models_dp():\n",
    "# \t\tmodels_dp = dict()\n",
    "# \t\t# consider tree depths from 1 to 7 and None=full\n",
    "# \t\tdepths = [1,2,3,4,5,6,7,8,9,10,100] + [None]\n",
    "# \t\tfor n in depths:\n",
    "# \t\t\tif regressor == 'dt':\n",
    "# \t\t\t\tmodels_dp[str(n)] = DecisionTreeRegressor(max_depth=n)\n",
    "# \t\t\telif regressor == 'rf':\n",
    "# \t\t\t\tmodels_dp[str(n)] = RandomForestRegressor(max_depth=n)\n",
    "# \t\t\telif regressor == 'et':\n",
    "# \t\t\t\tmodels_dp[str(n)] = ExtraTreesRegressor(max_depth=n)\t\n",
    "# \t\treturn models_dp\n",
    "\n",
    "# \t# evaluate a given model using cross-validation\n",
    "# \tdef evaluate_model(model, x, y):\n",
    "# \t\t# define the evaluation procedure\n",
    "# \t\tcv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# \t\t# evaluate the model and collect the results\n",
    "# \t\tscores = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "# \t\t# negative scores due to scoring mechanism of sklearn\n",
    "# \t\treturn -scores\n",
    "\n",
    "# \t# get the models to evaluate\n",
    "# \tmodels_dp = get_models_dp()\n",
    "# \t# evaluate the models and store results\n",
    "# \tresults_dp, names_dp = list(), list()\n",
    "# \tfor name, model in models_dp.items():\n",
    "# \t\t# evaluate the model\n",
    "# \t\tscores_dp = evaluate_model(model, x, y)\n",
    "# \t\t# store the results\n",
    "# \t\tresults_dp.append(scores_dp)\n",
    "# \t\tnames_dp.append(name)\n",
    "# \t\t# summarize the performance along the way\n",
    "# \t\tprint('>%s %.3f (%.3f)' % (name, mean(scores_dp), std(scores_dp)))\n",
    "\n",
    "\n",
    "# \tmean_dp = np.mean(results_dp,axis=1)\n",
    "# \tmin_error_dp = np.min(mean_dp)\n",
    "# \tprint(min_error_dp)\n",
    "# \tbst_n_estimators_dp= np.argmin(mean_dp)\n",
    "# \tprint(f\"The minimum RMSE obtained is {min_error_dp:.3f}\")\n",
    "# \tplt.plot(names_dp,mean_dp,\"b.-\")\n",
    "# \t# Test\n",
    "# \t# plt.errorbar(names_dp,mean_dp,results_dp)\n",
    "# \t# plt.plot([bst_n_estimators_dp, bst_n_estimators_dp], [0, min_error_dp], \"k--\",linewidth=1)\n",
    "# \t# plt.plot([-1, 12-1], [min_error_dp, min_error_dp], \"k--\",linewidth=1)\n",
    "# \t# plt.plot(bst_n_estimators_dp, min_error_dp, \"ko\",linewidth = 1)\n",
    "# \t# plt.text(bst_n_estimators_dp, min_error_dp*1.2, \"Minimum\", ha=\"center\", fontsize=14,bbox={'facecolor' : 'white','alpha':1})\n",
    "# \tplt.grid(linestyle = '--', linewidth = 0.5)\n",
    "# \t# plt.xlim(0,12-1)\n",
    "# \tplt.ylim(0,4)\n",
    "# \tplt.xlabel(r\"Tree Depth\",fontsize=18)\n",
    "# \tplt.ylabel(r\"RMSE\",fontsize=18)\n",
    "# \tplt.title(r\"Validation error\", fontsize=18)\n",
    "# \tplt.text(4,2.5, r'def_param : \\tt{max_depth = None}', bbox={'facecolor' : 'white','alpha':1},fontsize=12)\n",
    "# \tplt.boxplot(results_dp, labels=names_dp,positions=range(len(names_dp)), showmeans=True)\n",
    "# \tplt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROM JUNE AIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# score_rf = cross_val_score(model_rfr_ftr,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_et = cross_val_score(model_etr,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_dt = cross_val_score(model_dtr_ftr,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_mlr = cross_val_score(model_mlr_ftr,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_rfopt = cross_val_score(model_rfr_hpov,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_etopt = cross_val_score(model_etr_hpov,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)\n",
    "# score_dtopt = cross_val_score(model_dtr_hpov,x_date,y_date,\n",
    "#                            scoring='r2',cv=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse_rf = cross_val_score(model_rfr_ftr,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_et = cross_val_score(model_etr,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_dt = cross_val_score(model_dtr_ftr,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_rfopt = cross_val_score(model_rfr_hpov,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_etopt = cross_val_score(model_etr_hpov,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)\n",
    "# rmse_dtopt = cross_val_score(model_dtr_hpov,x_date,y_date,\n",
    "#                            scoring='neg_root_mean_squared_error',cv=10)Check STW and SOG\n",
    "# Keep this part for SOG and STW check\n",
    "\n",
    "# df[\"vgms\"] = df[\"SOG\"]/1.9438\n",
    "# rad_gamma = np.deg2rad(df[\"True North Current Direction\"])\n",
    "# rad_cog = np.deg2rad(df[\"COG\"])\n",
    "# df[\"vgx\"] = df[\"vgms\"] * np.sin(rad_cog)\n",
    "# df[\"vcx\"] = df[\"Current Speed\"] * np.sin(rad_gamma)\n",
    "# df[\"stw_x\"] = (df[\"vgx\"] - df[\"vcx\"])\n",
    "# df[\"vgy\"] = df[\"vgms\"] * np.cos(rad_cog)\n",
    "# df[\"vcy\"] = df[\"Current Speed\"] * np.cos(rad_gamma)\n",
    "# df[\"stw_y\"] = (df[\"vgy\"] - df[\"vcy\"])\n",
    "# df[\"vwms_a\"] = np.sqrt(df[\"stw_x\"]**2 + df[\"stw_y\"]**2)\n",
    "# df[\"stw_act\"] = df[\"vwms_a\"]*1.9438\n",
    "\n",
    "# dfsog= df[df[\"SOG\"] > df[\"stw_act\"] ]\n",
    "# dfsog.head(n=10)\n",
    "# dfsog.to_csv(\"SOG_june_update.csv\")\n",
    "\n",
    "# dfstw= df[df[\"SOG\"] < df[\"stw_act\"] ]\n",
    "# dfstw.head(n=10)\n",
    "# dfstw.to_csv(\"STW_june_update.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import scipy.stats as stats\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# stw = dfprog.stw_act\n",
    "# foc = dfprog.foc_act_d\n",
    "\n",
    "# stw = stw.to_numpy()\n",
    "# foc = foc.to_numpy()\n",
    "\n",
    "# #specify degree of 3 for polynomial regression model\n",
    "# #include bias=False means don't force y-intercept to equal zero\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# #reshape data to work properly with sklearn\n",
    "# poly_features = poly.fit_transform(stw.reshape(-1,1))\n",
    "# # poly_features = lat\n",
    "\n",
    "\n",
    "# #fit polynomial regression model\n",
    "# poly_reg_model = LinearRegression()\n",
    "# poly_reg_model.fit(poly_features, stw)\n",
    "\n",
    "# #display model coefficients\n",
    "# print(poly_reg_model.intercept_, poly_reg_model.coef_)\n",
    "\n",
    "# y_predicted = label_predict(poly_reg_model,poly_features)\n",
    "# y_line \n",
    "\n",
    "# #create scatterplot of x vs. y\n",
    "# plt.scatter(stw, foc)\n",
    "# # myline = np.linspace(55.1, 55.36, 100)\n",
    "\n",
    "# # add line to show fitted polynomial regression model\n",
    "# # plt.plot(poly_features, y_predicted, color='purple')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize Single Day Journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_df = dfone6.drop(['Unnamed: 0','Time','Air density above oceans',\n",
    "                    'Surface pressure','Width','Length'],axis=1)\n",
    "ga_df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_df = ga_df.rename({'Max wave height': 'waveheight', 'Draught': 'draught',\n",
    "                           'SOG': 'sog', 'Wind Speed': 'windspeed', \n",
    "                           'True Wind Direction': 'truewinddir','Temperature above oceans' : 'oceantemperature',\n",
    "                           'COG': 'cog', 'Current Speed' : 'curspeed','True Wave Direction' : 'truewavedir',\n",
    "                            'Swell period': 'swellperiod','Wind wave period': 'windwaveperiod','Sea surface temperature': 'surftemp',\n",
    "                            'Combined wind waves and swell height': 'windwaveswellheight','Swell height': 'swellheight','Wind wave height': 'windwaveheight',\n",
    "                            'Heading': 'heading','True Current Direction': 'truecurrentdir','True Swell Direction': 'trueswelldir',\n",
    "                            'True Wind Wave Direction': 'truewindwavedir','Wave period': 'waveperiod',\n",
    "                            'True North Wind Direction' : 'truenorthwinddir' , 'True North Current Direction' : 'truenorthcurrentdir'\n",
    "                           }, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ga_df = ga_df[['oceantemperature','waveheight','swellperiod','windwaveperiod','waveperiod','surftemp','windwaveswellheight','swellheight','windwaveheight','draught','sog','cog','heading','windspeed','curspeed','truewinddir','truecurrentdir','trueswelldir','truewindwavedir','truewavedir', 'truenorthwinddir' , 'truenorthcurrentdir']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lat = ga_df.LAT\n",
    "sog = ga_df.sog\n",
    "\n",
    "#create scatterplot\n",
    "plt.scatter(lat, sog)\n",
    "plt.show()\n",
    "\n",
    "lat = lat.to_numpy()\n",
    "sog = sog.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.signal import savgol_filter,savgol_coeffs\n",
    "\n",
    "# yhat = savgol_filter(sog, 5, 3) # window size 5, polynomial order 3\n",
    "# a = savgol_coeffs(5,3)\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(lat,sog)\n",
    "# plt.plot(lat,yhat, color='red')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#specify degree of 3 for polynomial regression model\n",
    "#include bias=False means don't force y-intercept to equal zero\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "#reshape data to work properly with sklearn\n",
    "poly_features = poly.fit_transform(lat.reshape(-1, 1))\n",
    "# poly_features = lat\n",
    "\n",
    "\n",
    "#fit polynomial regression model\n",
    "poly_reg_model = LinearRegression()\n",
    "poly_reg_model.fit(poly_features, sog)\n",
    "\n",
    "#display model coefficients\n",
    "print(poly_reg_model.intercept_, poly_reg_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use model to make predictions on response variable\n",
    "myline = np.linspace(55.1, 55.36, 100)\n",
    "poly_features2 = poly.fit_transform(myline.reshape(-1, 1))\n",
    "y_predicted = label_predict(poly_reg_model,poly_features2)\n",
    "\n",
    "\n",
    "#create scatterplot of x vs. y\n",
    "plt.scatter(lat, sog)\n",
    "myline = np.linspace(55.1, 55.36, 100)\n",
    "\n",
    "# add line to show fitted polynomial regression model\n",
    "plt.plot(myline, y_predicted, color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(model_rfr_ftr.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 10, stop = 300, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = [int(x) for x in np.linspace(1, 12, num = 11)]\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 200, num = 10)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]# Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "# pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random = RandomizedSearchCV(estimator = model_rfr_hpov, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "# rf_random.fit(x_date, y_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, test_features, test_labels):\n",
    "#     from sklearn.metrics import explained_variance_score,mean_absolute_error,mean_squared_error,median_absolute_error\n",
    "\n",
    "#     predictions = model.predict(test_features)\n",
    "#     rsquared = model.score(test_features,test_labels)\n",
    "#     expVar = explained_variance_score(test_labels,predictions)\n",
    "#     MAE = mean_absolute_error(test_labels,predictions)\n",
    "#     MAD = median_absolute_error(test_labels,predictions)\n",
    "#     RMSE = np.sqrt(mean_squared_error(test_labels,predictions))\n",
    "\n",
    "#     print(f\"Model Performance of {model}\")\n",
    "#     print(f\"R^2: {rsquared:0.4f}\")\n",
    "#     print(f\"explained Variance = {expVar:0.4f}\")\n",
    "#     print(f\"MAE = {MAE:0.4f}\")\n",
    "#     print(f\"RMSE = {RMSE:0.4f}\")\n",
    "#     print(f\"MAD = {MAD:0.4f}\\n\")\n",
    "    \n",
    "#     return rsquared,expVar,MAE,RMSE,MAD\n",
    "\n",
    "# base_model = model_rfr_hpov\n",
    "# base_model.fit(x_date, y_date)\n",
    "# rsquared_base,expVar_base,MAE_base,RMSE_base,MAD_base = evaluate(base_model, x_date, y_date)\n",
    "\n",
    "# best_random = rf_random.best_estimator_\n",
    "# rsquared_random,expVar_random,MAE_random,RMSE_random,MAD_random = evaluate(best_random, x_date, y_date)\n",
    "# print('Improvement of Rsquared {:0.3f}%.'.format( 100 * (rsquared_random - rsquared_base) / rsquared_base))\n",
    "# print('Improvement of explainedVariance {:0.3f}%.'.format( 100 * (expVar_random - expVar_base) / expVar_base))\n",
    "# print('Improvement of MAE {:0.3f}%.'.format( 100 * (MAE_base - MAE_random) / MAE_base)) # MAE is other way around since best score is 0\n",
    "# print('Improvement of RMSE {:0.3f}%.'.format( 100 * (RMSE_base - RMSE_random) / RMSE_base)) # RMSE is other way around since best score is 0\n",
    "# print('Improvement of MAD {:0.3f}%.'.format( 100 * (MAD_base - MAD_random) / MAD_base)) # MAD is other way around since best score is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit a second degree polynomial to the economic data\n",
    "# from numpy import arange\n",
    "# from scipy.optimize import curve_fit\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "\n",
    "# # define the true objective function\n",
    "# def objective(x, a, b, c):\n",
    "# \treturn a * x + b * x**2 + c\n",
    "\n",
    "# # choose the input and output variables\n",
    "# x, y = dfprog.stw_act, dfprog.foc_act_d\n",
    "# x2, y2 = dfprog.stw_pred, dfprog.foc_pred_d\n",
    "# # curve fit\n",
    "# popt, _ = curve_fit(objective, x, y)\n",
    "# popt2, _ = curve_fit(objective, x2, y2)\n",
    "\n",
    "# # summarize the parameter values\n",
    "# a, b, c = popt\n",
    "# print('y = %.5f * x + %.5f * x^2 + %.5f' % (a, b, c))\n",
    "# a2, b2, c2 = popt2\n",
    "# print('y = %.5f * x + %.5f * x^2 + %.5f' % (a2, b2, c2))\n",
    "# # plot input vs output\n",
    "# # plt.scatter(x, y)\n",
    "# # plt.scatter(x2, y2)\n",
    "# # define a sequence of inputs between the smallest and largest known inputs\n",
    "# x_line = arange(min(x), max(x), 1)\n",
    "# x_line2 = arange(min(x), max(x), 1)\n",
    "# # calculate the output for the range\n",
    "# y_line = objective(x_line, a, b, c)\n",
    "# y_line2 = objective(x_line2, a2, b2, c2)\n",
    "# # create a line plot for the mapping function\n",
    "# plt.plot(x_line, y_line, color='red')\n",
    "# plt\t.plot(x_line2, y_line2, color='green')\n",
    "# plt.xlabel('STW', fontsize=15)\n",
    "# plt.ylabel('FOC', fontsize=15)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conversion of predicted SOG to m/s\n",
    "\n",
    "# dfpre2[\"sog_act\"] = dfpre2[\"sog\"]/1.9438\n",
    "\n",
    "# # Conversion of the angles to radian\n",
    "\n",
    "# rad_cog = np.deg2rad(dfpre2[\"heading\"])\n",
    "# # Calculation of the actual x-component of SOG\n",
    "\n",
    "# dfpre2[\"sog_x_act\"] = dfpre2[\"sog_act\"] * np.sin(rad_cog)\n",
    "# dfpre2[\"stw_x_act\"] = (dfpre2[\"sog_x_act\"] - dfpre2[\"eastcurrent\"])\n",
    "\n",
    "# # Calculation of the actual y-component of SOG\n",
    "\n",
    "# dfpre2[\"sog_y_act\"] = dfpre2[\"sog_act\"] * np.cos(rad_cog)\n",
    "# dfpre2[\"stw_y_act\"] = (dfpre2[\"sog_y_act\"] - dfpre2[\"northcurrent\"])\n",
    "\n",
    "# # For the actual data\n",
    "# dfpre2[\"vwms_a\"] = np.sqrt(dfpre2[\"stw_x_act\"]**2 + dfpre2[\"stw_y_act\"]**2)\n",
    "# dfpre2[\"stw_act\"] = dfpre2[\"vwms_a\"]*1.9438\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # dfprog = dfp.drop(['vgms','vgx','vcx','stw_x',\n",
    "# #                       'vgy','vcy','stw_y',\n",
    "# #                       'vgms_act','vgx_act','stw_x_act',\n",
    "# #                       'vgy_act','stw_y_act',\n",
    "# #                       'vwms_p','vwms_a'],axis=1)\n",
    "# # #df_ship.head(n=5)\n",
    "# dfpre2.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geron1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
